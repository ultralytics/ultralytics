---
comments: true
description: Explore la amplia gama de modelos de la familia YOLO, SAM, MobileSAM, FastSAM, YOLO-NAS y RT-DETR soportados por Ultralytics. Comienza con ejemplos para el uso tanto de CLI como de Python.
keywords: Ultralytics, documentaci贸n, YOLO, SAM, MobileSAM, FastSAM, YOLO-NAS, RT-DETR, modelos, arquitecturas, Python, CLI
---

# Modelos soportados por Ultralytics

隆Bienvenido a la documentaci贸n de modelos de Ultralytics! Ofrecemos soporte para una amplia gama de modelos, cada uno adaptado a tareas espec铆ficas como [detecci贸n de objetos](../tasks/detect.md), [segmentaci贸n de instancias](../tasks/segment.md), [clasificaci贸n de im谩genes](../tasks/classify.md), [estimaci贸n de posturas](../tasks/pose.md), y [seguimiento de m煤ltiples objetos](../modes/track.md). Si est谩s interesado en contribuir con tu arquitectura de modelo a Ultralytics, consulta nuestra [Gu铆a de Contribuci贸n](../../help/contributing.md).

!!! Note "Nota"

     Estamos trabajando arduamente para mejorar nuestra documentaci贸n en varios idiomas actualmente en construcci贸n. 隆Gracias por tu paciencia! 

## Modelos destacados

Aqu铆 est谩n algunos de los modelos clave soportados:

1. **[YOLOv3](yolov3.md)**: La tercera iteraci贸n de la familia de modelos YOLO, original de Joseph Redmon, conocida por su capacidad de detecci贸n de objetos en tiempo real eficientemente.
2. **[YOLOv4](yolov4.md)**: Una actualizaci贸n nativa de darknet para YOLOv3, lanzada por Alexey Bochkovskiy en 2020.
3. **[YOLOv5](yolov5.md)**: Una versi贸n mejorada de la arquitectura YOLO por Ultralytics, ofreciendo un mejor rendimiento y compromiso de velocidad comparado con versiones anteriores.
4. **[YOLOv6](yolov6.md)**: Lanzado por [Meituan](https://about.meituan.com/) en 2022, y utilizado en muchos de los robots de entrega aut贸nomos de la compa帽铆a.
5. **[YOLOv7](yolov7.md)**: Modelos YOLO actualizados lanzados en 2022 por los autores de YOLOv4.
6. **[YOLOv8](yolov8.md) NUEVO **: La 煤ltima versi贸n de la familia YOLO, con capacidades mejoradas como segmentaci贸n de instancias, estimaci贸n de posturas/puntos clave y clasificaci贸n.
7. **[Modelo Segment Anything (SAM)](sam.md)**: Modelo Segment Anything (SAM) de Meta.
8. **[Mobile Segment Anything Model (MobileSAM)](mobile-sam.md)**: MobileSAM para aplicaciones m贸viles, por la Universidad de Kyung Hee.
9. **[Fast Segment Anything Model (FastSAM)](fast-sam.md)**: FastSAM por el Grupo de An谩lisis de Imagen y Video, Instituto de Automatizaci贸n, Academia China de Ciencias.
10. **[YOLO-NAS](yolo-nas.md)**: Modelos YOLO de B煤squeda de Arquitectura Neural (NAS).
11. **[Transformadores de Detecci贸n en Tiempo Real (RT-DETR)](rtdetr.md)**: Modelos de Transformador de Detecci贸n en Tiempo Real (RT-DETR) de Baidu's PaddlePaddle.

<p align="center">
  <br>
  <iframe width="720" height="405" src="https://www.youtube.com/embed/MWq1UxqTClU?si=nHAW-lYDzrz68jR0"
    title="Reproductor de video de YouTube" frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen>
  </iframe>
  <br>
  <strong>Mira:</strong> Ejecuta modelos YOLO de Ultralytics en solo unas pocas l铆neas de c贸digo.
</p>

## Empezando: Ejemplos de Uso

Este ejemplo proporciona ejemplos simples de entrenamiento e inferencia YOLO. Para la documentaci贸n completa de estos y otros [modos](../modes/index.md), consulta las p谩ginas de documentaci贸n de [Predict](../modes/predict.md), [Train](../modes/train.md), [Val](../modes/val.md) y [Export](../modes/export.md).

Nota que el siguiente ejemplo es para los modelos YOLOv8 [Detect](../tasks/detect.md) para detecci贸n de objetos. Para tareas adicionales soportadas, consulta la documentaci贸n de [Segment](../tasks/segment.md), [Classify](../tasks/classify.md) y [Pose](../tasks/pose.md).

!!! Example "Ejemplo"

    === "Python"

        Los modelos pre-entrenados `*.pt` de PyTorch as铆 como los archivos de configuraci贸n `*.yaml` se pueden pasar a las clases `YOLO()`, `SAM()`, `NAS()` y `RTDETR()` para crear una instancia de modelo en Python:

        ```python
        from ultralytics import YOLO

        # Cargar un modelo YOLOv8n preentrenado en COCO
        model = YOLO('yolov8n.pt')

        # Mostrar informaci贸n del modelo (opcional)
        model.info()

        # Entrenar el modelo en el conjunto de datos de ejemplo COCO8 durante 100 茅pocas
        results = model.train(data='coco8.yaml', epochs=100, imgsz=640)

        # Ejecutar inferencia con el modelo YOLOv8n en la imagen 'bus.jpg'
        results = model('path/to/bus.jpg')
        ```

    === "CLI"

        Los comandos CLI est谩n disponibles para ejecutar directamente los modelos:

        ```bash
        # Cargar un modelo YOLOv8n preentrenado en COCO y entrenarlo en el conjunto de datos de ejemplo COCO8 durante 100 茅pocas
        yolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640

        # Cargar un modelo YOLOv8n preentrenado en COCO y ejecutar inferencia en la imagen 'bus.jpg'
        yolo predict model=yolov8n.pt source=path/to/bus.jpg
        ```

## Contribuir con Nuevos Modelos

驴Interesado en contribuir con tu modelo a Ultralytics? 隆Genial! Siempre estamos abiertos a expandir nuestro portafolio de modelos.

1. **Haz un Fork del Repositorio**: Comienza haciendo un fork del [repositorio de GitHub de Ultralytics](https://github.com/ultralytics/ultralytics).

2. **Clona tu Fork**: Clona tu fork a tu m谩quina local y crea una nueva rama para trabajar.

3. **Implementa tu Modelo**: A帽ade tu modelo siguiendo los est谩ndares de codificaci贸n y directrices proporcionadas en nuestra [Gu铆a de Contribuci贸n](../../help/contributing.md).

4. **Prueba Rigurosamente**: Aseg煤rate de probar tu modelo rigurosamente, tanto de forma aislada como parte del proceso.

5. **Crea un Pull Request**: Una vez que est茅s satisfecho con tu modelo, crea un pull request al repositorio principal para revisi贸n.

6. **Revisi贸n de C贸digo y Fusi贸n**: Despu茅s de la revisi贸n, si tu modelo cumple con nuestros criterios, ser谩 fusionado al repositorio principal.

Para pasos detallados, consulta nuestra [Gu铆a de Contribuci贸n](../../help/contributing.md).
