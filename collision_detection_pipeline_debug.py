#!/usr/bin/env python3
"""
collision_detection_pipeline_debug.py.

åˆ†é˜¶æ®µè°ƒè¯•ç‰ˆæœ¬ï¼Œæ”¯æŒï¼š
- å„é˜¶æ®µæš‚åœç‚¹
- ç”Ÿæˆå¯è§†åŒ–æ£€æµ‹æ¡†è§†é¢‘
- YOLOæ£€æµ‹ç»Ÿè®¡æŠ¥å‘Š
- è·³å¸§å¤„ç†åŠ å¿«é€Ÿåº¦
- è¾“å‡ºå…³é”®å¸§ï¼ˆå¸¦æ£€æµ‹æ¡†ï¼‰
"""

import argparse
import json
import os
import sys
from collections import defaultdict
from datetime import datetime
from pathlib import Path

import cv2
import numpy as np

# å¯¼å…¥YOLOå’Œç›¸å…³æ¨¡å—
sys.path.append(os.path.dirname(__file__))
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from homography_transform_utils import compute_transformation_matrix, load_homography, transform_frame_manual
from ultralytics import YOLO


class DebugCollisionDetectionPipeline:
    def __init__(self, video_path, homography_path, output_base="../../results", frame_skip=1, debug_mode=True):
        """åˆå§‹åŒ–pipeline.

        Args:
            video_path: åŸå§‹è§†é¢‘è·¯å¾„
            homography_path: Homography JSONè·¯å¾„
            output_base: ç»“æœåŸºç¡€ç›®å½•
            frame_skip: è·³å¸§æ•°ï¼ˆ1=ä¸è·³å¸§ï¼Œ10=æ¯10å¸§å¤„ç†1å¸§ï¼‰
            debug_mode: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆç”Ÿæˆæ›´å¤šå¯è§†åŒ–ï¼‰
        """
        self.video_path = video_path
        self.homography_path = homography_path
        self.output_base = Path(output_base)
        self.frame_skip = frame_skip
        self.debug_mode = debug_mode

        # åˆ›å»ºæ—¶é—´æˆ³
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_dir = self.output_base / self.timestamp

        print("=" * 70)
        print("ç¢°æ’æ£€æµ‹Pipeline - åˆ†é˜¶æ®µè°ƒè¯•ç‰ˆæœ¬")
        print("=" * 70)
        print(f"æ—¶é—´æˆ³: {self.timestamp}")
        print(f"ç»“æœç›®å½•: {self.run_dir}")
        print(f"è·³å¸§æ•°: {self.frame_skip}")
        if self.debug_mode:
            print("è°ƒè¯•æ¨¡å¼: å·²å¯ç”¨ âœ“")
        print()

        # åˆ›å»ºå­ç›®å½•
        self.homography_dir = self.run_dir / "1_homography"
        self.warped_video_dir = self.run_dir / "2_warped_video"
        self.yolo_dir = self.run_dir / "3_yolo_detection"
        self.collision_dir = self.run_dir / "4_collision_analysis"

        for dir_path in [self.homography_dir, self.warped_video_dir, self.yolo_dir, self.collision_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

    def load_homography(self):
        """åŠ è½½HomographyçŸ©é˜µ."""
        print("\nã€æ­¥éª¤1: åŠ è½½HomographyçŸ©é˜µã€‘")

        self.H, self.world_points = load_homography(self.homography_path)

        pixel_points = self.get_pixel_points_from_homography()

        print("âœ“ HomographyçŸ©é˜µå·²åŠ è½½")
        print(f"  åƒç´ ç‚¹æ•°: {len(pixel_points)}")
        print("  ä¸–ç•Œåæ ‡èŒƒå›´:")
        print(f"    X: [{min(w[0] for w in self.world_points):.2f}, {max(w[0] for w in self.world_points):.2f}]m")
        print(f"    Y: [{min(w[1] for w in self.world_points):.2f}, {max(w[1] for w in self.world_points):.2f}]m")

        return self.H, self.world_points

    def get_pixel_points_from_homography(self):
        """ä»homography JSONè·å–åƒç´ ç‚¹."""
        with open(self.homography_path) as f:
            data = json.load(f)
        return data.get("pixel_points", [])

    def create_verification_image(self):
        """ç”ŸæˆéªŒè¯å›¾."""
        print("\nã€æ­¥éª¤1.5: ç”ŸæˆéªŒè¯å›¾ã€‘")

        cap = cv2.VideoCapture(self.video_path)
        ret, frame = cap.read()
        cap.release()

        if not ret:
            print("âŒ æ— æ³•è¯»å–è§†é¢‘")
            return

        # ç»˜åˆ¶æ ‡å®šç‚¹
        frame_marked = frame.copy()
        pixel_points = self.get_pixel_points_from_homography()

        for i, (px, py) in enumerate(pixel_points):
            cv2.circle(frame_marked, (int(px), int(py)), 10, (0, 255, 0), 2)
            world_point = self.world_points[i]
            label = f"({world_point[0]:.1f}m, {world_point[1]:.1f}m)"
            cv2.putText(
                frame_marked, label, (int(px) + 15, int(py) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1
            )

        verify_path = self.homography_dir / "verify_original.jpg"
        cv2.imwrite(str(verify_path), frame_marked)

        print(f"âœ“ éªŒè¯å›¾å·²ä¿å­˜: {verify_path.name}")
        return frame_marked

    def pause_checkpoint(self, stage_name):
        """æš‚åœç‚¹ - ç­‰å¾…ç”¨æˆ·ç¡®è®¤ç»§ç»­."""
        if not self.debug_mode:
            return

        print(f"\nâ¸ï¸  é˜¶æ®µ '{stage_name}' å·²å®Œæˆ")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶å·²ä¿å­˜åˆ°: {self.run_dir}/{stage_name}/")
        response = input("æŒ‰Enterç»§ç»­ï¼Œæˆ–è¾“å…¥'q'é€€å‡º: ").strip().lower()

        if response == "q":
            print("âŒ ç”¨æˆ·ä¸­æ­¢pipeline")
            sys.exit(0)

    def transform_video(self):
        """å¯¹è§†é¢‘è¿›è¡Œé€è§†å˜æ¢."""
        print("\nã€æ­¥éª¤2: è§†é¢‘é€è§†å˜æ¢ã€‘")

        # ä½¿ç”¨æ­£ç¡®çš„è¾“å‡ºå°ºå¯¸
        output_size = (180, 1200)

        # è®¡ç®—å˜æ¢çŸ©é˜µ
        M, world_bounds = compute_transformation_matrix(self.H, self.world_points, output_size)
        min_x, max_x, min_y, max_y = world_bounds

        # å¤„ç†è§†é¢‘
        cap = cv2.VideoCapture(self.video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºæ–‡ä»¶å
        input_path = Path(self.video_path)
        video_name = input_path.stem
        output_filename = f"{video_name}_warped_{self.timestamp}.mp4"
        warped_path = self.warped_video_dir / output_filename

        # åˆ›å»ºVideoWriter (ä¿æŒåŸFPSï¼Œä½†æ€»å¸§æ•°ä¼šå‡å°‘)
        output_fps = fps / self.frame_skip  # è·³å¸§åçš„FPS
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        out = cv2.VideoWriter(str(warped_path), fourcc, output_fps, output_size)

        total_to_process = (total_frames + self.frame_skip - 1) // self.frame_skip
        print(f"å¤„ç†ä¸­: {total_frames}å¸§ â†’ {total_to_process}å¸§ (è·³å¸§:{self.frame_skip})...")
        print(f"è¾“å‡ºå°ºå¯¸: {output_size[0]}Ã—{output_size[1]} (å®½Ã—é«˜)")

        frame_count = 0
        processed = 0

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            frame_count += 1

            # è·³å¸§å¤„ç†
            if (frame_count - 1) % self.frame_skip != 0:
                continue

            # æ‰‹å·¥é€åƒç´ å˜æ¢
            warped = transform_frame_manual(frame, M, output_size)
            out.write(warped)

            processed += 1
            if processed % 5 == 0:
                print(f"  {processed}/{total_to_process} ({100 * processed / total_to_process:.1f}%)")

        cap.release()
        out.release()

        print(f"âœ“ warpedè§†é¢‘å·²ä¿å­˜: {warped_path.name}")
        print(f"  å¤„ç†å¸§æ•°: {processed}")
        print(f"  è¾“å‡ºFPS: {output_fps:.1f}")
        print(f"  ä¸–ç•Œåæ ‡èŒƒå›´: X=[{min_x:.2f}, {max_x:.2f}]m, Y=[{min_y:.2f}, {max_y:.2f}]m")
        self.warped_video_path = str(warped_path)

        return str(warped_path)

    def detect_and_visualize(self, conf_threshold=0.45):
        """YOLOæ£€æµ‹ + å¯è§†åŒ– + ç»Ÿè®¡æŠ¥å‘Š ç”Ÿæˆï¼š 1. å¸¦æ£€æµ‹æ¡†çš„è§†é¢‘ 2. YOLOç»Ÿè®¡æŠ¥å‘Š 3. å…³é”®å¸§ï¼ˆå¸¦æ£€æµ‹æ¡†ï¼‰."""
        print("\nã€æ­¥éª¤3: YOLOæ£€æµ‹ + å¯è§†åŒ–ã€‘")

        print("åŠ è½½YOLOv11næ¨¡å‹...")
        model = YOLO("yolo11n.pt")

        # æ‰“å¼€warpedè§†é¢‘
        cap = cv2.VideoCapture(self.warped_video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        # åˆ›å»ºå¯è§†åŒ–è§†é¢‘
        viz_output = self.yolo_dir / f"yolo_detection_viz_{self.timestamp}.mp4"
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        viz_out = cv2.VideoWriter(str(viz_output), fourcc, fps, (width, height))

        # ç»Ÿè®¡ä¿¡æ¯
        class_counts = defaultdict(int)
        frames_with_detections = 0
        detected_frames_info = []
        keyframes = []  # ä¿å­˜å…³é”®å¸§

        print(f"å¤„ç†ä¸­: {total_frames}å¸§ (è·³å¸§æ•°: {self.frame_skip})...")

        frame_idx = 0
        processed_frame = 0

        for result in model.track(source=self.warped_video_path, stream=True, persist=True, conf=conf_threshold):
            frame_idx += 1

            # è·³å¸§å¤„ç†
            if (frame_idx - 1) % self.frame_skip != 0:
                continue

            processed_frame += 1
            frame = result.orig_img.copy()

            # è·å–æ£€æµ‹ç»“æœ
            if result.boxes is None or len(result.boxes) == 0:
                # æ— æ£€æµ‹ï¼Œå†™å…¥ç©ºå¸§
                viz_out.write(frame)
                if processed_frame % (30 // self.frame_skip + 1) == 0:
                    print(f"  Frame {processed_frame} - æ— æ£€æµ‹")
                continue

            # æœ‰æ£€æµ‹
            frames_with_detections += 1
            boxes = result.boxes.xywh.cpu().numpy()
            classes = result.boxes.cls.cpu().numpy()
            confidences = result.boxes.conf.cpu().numpy()

            frame_detections = []

            # ç»˜åˆ¶æ£€æµ‹æ¡†å’Œæ ‡ç­¾
            for box, cls_id, conf in zip(boxes, classes, confidences):
                x, y, w, h = box
                x1, y1 = int(x - w / 2), int(y - h / 2)
                x2, y2 = int(x + w / 2), int(y + h / 2)

                class_name = model.names[int(cls_id)]
                class_counts[class_name] += 1

                # ç»˜åˆ¶è¾¹æ¡†
                color = (0, 255, 0)  # ç»¿è‰²
                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)

                # ç»˜åˆ¶æ ‡ç­¾
                label = f"{class_name} {conf:.2f}"
                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

                frame_detections.append({"class": class_name, "confidence": float(conf), "bbox": [x1, y1, x2, y2]})

            # æ·»åŠ å¸§ä¿¡æ¯æ–‡æœ¬
            cv2.putText(
                frame,
                f"Frame: {frame_idx} | Objects: {len(boxes)}",
                (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.7,
                (255, 255, 255),
                2,
            )

            # ä¿å­˜å¸§ä¿¡æ¯
            detected_frames_info.append(
                {"frame": frame_idx, "objects_count": len(boxes), "detections": frame_detections}
            )

            # ä¿å­˜å…³é”®å¸§ï¼ˆæœ‰æ£€æµ‹çš„å¸§ï¼‰
            keyframe_path = self.yolo_dir / f"keyframe_{frame_idx:04d}.jpg"
            cv2.imwrite(str(keyframe_path), frame)
            keyframes.append({"frame": frame_idx, "path": keyframe_path.name, "objects": len(boxes)})

            # å†™å…¥å¯è§†åŒ–è§†é¢‘
            viz_out.write(frame)

            if processed_frame % (30 // self.frame_skip + 1) == 0:
                print(f"  Frame {processed_frame}/{total_frames // self.frame_skip} - {len(boxes)}ä¸ªç‰©ä½“")

        cap.release()
        viz_out.release()

        print("âœ“ YOLOæ£€æµ‹å®Œæˆ")
        print(f"  å¤„ç†å¸§æ•°: {processed_frame}")
        print(f"  æ£€æµ‹åˆ°ç‰©ä½“çš„å¸§æ•°: {frames_with_detections}")
        print(f"  å¯è§†åŒ–è§†é¢‘: {viz_output.name}")
        print(f"  å…³é”®å¸§æ•°: {len(keyframes)}")

        # ç”ŸæˆYOLOç»Ÿè®¡æŠ¥å‘Š
        self.generate_yolo_report(class_counts, frames_with_detections, detected_frames_info, keyframes)

        return detected_frames_info, keyframes

    def generate_yolo_report(self, class_counts, frames_with_detections, detected_frames_info, keyframes):
        """ç”ŸæˆYOLOæ£€æµ‹ç»Ÿè®¡æŠ¥å‘Š."""
        report_path = self.yolo_dir / "yolo_detection_report.txt"

        with open(report_path, "w") as f:
            f.write("=" * 70 + "\n")
            f.write("YOLOç‰©ä½“æ£€æµ‹ç»Ÿè®¡æŠ¥å‘Š\n")
            f.write("=" * 70 + "\n\n")

            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è¾“å…¥è§†é¢‘: {self.video_path}\n")
            f.write(f"å¤„ç†å¸§ç‡è·³å¸§: {self.frame_skip}\n\n")

            f.write("æ£€æµ‹ç»Ÿè®¡:\n")
            f.write(f"  - æ£€æµ‹åˆ°ç‰©ä½“çš„å¸§æ•°: {frames_with_detections}\n")
            f.write(f"  - ç”Ÿæˆçš„å…³é”®å¸§æ•°: {len(keyframes)}\n\n")

            f.write("ç‰©ä½“ç±»åˆ«ç»Ÿè®¡:\n")
            total_objects = sum(class_counts.values())
            for class_name, count in sorted(class_counts.items(), key=lambda x: x[1], reverse=True):
                percentage = 100 * count / total_objects if total_objects > 0 else 0
                f.write(f"  - {class_name}: {count}ä¸ª ({percentage:.1f}%)\n")
            f.write(f"  - æ€»è®¡: {total_objects}ä¸ª\n\n")

            f.write("å…³é”®å¸§åˆ—è¡¨:\n")
            for keyframe in keyframes[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                f.write(f"  Frame {keyframe['frame']}: {keyframe['objects']}ä¸ªç‰©ä½“ - {keyframe['path']}\n")

            if len(keyframes) > 10:
                f.write(f"  ... ä»¥åŠå…¶ä»– {len(keyframes) - 10} ä¸ªå…³é”®å¸§\n")

            f.write("\n" + "=" * 70 + "\n")

        print(f"âœ“ YOLOæŠ¥å‘Šå·²ä¿å­˜: {report_path.name}")

        # æ‰“å°æŠ¥å‘Šåˆ°æ§åˆ¶å°
        with open(report_path) as f:
            print("\n" + f.read())

    def analyze_collisions(self, detected_frames_info):
        """åˆ†æç¢°æ’ï¼ˆè·ç¦»è®¡ç®—ï¼‰."""
        print("\nã€æ­¥éª¤4: ç¢°æ’è·ç¦»åˆ†æã€‘")

        # ä¸–ç•Œåæ ‡èŒƒå›´
        world_bounds = (-3.75, 3.75, 0.0, 50.0)
        actual_size = (640, 96)  # YOLOå®é™…è¾“å…¥å¤§å°

        world_width = world_bounds[1] - world_bounds[0]
        world_height = world_bounds[3] - world_bounds[2]

        collision_events = []
        near_miss_events = []

        print("è®¡ç®—ç‰©ä½“é—´è·ç¦»...")

        for frame_info in detected_frames_info:
            if frame_info["objects_count"] < 2:
                continue

            detections = frame_info["detections"]
            frame_num = frame_info["frame"]

            # è®¡ç®—æ‰€æœ‰ç‰©ä½“å¯¹çš„è·ç¦»
            for i in range(len(detections)):
                for j in range(i + 1, len(detections)):
                    bbox1 = detections[i]["bbox"]
                    bbox2 = detections[j]["bbox"]

                    # è·å–bboxä¸­å¿ƒç‚¹
                    x1 = (bbox1[0] + bbox1[2]) / 2
                    y1 = (bbox1[1] + bbox1[3]) / 2
                    x2 = (bbox2[0] + bbox2[2]) / 2
                    y2 = (bbox2[1] + bbox2[3]) / 2

                    # æ¢ç®—åˆ°ä¸–ç•Œåæ ‡
                    x1_world = world_bounds[0] + (x1 / actual_size[0]) * world_width
                    y1_world = world_bounds[2] + (y1 / actual_size[1]) * world_height
                    x2_world = world_bounds[0] + (x2 / actual_size[0]) * world_width
                    y2_world = world_bounds[2] + (y2 / actual_size[1]) * world_height

                    # è®¡ç®—è·ç¦»
                    distance = np.sqrt((x2_world - x1_world) ** 2 + (y2_world - y1_world) ** 2)

                    # åˆ†ç±»
                    event = {
                        "frame": frame_num,
                        "class1": detections[i]["class"],
                        "class2": detections[j]["class"],
                        "distance": float(distance),
                        "distance_str": f"{distance:.2f}m",
                    }

                    if distance < 0.5:
                        event["level"] = "COLLISION"  # ç¢°æ’
                        collision_events.append(event)
                        print(
                            f"  âš ï¸  COLLISION: Frame {frame_num}, "
                            f"{detections[i]['class']} - {detections[j]['class']}, "
                            f"è·ç¦»: {distance:.2f}m"
                        )
                    elif distance < 1.5:
                        event["level"] = "NEAR_MISS"  # æ¥è¿‘
                        near_miss_events.append(event)

            if frame_info == detected_frames_info[-1] or detected_frames_info.index(frame_info) % 10 == 0:
                frame_events = len([e for e in collision_events + near_miss_events if e["frame"] == frame_num])
                print(f"  Frame {frame_num}: å·²åˆ†æ {frame_events}ä¸ªè·ç¦»")

        print("âœ“ åˆ†æå®Œæˆ:")
        print(f"  - ç¢°æ’äº‹ä»¶: {len(collision_events)}")
        print(f"  - æ¥è¿‘äº‹ä»¶: {len(near_miss_events)}")

        # ä¿å­˜ç»“æœ
        self.save_collision_results(collision_events, near_miss_events)

        return collision_events, near_miss_events

    def save_collision_results(self, collision_events, near_miss_events):
        """ä¿å­˜ç¢°æ’åˆ†æç»“æœ."""
        # ä¿å­˜JSON
        collision_path = self.collision_dir / "collision_events.json"
        with open(collision_path, "w") as f:
            json.dump(collision_events, f, indent=2)

        near_miss_path = self.collision_dir / "near_miss_events.json"
        with open(near_miss_path, "w") as f:
            json.dump(near_miss_events, f, indent=2)

        # ç”ŸæˆæŠ¥å‘Š
        report_path = self.collision_dir / "collision_analysis_report.txt"
        with open(report_path, "w") as f:
            f.write("=" * 70 + "\n")
            f.write("ç¢°æ’åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 70 + "\n\n")

            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("åˆ†æèŒƒå›´: <0.5m (ç¢°æ’), <1.5m (æ¥è¿‘)\n\n")

            f.write("ç»Ÿè®¡ç»“æœ:\n")
            f.write(f"  - ç¢°æ’äº‹ä»¶: {len(collision_events)}\n")
            f.write(f"  - æ¥è¿‘äº‹ä»¶: {len(near_miss_events)}\n\n")

            if collision_events:
                f.write("ç¢°æ’äº‹ä»¶è¯¦æƒ…:\n")
                for i, event in enumerate(collision_events, 1):
                    f.write(f"{i}. Frame {event['frame']}\n")
                    f.write(f"   ç‰©ä½“: {event['class1']} - {event['class2']}\n")
                    f.write(f"   è·ç¦»: {event['distance_str']}\n\n")

            if near_miss_events:
                f.write("\næ¥è¿‘äº‹ä»¶è¯¦æƒ… (å‰10ä¸ª):\n")
                for i, event in enumerate(near_miss_events[:10], 1):
                    f.write(
                        f"{i}. Frame {event['frame']}: "
                        f"{event['class1']}-{event['class2']}, "
                        f"è·ç¦» {event['distance_str']}\n"
                    )

        print(f"âœ“ ç¢°æ’æŠ¥å‘Šå·²ä¿å­˜: {report_path.name}")

    def run(self, conf_threshold=0.45):
        """è¿è¡Œå®Œæ•´pipeline."""
        try:
            # æ­¥éª¤1: åŠ è½½Homography
            self.load_homography()
            self.create_verification_image()
            self.pause_checkpoint("1_homography")

            # æ­¥éª¤2: è§†é¢‘é€è§†å˜æ¢
            self.transform_video()
            self.pause_checkpoint("2_warped_video")

            # æ­¥éª¤3: YOLOæ£€æµ‹
            detected_frames_info, _keyframes = self.detect_and_visualize(conf_threshold)
            self.pause_checkpoint("3_yolo_detection")

            # æ­¥éª¤4: ç¢°æ’åˆ†æ
            if detected_frames_info:
                _collision_events, _near_miss_events = self.analyze_collisions(detected_frames_info)

            print(f"\n{'=' * 70}")
            print("âœ“ Pipelineå®Œæˆï¼")
            print(f"{'=' * 70}")
            print(f"ç»“æœä¿å­˜åœ¨: {self.run_dir}")
            print("\næ–‡ä»¶å¤¹ç»“æ„:")
            print("  1_homography/          - æ ‡å®šéªŒè¯")
            print("  2_warped_video/        - é¸Ÿç°å›¾è§†é¢‘")
            print("  3_yolo_detection/      - YOLOæ£€æµ‹ç»“æœ")
            print("    â”œâ”€â”€ yolo_detection_viz_*.mp4       (å¸¦æ¡†è§†é¢‘)")
            print("    â”œâ”€â”€ yolo_detection_report.txt      (æ£€æµ‹æŠ¥å‘Š)")
            print("    â””â”€â”€ keyframe_*.jpg                 (å…³é”®å¸§)")
            print("  4_collision_analysis/  - ç¢°æ’åˆ†æ")
            print("    â”œâ”€â”€ collision_events.json          (ç¢°æ’äº‹ä»¶)")
            print("    â”œâ”€â”€ near_miss_events.json          (æ¥è¿‘äº‹ä»¶)")
            print("    â””â”€â”€ collision_analysis_report.txt  (åˆ†ææŠ¥å‘Š)")

        except Exception as e:
            print(f"âŒ Pipelineé”™è¯¯: {e}")
            import traceback

            traceback.print_exc()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ç¢°æ’æ£€æµ‹Pipeline - åˆ†é˜¶æ®µè°ƒè¯•ç‰ˆæœ¬")
    parser.add_argument("--video", type=str, required=True, help="è¾“å…¥è§†é¢‘è·¯å¾„")
    parser.add_argument("--homography", type=str, required=True, help="Homography JSONè·¯å¾„")
    parser.add_argument("--output", type=str, default="../../results", help="ç»“æœåŸºç¡€ç›®å½•")
    parser.add_argument("--conf", type=float, default=0.45, help="YOLOç½®ä¿¡åº¦é˜ˆå€¼")
    parser.add_argument("--frame-skip", type=int, default=1, help="è·³å¸§æ•°ï¼ˆ1=ä¸è·³ï¼Œ10=æ¯10å¸§å¤„ç†1å¸§ï¼ŒåŠ å¿«é€Ÿåº¦ï¼‰")
    parser.add_argument("--debug", action="store_true", default=True, help="å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆå„é˜¶æ®µæš‚åœï¼‰")
    parser.add_argument("--no-pause", action="store_true", help="ç¦ç”¨é˜¶æ®µæš‚åœï¼Œè¿ç»­è¿è¡Œ")

    args = parser.parse_args()

    pipeline = DebugCollisionDetectionPipeline(
        args.video, args.homography, args.output, frame_skip=args.frame_skip, debug_mode=(not args.no_pause)
    )
    pipeline.run(args.conf)
