"""
collision_detection_pipeline_yolo_first_method_a.py

YOLO-First ç¢°æ’æ£€æµ‹ç®¡é“ (Method A )
æ‰§è¡Œé¡ºåº: YOLOæ£€æµ‹ â†’ è½¨è¿¹æ„å»º(px) â†’ å…³é”®å¸§æ£€æµ‹ â†’ Homographyå˜æ¢(ä»…å…³é”®å¸§) â†’ TTCåˆ†æ


æµç¨‹:
1. YOLO æ£€æµ‹ (åŸå§‹è§†é¢‘æˆ–è·³å¸§è§†é¢‘)
   - åœ¨åŸå§‹åˆ†è¾¨ç‡ä¸Šæ£€æµ‹æ‰€æœ‰ç‰©ä½“
   - ä¿å­˜æ£€æµ‹æ¡†å’Œ Track ID (åƒç´ ç©ºé—´)

2. è½¨è¿¹æ„å»º (åƒç´ ç©ºé—´)
   - å…³è” Track IDï¼Œå»ºç«‹è½¨è¿¹
   - ä¼°è®¡é€Ÿåº¦ (px/s)
   - æ‰€æœ‰è®¡ç®—åœ¨åƒç´ åæ ‡ç³»

3. å…³é”®å¸§æ£€æµ‹ (æ¥è¿‘äº‹ä»¶è¯†åˆ«)
   - è¯†åˆ«"è·ç¦» < 150px"çš„ç‰©ä½“å¯¹
   - æ ‡è®°ä¸ºå…³é”®å¸§

4. Homography å˜æ¢ (ä»…å…³é”®å¸§) 
   - åªå¯¹å…³é”®å¸§ä¸­çš„ç‰©ä½“ç‚¹åšå˜æ¢
   - è½¬æ¢è·ç¦»å•ä½ (px â†’ m)
   - è½¬æ¢é€Ÿåº¦å•ä½ (px/s â†’ m/s)

5. TTC å’Œ Event åˆ†çº§
   - è®¡ç®— TTC (åœ¨ä¸–ç•Œåæ ‡)
   - åˆ†çº§äº‹ä»¶ (L1/L2/L3)
   - ç”ŸæˆæŠ¥å‘Š

ä¼˜åŠ¿: 
- 
-  æ€§èƒ½æœ€ä¼˜ (ä»…å˜æ¢5-10%çš„æ•°æ®)
-  é€»è¾‘æ¸…æ™° (å…ˆæ‰¾æ¥è¿‘çš„ï¼Œå†ç²¾ç¡®åˆ†æ)
"""

import os
import sys
import json
import cv2
import numpy as np
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# å¯¼å…¥YOLO
sys.path.append(os.path.dirname(__file__))
from ultralytics import YOLO

# å¯¼å…¥å¤šé”šç‚¹ç¢°æ’æ£€æµ‹æ¨¡å—
from anchor_points import VehicleAnchors, PedestrianAnchors, BicycleAnchors, MotorcycleAnchors, get_vehicle_heading
from collision_analyzer import CollisionAnalyzer


class YOLOFirstPipelineA:
    def __init__(self, video_path, homography_path=None, output_base=None, skip_frames=3, model='yolo11n', min_track_length=3):
        """åˆå§‹åŒ– YOLO-First pipeline 
        
        Args:
            video_path: åŸå§‹è§†é¢‘è·¯å¾„
            homography_path: Homography JSONè·¯å¾„ (ç”¨äºStep 4)
            output_base: ç»“æœåŸºç¡€ç›®å½•
            skip_frames: æŠ½å¸§å‚æ•°ï¼Œæ¯éš” skip_frames å¸§å¤„ç†ä¸€å¸§ (æœ€å°å€¼=3ï¼Œç”¨äºæ€§èƒ½ä¼˜åŒ–å’Œé€Ÿåº¦å‡†ç¡®æ€§)
            model: YOLO æ¨¡å‹é€‰æ‹© (yolo11n/yolo11m/yolo11l)
            min_track_length: æœ€å°è½¨è¿¹é•¿åº¦ï¼ŒçŸ­äºæ­¤çš„è¢«è®¤ä¸ºæ˜¯è¯¯æ£€
        """
        self.video_path = video_path
        self.homography_path = homography_path
        # å¼ºåˆ¶skip_framesè‡³å°‘ä¸º3ï¼Œé¿å…å®Œå…¨ä¸è·³å¸§çš„ä½æ•ˆå¤„ç†
        self.skip_frames = max(3, skip_frames)  # å¼ºåˆ¶è‡³å°‘è·³å¸§3
        self.model = model  # YOLO æ¨¡å‹
        self.min_track_length = min_track_length  # æœ€å°è½¨è¿¹é•¿åº¦
        # ä½¿ç”¨ /workspace/ultralytics/results ä½œä¸ºè¾“å‡ºç›®å½•ï¼ˆç¡®ä¿è·¯å¾„æ­£ç¡®ï¼‰
        if output_base is None:
            output_base = "/workspace/ultralytics/results"
        self.output_base = Path(output_base)
        self.H = None
        self.pixel_per_meter = 1.0
        
        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_dir = (self.output_base / f"{timestamp}_yolo_first_method_a").resolve()
        
        # åˆ›å»ºå­ç›®å½•ç»“æ„ (Method A)
        self.detection_dir = self.run_dir / "1_yolo_detection"
        self.trajectory_dir = self.run_dir / "2_trajectories"
        self.keyframe_dir = self.run_dir / "3_key_frames"
        self.homography_dir = self.run_dir / "4_homography_transform"
        self.analysis_dir = self.run_dir / "5_collision_analysis"
        
        for d in [self.detection_dir, self.trajectory_dir, self.keyframe_dir, 
                  self.homography_dir, self.analysis_dir]:
            d.mkdir(parents=True, exist_ok=True)
        
        print(f"\n{'='*70}")
        print(f"YOLO-First ç¢°æ’æ£€æµ‹Pipeline (Method A - å¯¼å¸ˆæ¨è)")
        print(f"{'='*70}")
        print(f"æ—¶é—´æˆ³: {timestamp}")
        print(f"ç»“æœç›®å½•: {self.run_dir}")
        print(f"æ‰§è¡Œé¡ºåº: YOLO â†’ è½¨è¿¹(px) â†’ å…³é”®å¸§ â†’ Homography(å…³é”®å¸§) â†’ TTC")
    
    def load_homography(self):
        """åŠ è½½ Homography çŸ©é˜µ (Step 4 éœ€è¦)"""
        if not self.homography_path:
            print(f"\nâš ï¸  æœªæä¾› Homographyï¼Œå°†ä»…åœ¨åƒç´ ç©ºé—´å¤„ç†")
            return False
        
        try:
            with open(self.homography_path) as f:
                H_data = json.load(f)
            
            self.H = np.array(H_data['homography_matrix'], dtype=np.float32)
            pixel_points = H_data['pixel_points']
            world_points = H_data['world_points']
            
            # ä¿å­˜åˆ°è¾“å‡ºç›®å½•
            with open(self.homography_dir / 'homography.json', 'w') as f:
                json.dump(H_data, f, indent=2)
            
            # è®¡ç®—åƒç´ åˆ°ç±³çš„ç¼©æ”¾å› å­
            if len(world_points) >= 2 and len(pixel_points) >= 2:
                px_dist = np.sqrt((pixel_points[0][0] - pixel_points[1][0])**2 + 
                                 (pixel_points[0][1] - pixel_points[1][1])**2)
                world_dist = np.sqrt((world_points[0][0] - world_points[1][0])**2 + 
                                    (world_points[0][1] - world_points[1][1])**2)
                
                self.pixel_per_meter = px_dist / world_dist if world_dist > 0 else 1.0
            
            print(f"  âœ“ Homographyå·²åŠ è½½")
            print(f"    ç¼©æ”¾å› å­: {self.pixel_per_meter:.2f} px/m")
            
            return True
        
        except Exception as e:
            print(f"  âŒ åŠ è½½Homographyå¤±è´¥: {e}")
            return False
    
    # =========================================================================
    # å¯è§†åŒ–å’Œå›¾åƒä¿å­˜
    # =========================================================================
    
    def save_detection_frame(self, video_path, frame_num, output_path, detections=None):
        """ä¿å­˜æŒ‡å®šå¸§çš„å›¾åƒï¼ˆå¸¦ YOLO æ£€æµ‹æ¡†ï¼‰"""
        cap = cv2.VideoCapture(video_path)
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num - 1)
        ret, frame = cap.read()
        cap.release()
        
        if not ret:
            return False
        
        # å¦‚æœæœ‰æ£€æµ‹æ•°æ®ï¼Œç»˜åˆ¶æ£€æµ‹æ¡†
        if detections:
            for obj in detections.get('objects', []):
                x, y, w, h = obj['bbox_xywh']
                x1, y1 = int(x - w/2), int(y - h/2)
                x2, y2 = int(x + w/2), int(y + h/2)
                
                # ç»˜åˆ¶æ£€æµ‹æ¡†
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
                # ç»˜åˆ¶ Track ID
                track_id = obj['track_id']
                conf = obj['conf']
                text = f"ID:{track_id} ({conf:.2f})"
                cv2.putText(frame, text, (x1, y1-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        # ä¿å­˜å›¾åƒ
        cv2.imwrite(str(output_path), frame)
        return True
    
    def save_keyframe_with_distance(self, video_path, frame_num, output_path, proximity_event):
        """ä¿å­˜å…³é”®å¸§å›¾åƒï¼ˆç»˜åˆ¶ä¸¤ä¸ªæ¥è¿‘çš„ç‰©ä½“ã€è·ç¦»ã€å¤šé”šç‚¹ç¢°æ’ç‚¹ï¼‰"""
        cap = cv2.VideoCapture(video_path)
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num - 1)
        ret, frame = cap.read()
        cap.release()
        
        if not ret:
            return False
        
        # è·å–ä¸¤ä¸ªç‰©ä½“çš„ä¿¡æ¯
        center_1 = proximity_event.get('center_1_px', proximity_event.get('center_1', [0, 0]))
        center_2 = proximity_event.get('center_2_px', proximity_event.get('center_2', [0, 0]))
        track_id_1 = proximity_event.get('track_id_1', -1)
        track_id_2 = proximity_event.get('track_id_2', -1)
        distance_pixel = proximity_event.get('distance_pixel', 0)
        distance_meters = proximity_event.get('distance_meters', 0)
        class_1 = proximity_event.get('class_1', 'Unknown')
        class_2 = proximity_event.get('class_2', 'Unknown')
        
        # ç»˜åˆ¶ä¸¤ä¸ªç‰©ä½“çš„ä¸­å¿ƒç‚¹
        pt1 = tuple(map(int, center_1))
        pt2 = tuple(map(int, center_2))
        
        # ç»˜åˆ¶åœ†ç‚¹å’ŒID
        cv2.circle(frame, pt1, 5, (0, 255, 0), -1)  # ç»¿è‰²åœ†ç‚¹
        cv2.putText(frame, f"ID:{track_id_1}", (pt1[0]+10, pt1[1]-10),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        cv2.circle(frame, pt2, 5, (0, 0, 255), -1)  # çº¢è‰²åœ†ç‚¹
        cv2.putText(frame, f"ID:{track_id_2}", (pt2[0]+10, pt2[1]-10),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        
        # ç»˜åˆ¶è¿æ¥çº¿
        cv2.line(frame, pt1, pt2, (255, 0, 0), 2)  # è“è‰²çº¿
        
        # ========== å¤šé”šç‚¹ç¢°æ’å¯è§†åŒ– ==========
        # å¦‚æœæœ‰å¤šé”šç‚¹åˆ†æç»“æœï¼Œç»˜åˆ¶æœ€è¿‘ç¢°æ’ç‚¹
        if 'multi_anchor_detailed' in proximity_event:
            try:
                multi_anchor = proximity_event['multi_anchor_detailed']
                closest_parts = multi_anchor.get('closest_parts', {})
                
                point1_px = closest_parts.get('point1_px')
                point2_px = closest_parts.get('point2_px')
                
                if point1_px and point2_px:
                    # ç¡®ä¿åæ ‡æ˜¯æ•´æ•°
                    anchor_pt1 = tuple(map(int, point1_px))
                    anchor_pt2 = tuple(map(int, point2_px))
                    
                    # ç»˜åˆ¶æœ€è¿‘ç¢°æ’ç‚¹ï¼šå¤§åœ†åœˆï¼ˆç»¿/çº¢ï¼‰
                    cv2.circle(frame, anchor_pt1, 12, (0, 255, 0), 2)  # ç»¿è‰²å¤§åœ†åœˆ (Object 1)
                    cv2.circle(frame, anchor_pt2, 12, (0, 0, 255), 2)  # çº¢è‰²å¤§åœ†åœˆ (Object 2)
                    
                    # ç»˜åˆ¶æœ€è¿‘ç¢°æ’ç‚¹ä¹‹é—´çš„è¿çº¿ï¼ˆç´«è‰²ï¼‰
                    cv2.line(frame, anchor_pt1, anchor_pt2, (255, 0, 255), 2)
                    
                    # æ˜¾ç¤ºé”šç‚¹åç§°å’Œè·ç¦»
                    obj1_part = closest_parts.get('object1_part', '?')
                    obj2_part = closest_parts.get('object2_part', '?')
                    min_dist_m = multi_anchor.get('min_distance_meters', 0)
                    risk_level = multi_anchor.get('risk_level', 'UNKNOWN')
                    ttc = multi_anchor.get('ttc_seconds')
                    
                    # åœ¨è¿æ¥çº¿ä¸­ç‚¹æ˜¾ç¤ºè·ç¦»å’Œé£é™©ç­‰çº§
                    mid_x = (anchor_pt1[0] + anchor_pt2[0]) // 2
                    mid_y = (anchor_pt1[1] + anchor_pt2[1]) // 2
                    
                    # è·ç¦»ä¿¡æ¯
                    dist_text = f"Anchor: {min_dist_m:.2f}m"
                    cv2.putText(frame, dist_text, (mid_x-80, mid_y-30),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)
                    
                    # é£é™©ç­‰çº§å’Œé¢œè‰²
                    risk_color_map = {
                        'CRITICAL': (0, 0, 255),    # Red
                        'HIGH': (0, 165, 255),       # Orange
                        'MEDIUM': (0, 255, 255),     # Yellow
                        'LOW': (0, 255, 0),          # Green
                    }
                    risk_color = risk_color_map.get(risk_level, (255, 255, 255))
                    cv2.putText(frame, f"Risk: {risk_level}", (mid_x-80, mid_y+10),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, risk_color, 2)
                    
                    # TTCä¿¡æ¯
                    if ttc is not None:
                        ttc_text = f"TTC: {ttc:.2f}s" if ttc > 0 else "TTC: CRITICAL"
                        cv2.putText(frame, ttc_text, (mid_x-80, mid_y+50),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, risk_color, 2)
                    
                    # ç¢°æ’éƒ¨åˆ†æ ‡æ³¨
                    cv2.putText(frame, obj1_part, (anchor_pt1[0]-50, anchor_pt1[1]-20),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
                    cv2.putText(frame, obj2_part, (anchor_pt2[0]+20, anchor_pt2[1]-20),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
            except Exception as e:
                # å¦‚æœå¤šé”šç‚¹å¯è§†åŒ–å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨ç®€å•çš„ä¸­å¿ƒç‚¹å¯è§†åŒ–
                pass
        
        # æ˜¾ç¤ºè·ç¦»ä¿¡æ¯ (åƒç´ å’Œä¸–ç•Œåæ ‡)
        mid_x = (pt1[0] + pt2[0]) // 2
        mid_y = (pt1[1] + pt2[1]) // 2
        distance_text = f"Center Distance: {distance_meters:.2f}m ({distance_pixel:.0f}px)"
        cv2.putText(frame, distance_text, (mid_x-130, mid_y+30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
        
        # æ˜¾ç¤ºç‰©ä½“ç±»åˆ«ä¿¡æ¯
        class_text = f"{class_1} vs {class_2}"
        cv2.putText(frame, class_text, (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
        
        # ä¿å­˜å›¾åƒ
        cv2.imwrite(str(output_path), frame)
        return True

    
    # =========================================================================
    # STEP 1: YOLO æ£€æµ‹ (åƒç´ ç©ºé—´)
    # =========================================================================
    
    def run_yolo_detection(self, conf_threshold=0.45):
        """Step 1: YOLO æ£€æµ‹ (åŸå§‹è§†é¢‘æˆ–è·³å¸§è§†é¢‘)
        
        è¾“å‡º:
        - ä¿å­˜æ‰€æœ‰æ£€æµ‹æ¡†å’Œ Track ID (åƒç´ ç©ºé—´)
        - ç”Ÿæˆæ£€æµ‹ç»Ÿè®¡
        """
        print(f"\nã€Step 1: YOLO æ£€æµ‹ã€‘")
        
        # åŠ è½½æŒ‡å®šçš„ YOLO æ¨¡å‹
        model = YOLO(f'{self.model}.pt')
        print(f"  åŠ è½½æ¨¡å‹: {self.model}.pt")
        
        cap = cv2.VideoCapture(self.video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        all_detections = []
        frame_count = 0
        detection_frames_count = 0
        
        # è®¡ç®—è·³å¸§åçš„å¤„ç†å¸§æ•°
        expected_processing_frames = (total_frames + self.skip_frames - 1) // self.skip_frames
        
        # æŠ½å¸§å¤„ç†
        if self.skip_frames > 1:
            print(f"  å¤„ç†ä¸­: å°†å¤„ç† ~{expected_processing_frames} å¸§ (ä»æ€»å…± {total_frames}å¸§ä¸­ï¼Œæ¯éš”{self.skip_frames}å¸§å¤„ç†ä¸€å¸§)...")
        else:
            print(f"  å¤„ç†ä¸­: {total_frames}å¸§ @ {fps:.2f}FPS...")
        
        # å¦‚æœéœ€è¦è·³å¸§ï¼Œå…ˆæ”¶é›†è¦å¤„ç†çš„å¸§
        frames_to_process = []
        if self.skip_frames > 1:
            # åªè¯»å–éœ€è¦å¤„ç†çš„å¸§
            for frame_idx in range(0, total_frames, self.skip_frames):
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if ret:
                    frames_to_process.append((frame_idx + 1, frame))  # frame_idx+1 because frames are 1-indexed
            cap.release()
            
            print(f"    âœ“ å·²åŠ è½½{len(frames_to_process)}å¸§è¿›è¡Œå¤„ç†")
            
            # ç”¨YOLOå¤„ç†è¿™äº›å¸§
            for frame_num, frame_img in frames_to_process:
                results = model.track(source=frame_img, persist=True, conf=conf_threshold)
                
                for result in results:
                    frame_count = frame_num
                    
                    if result.boxes is None or len(result.boxes) == 0:
                        if frame_num % 30 == 0:
                            print(f"    Frame {frame_num}/{total_frames} - æ— ç‰©ä½“")
                        continue
                    
                    detection_frames_count += 1
                    boxes = result.boxes.xywh.cpu().numpy()
                    ids = result.boxes.id
                    classes = result.boxes.cls.cpu().numpy().astype(int)
                    confs = result.boxes.conf.cpu().numpy()
                    
                    frame_detections = {
                        'frame': frame_count,
                        'time': frame_count / fps,
                        'objects': []
                    }
                    
                    # åªæœ‰åœ¨æœ‰æ£€æµ‹åˆ°å¯¹è±¡æ—¶æ‰å¤„ç†
                    if len(boxes) > 0 and ids is not None:
                        for i in range(len(boxes)):
                            obj_data = {
                                'track_id': int(ids[i]) if ids[i] is not None else -1,
                                'class': int(classes[i]),
                                'conf': float(confs[i]),
                                'bbox_xywh': boxes[i].tolist(),  # [x_center, y_center, w, h] åƒç´ 
                            }
                            frame_detections['objects'].append(obj_data)
                        
                        # ä¿å­˜æ£€æµ‹æ¡†å›¾åƒï¼ˆæ¯ä¸ªæœ‰æ£€æµ‹çš„å¸§ï¼‰
                        frame_img_path = self.detection_dir / f"frame_{frame_count:04d}.jpg"
                        cv2.imwrite(str(frame_img_path), frame_img)
                    
                    all_detections.append(frame_detections)
                    
                    if frame_num % 30 == 0:
                        print(f"    Frame {frame_num}/{total_frames} - {len(boxes)}ä¸ªç‰©ä½“")
        else:
            # ä¸è·³å¸§ï¼šå¤„ç†æ‰€æœ‰å¸§
            for result in model.track(source=self.video_path, stream=True, 
                                     persist=True, conf=conf_threshold):
                frame_count += 1
                
                if result.boxes is None or len(result.boxes) == 0:
                    if frame_count % 30 == 0:
                        print(f"    Frame {frame_count}/{total_frames} - æ— ç‰©ä½“")
                    continue
                
                detection_frames_count += 1
                boxes = result.boxes.xywh.cpu().numpy()
                ids = result.boxes.id
                classes = result.boxes.cls.cpu().numpy().astype(int)
                confs = result.boxes.conf.cpu().numpy()
                
                frame_detections = {
                    'frame': frame_count,
                    'time': frame_count / fps,
                    'objects': []
                }
                
                # åªæœ‰åœ¨æœ‰æ£€æµ‹åˆ°å¯¹è±¡æ—¶æ‰å¤„ç†
                if len(boxes) > 0 and ids is not None:
                    for i in range(len(boxes)):
                        obj_data = {
                            'track_id': int(ids[i]) if ids[i] is not None else -1,
                            'class': int(classes[i]),
                            'conf': float(confs[i]),
                            'bbox_xywh': boxes[i].tolist(),  # [x_center, y_center, w, h] åƒç´ 
                        }
                        frame_detections['objects'].append(obj_data)
                    
                    # ä¿å­˜æ£€æµ‹æ¡†å›¾åƒï¼ˆæ¯ä¸ªæœ‰æ£€æµ‹çš„å¸§ï¼‰
                    frame_img_path = self.detection_dir / f"frame_{frame_count:04d}.jpg"
                    self.save_detection_frame(self.video_path, frame_count, frame_img_path, frame_detections)
                
                all_detections.append(frame_detections)
                
                if frame_count % 30 == 0:
                    print(f"    Frame {frame_count}/{total_frames} - {len(boxes)}ä¸ªç‰©ä½“")
            
            cap.release()

        
        # ä¿å­˜åŸå§‹æ£€æµ‹ç»“æœ (åƒç´ ç©ºé—´)
        detections_path = self.detection_dir / 'detections_pixel.json'
        with open(detections_path, 'w') as f:
            json.dump(all_detections, f, indent=2)
        
        # ç”Ÿæˆæ£€æµ‹ç»Ÿè®¡
        stats = {
            'total_frames': total_frames,
            'fps': fps,
            'detection_frames': detection_frames_count,
            'confidence_threshold': conf_threshold,
        }
        stats_path = self.detection_dir / 'detection_stats.json'
        with open(stats_path, 'w') as f:
            json.dump(stats, f, indent=2)
        
        print(f"  âœ“ Step 1å®Œæˆ: {detection_frames_count}å¸§æ£€æµ‹åˆ°ç‰©ä½“")
        print(f"    è¾“å‡º: {detections_path.name}")
        
        return all_detections
    
    # =========================================================================
    # STEP 1.5: åŒå¸§å†…ç‰©ä½“åˆ†å‰²åˆå¹¶ (å¤„ç†YOLOæŠŠä¸€ä¸ªç‰©ä½“åˆ†æˆä¸¤ä¸ªçš„é—®é¢˜)
    # =========================================================================
    
    def merge_fragmented_objects_in_frame(self, all_detections, same_class_distance_threshold=200):
        """åœ¨åŒä¸€å¸§å†…ï¼Œåˆå¹¶è¢«åˆ†å‰²çš„åŒç±»ç‰©ä½“
        
        åŸç†ï¼š
        - YOLOæœ‰æ—¶ä¼šæŠŠä¸€ä¸ªç‰©ä½“æ£€æµ‹æˆå¤šä¸ªï¼ˆæ¯”å¦‚æ‘©æ‰˜è½¦çš„å‰åéƒ¨åˆ†ï¼‰
        - åœ¨åŒä¸€å¸§å†…ï¼Œå¦‚æœä¸¤ä¸ªç‰©ä½“ï¼š
          1. ç±»åˆ«ç›¸åŒï¼ˆéƒ½æ˜¯motorcycleï¼‰
          2. ä¸­å¿ƒè·ç¦» < same_class_distance_threshold (åƒç´ )
          3. åˆ™è®¤ä¸ºæ˜¯åŒä¸€ç‰©ä½“è¢«åˆ†å‰²ï¼Œåˆå¹¶å®ƒä»¬
        - ä¿ç•™ç½®ä¿¡åº¦æ›´é«˜çš„é‚£ä¸ªï¼Œåˆ é™¤ç½®ä¿¡åº¦ä½çš„
        """
        print(f"\nã€Step 1.5: åŒå¸§å†…ç‰©ä½“åˆ†å‰²åˆå¹¶ã€‘")
        print(f"  â„¹ï¸  åœ¨æ¯ä¸€å¸§å†…æ£€æµ‹å’Œåˆå¹¶è¢«åˆ†å‰²çš„åŒç±»ç‰©ä½“")
        
        merged_count = 0
        
        for frame_data in all_detections:
            frame = frame_data['frame']
            objects = frame_data['objects']
            
            # è®°å½•å“ªäº›ç‰©ä½“åº”è¯¥è¢«åˆ é™¤ï¼ˆå› ä¸ºè¢«åˆå¹¶äº†ï¼‰
            to_remove = set()
            
            # æ£€æŸ¥æ‰€æœ‰ç‰©ä½“å¯¹
            for i, obj1 in enumerate(objects):
                if i in to_remove:
                    continue
                
                for j, obj2 in enumerate(objects):
                    if j <= i or j in to_remove:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åŒç±»åˆ«ä¸”è·ç¦»è¿‘
                    if obj1['class'] == obj2['class']:
                        x1, y1 = obj1['bbox_xywh'][0], obj1['bbox_xywh'][1]
                        x2, y2 = obj2['bbox_xywh'][0], obj2['bbox_xywh'][1]
                        
                        distance = np.sqrt((x2-x1)**2 + (y2-y1)**2)
                        
                        if distance < same_class_distance_threshold:
                            # åˆå¹¶ï¼šä¿ç•™ç½®ä¿¡åº¦é«˜çš„ï¼Œåˆ é™¤ç½®ä¿¡åº¦ä½çš„
                            if obj1['conf'] >= obj2['conf']:
                                to_remove.add(j)
                                merged_count += 1
                                class_name = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 
                                             4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck'}.get(obj1['class'], f"class_{obj1['class']}")
                                print(f"  ğŸ”€ åˆå¹¶: Frame {frame:03d} - ä¸¤ä¸ª {class_name} (ID {obj1['track_id']}, ID {obj2['track_id']}) è·ç¦» {distance:.1f}px < {same_class_distance_threshold}px")
                            else:
                                to_remove.add(i)
                                merged_count += 1
                                class_name = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 
                                             4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck'}.get(obj2['class'], f"class_{obj2['class']}")
                                print(f"  ğŸ”€ åˆå¹¶: Frame {frame:03d} - ä¸¤ä¸ª {class_name} (ID {obj2['track_id']}, ID {obj1['track_id']}) è·ç¦» {distance:.1f}px < {same_class_distance_threshold}px")
            
            # åˆ é™¤è¢«åˆå¹¶çš„ç‰©ä½“
            frame_data['objects'] = [obj for i, obj in enumerate(objects) if i not in to_remove]
        
        print(f"  âœ“ Step 1.5 å®Œæˆ: åˆå¹¶äº† {merged_count} ä¸ªåŒå¸§å†…çš„åˆ†å‰²ç‰©ä½“")
        
        return all_detections
    
    # =========================================================================
    # =========================================================================
    
    def build_trajectories(self, all_detections):
        """Step 2: æ„å»ºè½¨è¿¹ (åƒç´ ç©ºé—´ + ä¸–ç•Œåæ ‡ï¼Œpx/s + m/s)
        
        è¾“å…¥: åŸå§‹æ£€æµ‹ç»“æœ
        è¾“å‡º: å®Œæ•´è½¨è¿¹ (æŒ‰ track_id ç»„ç»‡ï¼ŒåŒæ—¶åŒ…å«åƒç´ å’Œä¸–ç•Œåæ ‡)
        
        Option B: åœ¨è½¨è¿¹æ„å»ºæ—¶å°±è¿›è¡ŒHomographyè½¬æ¢ï¼Œåç»­ç›´æ¥ä½¿ç”¨ä¸–ç•Œåæ ‡
        """
        print(f"\nã€Step 2: è½¨è¿¹æ„å»º (åƒç´ ç©ºé—´ + ä¸–ç•Œåæ ‡)ã€‘")
        
        # æŒ‰ track_id ç»„ç»‡è½¨è¿¹
        tracks = {}
        
        for frame_data in all_detections:
            for obj in frame_data['objects']:
                track_id = obj['track_id']
                
                if track_id not in tracks:
                    tracks[track_id] = []
                
                # è·å–åƒç´ åæ ‡
                center_x_px = obj['bbox_xywh'][0]
                center_y_px = obj['bbox_xywh'][1]
                
                # è½¬æ¢åˆ°ä¸–ç•Œåæ ‡ (å¦‚æœæœ‰HomographyçŸ©é˜µ)
                center_x_world = center_x_px
                center_y_world = center_y_px
                if self.H is not None:
                    pts_px = np.array([[center_x_px, center_y_px]], dtype=np.float32)
                    pts_world = cv2.perspectiveTransform(pts_px.reshape(1, 1, 2), self.H)
                    center_x_world = pts_world[0, 0, 0]
                    center_y_world = pts_world[0, 0, 1]
                
                # è½¨è¿¹ç‚¹ (åƒç´ ç©ºé—´ + ä¸–ç•Œåæ ‡)
                track_point = {
                    'frame': frame_data['frame'],
                    'time': frame_data['time'],
                    'class': obj['class'],
                    'conf': obj['conf'],
                    # åƒç´ åæ ‡
                    'center_x': float(center_x_px),
                    'center_y': float(center_y_px),
                    # ä¸–ç•Œåæ ‡ (Option Bæ–°å¢) - è½¬æ¢ä¸ºPython floatä»¥ä¾¿JSONåºåˆ—åŒ–
                    'center_x_world': float(center_x_world),
                    'center_y_world': float(center_y_world),
                }
                
                tracks[track_id].append(track_point)
        
        # è®¡ç®—æ¯ä¸ªè½¨è¿¹çš„é€Ÿåº¦ä¿¡æ¯ (px/s å’Œ m/s)
        for track_id, track_points in tracks.items():
            track_points.sort(key=lambda p: p['frame'])
            
            if len(track_points) >= 2:
                for i in range(1, len(track_points)):
                    prev = track_points[i-1]
                    curr = track_points[i]
                    
                    dt = curr['time'] - prev['time']
                    if dt > 0:
                        # åƒç´ ç©ºé—´é€Ÿåº¦
                        dx = curr['center_x'] - prev['center_x']
                        dy = curr['center_y'] - prev['center_y']
                        curr['vx'] = dx / dt  # px/s
                        curr['vy'] = dy / dt  # px/s
                        curr['speed'] = np.sqrt(dx**2 + dy**2) / dt  # px/s
                        
                        # ä¸–ç•Œåæ ‡é€Ÿåº¦ (Option Bæ–°å¢)
                        dx_world = curr['center_x_world'] - prev['center_x_world']
                        dy_world = curr['center_y_world'] - prev['center_y_world']
                        curr['vx_world'] = dx_world / dt  # m/s
                        curr['vy_world'] = dy_world / dt  # m/s
                        curr['speed_world'] = np.sqrt(dx_world**2 + dy_world**2) / dt  # m/s
                    else:
                        curr['vx'] = 0.0
                        curr['vy'] = 0.0
                        curr['speed'] = 0.0
                        curr['vx_world'] = 0.0
                        curr['vy_world'] = 0.0
                        curr['speed_world'] = 0.0
                
                track_points[0]['vx'] = 0.0
                track_points[0]['vy'] = 0.0
                track_points[0]['speed'] = 0.0
                track_points[0]['vx_world'] = 0.0
                track_points[0]['vy_world'] = 0.0
                track_points[0]['speed_world'] = 0.0
        
        # ä¿å­˜è½¨è¿¹
        tracks_path = self.trajectory_dir / 'tracks.json'
        with open(tracks_path, 'w') as f:
            json.dump(tracks, f, indent=2)
        
        # ç”Ÿæˆè½¨è¿¹ç»Ÿè®¡
        stats = {
            'total_tracks': len(tracks),
            'track_lengths': {str(tid): len(points) for tid, points in tracks.items()},
            'coordinate_system': 'pixel + world',
            'velocity_unit': 'px/s (pixel) + m/s (world)',
        }
        stats_path = self.trajectory_dir / 'track_stats.json'
        with open(stats_path, 'w') as f:
            json.dump(stats, f, indent=2)
        
        print(f"  âœ“ Step 2å®Œæˆ: {len(tracks)}æ¡è½¨è¿¹ (åƒç´ ç©ºé—´ + ä¸–ç•Œåæ ‡)")
        print(f"    åæ ‡ç³»ç»Ÿ: åƒç´  + ä¸–ç•Œåæ ‡ (å·²åœ¨Step 2è½¬æ¢)")
        print(f"    é€Ÿåº¦å•ä½: px/s (åƒç´ ) + m/s (ä¸–ç•Œ)")
        print(f"    è¾“å‡º: {tracks_path.name}")
        
        return tracks
    
    # =========================================================================
    # STEP 2.4: è½¨è¿¹é—´æ–­æ£€æµ‹ (è¯†åˆ«å‡ºç°â†’æ¶ˆå¤±â†’é‡æ–°å‡ºç°çš„å¯ç–‘è½¨è¿¹)
    # =========================================================================
    
    def detect_discontinuous_tracks(self, all_detections, max_gap_frames=None):
        """æ£€æµ‹è½¨è¿¹é—´æ–­ (å‡ºç°â†’æ¶ˆå¤±â†’é‡æ–°å‡ºç°)
        
        åŸç†ï¼š
        - åŒä¸€ä¸ª Track ID åœ¨æ—¶é—´åºåˆ—ä¸­å‡ºç°äº†é—´æ–­
        - æ¯”å¦‚ Track ID 13 åœ¨ Frame 82-91 å‡ºç°ï¼ŒFrame 92-93 æ¶ˆå¤±ï¼ŒFrame 94 é‡æ–°å‡ºç°
        - è¿™å¾ˆä¸åˆç†ï¼Œé™¤éç‰©ä½“çœŸçš„ç¦»å¼€äº†è§†é‡ï¼ˆæå°‘è§ï¼‰
        - æ›´å¯èƒ½æ˜¯è¿½è¸ªå¤±è´¥æˆ–è¯¯æ£€å¯¼è‡´çš„å¹½çµè½¨è¿¹
        
        æ³¨æ„ï¼šè€ƒè™‘ frame skipping çš„å½±å“
        - å¦‚æœç”¨ --skip-frames 3ï¼Œæ£€æµ‹çš„æ˜¯æ¯éš”3å¸§çš„æƒ…å†µ
        - æ‰€ä»¥"é—´æ–­"çš„å®šä¹‰åº”è¯¥ç›¸å¯¹å®½æ¾
        
        è¿”å›ï¼š
        - suspicious_track_ids: åŒ…å«çœŸæ­£çš„é—´æ–­çš„ Track ID é›†åˆ
        """
        print(f"\nã€Step 2.4: è½¨è¿¹é—´æ–­æ£€æµ‹ã€‘")
        print(f"  â„¹ï¸  æ£€æµ‹è½¨è¿¹é—´æ–­ï¼ˆå‡ºç°â†’æ¶ˆå¤±â†’é‡æ–°å‡ºç°ï¼Œè€ƒè™‘frame skippingï¼‰")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œæ ¹æ® skip_frames è®¾ç½®é»˜è®¤å€¼
        if max_gap_frames is None:
            max_gap_frames = self.skip_frames * 3  # å…è®¸æœ€å¤š skip_frames*3 çš„é—´æ–­
        
        # ä¸ºæ¯ä¸ª Track ID è®°å½•å®ƒå‡ºç°çš„æ‰€æœ‰å¸§
        track_frames = {}  # {track_id: [frame_nums]}
        
        for frame_data in all_detections:
            frame = frame_data['frame']
            for obj in frame_data['objects']:
                track_id = obj['track_id']
                if track_id not in track_frames:
                    track_frames[track_id] = []
                track_frames[track_id].append(frame)
        
        # æ£€æŸ¥æ¯ä¸ªè½¨è¿¹çš„è¿ç»­æ€§
        suspicious_tracks = {}  # {track_id: 'gap_info'}
        
        for track_id, frames in track_frames.items():
            frames = sorted(set(frames))  # å»é‡å¹¶æ’åº
            
            if len(frames) < 2:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰çœŸæ­£çš„é—´æ–­ï¼ˆä¸æ˜¯å› ä¸º frame skipping å¯¼è‡´çš„ï¼‰
            gaps = []
            for i in range(1, len(frames)):
                gap = frames[i] - frames[i-1]
                # åªæœ‰é—´æ–­ > skip_frames æ—¶æ‰ç®—çœŸæ­£çš„é—´æ–­
                if gap > self.skip_frames + 1:  # å…è®¸1å¸§çš„åå·®
                    gaps.append((frames[i-1], frames[i], gap))
            
            if gaps:
                # æœ‰çœŸæ­£é—´æ–­çš„è½¨è¿¹
                suspicious_tracks[track_id] = {
                    'total_frames': len(frames),
                    'first_frame': frames[0],
                    'last_frame': frames[-1],
                    'gaps': gaps
                }
        
        # æ‰“å°å¯ç–‘è½¨è¿¹ä¿¡æ¯
        if suspicious_tracks:
            print(f"  âš ï¸  æ£€æµ‹åˆ° {len(suspicious_tracks)} æ¡æœ‰çœŸæ­£é—´æ–­çš„è½¨è¿¹ï¼ˆå¯èƒ½æ˜¯è¯¯æ£€æˆ–è¿½è¸ªå¤±è´¥ï¼‰:")
            for track_id, info in sorted(suspicious_tracks.items()):
                print(f"     - Track ID {track_id}: {info['total_frames']}å¸§ ({info['first_frame']}-{info['last_frame']})")
                for gap_start, gap_end, gap_size in info['gaps']:
                    print(f"       â””â”€ é—´æ–­: Frame {gap_start} â†’ Frame {gap_end} (é—´éš” {gap_size} å¸§ï¼Œè¶…è¿‡å…è®¸çš„ {self.skip_frames + 1} å¸§)")
        else:
            print(f"  âœ“ æ²¡æœ‰æ£€æµ‹åˆ°çœŸæ­£çš„é—´æ–­è½¨è¿¹ï¼ˆè€ƒè™‘äº† skip_frames={self.skip_frames} çš„å½±å“ï¼‰")
        
        print(f"  â„¹ï¸  æ³¨æ„: é—´æ–­é˜ˆå€¼ = {self.skip_frames + 1} å¸§ï¼ˆåŸºäº skip_frames å‚æ•°ï¼‰")
        
        return suspicious_tracks
    
    # =========================================================================
    # STEP 2.5: è½¨è¿¹è¿ç»­æ€§è¿‡æ»¤ (æ’é™¤çŸ­è½¨è¿¹è¯¯æ£€)
    # =========================================================================
    
    def filter_short_tracks(self, all_detections, min_track_length=3):
        """è¿‡æ»¤çŸ­è½¨è¿¹ (å¯èƒ½æ˜¯ YOLO è¯¯æ£€)
        
        å¦‚æœä¸€ä¸ª Track ID åªå‡ºç°åœ¨å°‘äº min_track_length å¸§ä¸­ï¼Œ
        åˆ™è®¤ä¸ºæ˜¯è¯¯æ£€ï¼Œå°†å…¶ä»æ£€æµ‹ç»“æœä¸­ç§»é™¤ã€‚
        
        åŸç†ï¼š
        - çœŸå®ç‰©ä½“åº”è¯¥åœ¨è¿ç»­çš„å¤šä¸ªå¸§ä¸­è¢«æ£€æµ‹åˆ°
        - å¦‚æœç‰©ä½“çªç„¶å‡ºç°åˆæ¶ˆå¤±ï¼Œé€šå¸¸æ˜¯ YOLO çš„è¯¯æ£€
        """
        print(f"\nã€Step 2.5: è½¨è¿¹è¿ç»­æ€§è¿‡æ»¤ã€‘")
        
        # ç»Ÿè®¡æ¯ä¸ª Track ID çš„å‡ºç°å¸§æ•°
        track_lengths = {}
        for frame_data in all_detections:
            for obj in frame_data['objects']:
                track_id = obj['track_id']
                track_lengths[track_id] = track_lengths.get(track_id, 0) + 1
        
        # æ‰¾å‡ºçŸ­è½¨è¿¹ (å¯èƒ½æ˜¯è¯¯æ£€)
        short_tracks = {tid: length for tid, length in track_lengths.items() if length < min_track_length}
        valid_tracks = {tid: length for tid, length in track_lengths.items() if length >= min_track_length}
        
        print(f"  è½¨è¿¹é•¿åº¦ç»Ÿè®¡:")
        print(f"    - æ€»è½¨è¿¹: {len(track_lengths)}")
        print(f"    - æœ‰æ•ˆè½¨è¿¹ (>= {min_track_length}å¸§): {len(valid_tracks)}")
        print(f"    - çŸ­è½¨è¿¹/è¯¯æ£€ (< {min_track_length}å¸§): {len(short_tracks)}")
        
        if short_tracks:
            print(f"  ğŸ—‘ï¸  ç§»é™¤çš„çŸ­è½¨è¿¹:")
            for tid, length in sorted(short_tracks.items(), key=lambda x: x[1]):
                print(f"     - Track ID {tid}: {length} å¸§")
        
        # è¿‡æ»¤æ£€æµ‹ç»“æœï¼Œç§»é™¤çŸ­è½¨è¿¹ä¸­çš„ç‰©ä½“
        filtered_detections = []
        for frame_data in all_detections:
            new_frame_data = frame_data.copy()
            new_frame_data['objects'] = [
                obj for obj in frame_data['objects']
                if obj['track_id'] in valid_tracks
            ]
            filtered_detections.append(new_frame_data)
        
        print(f"  âœ“ Step 2.5 å®Œæˆ: ç§»é™¤äº† {len(short_tracks)} æ¡çŸ­è½¨è¿¹")
        
        return filtered_detections
    
    # =========================================================================
    # STEP 2.6: Track ID é‡è¿æ£€æµ‹ (åŒä¸€ç‰©ä½“å¤šä¸ªIDåˆå¹¶)
    # =========================================================================
    
    def merge_fragmented_tracks(self, all_detections, max_gap_frames=2, max_distance_pixels=100):
        """åˆå¹¶è¢«æ–­å¼€çš„Track ID (åŒä¸€ç‰©ä½“è¿½è¸ªå¤±è´¥å¯¼è‡´çš„é‡å¤ID)
        
        åŸç†ï¼š
        - å¦‚æœ Track ID A åœ¨æŸå¸§æ¶ˆå¤±
        - ç„¶ååœ¨ max_gap_frames å¸§å†…ï¼Œä¸€ä¸ªæ–°çš„ Track ID B å‡ºç°
        - ä¸”ä¸¤ä¸ªIDçš„ç‰©ä½“ä¸­å¿ƒè·ç¦» < max_distance_pixels
        - åˆ™è®¤ä¸ºæ˜¯åŒä¸€ç‰©ä½“ï¼Œåº”è¯¥åˆå¹¶ID
        
        è¿™ç‰¹åˆ«é€‚åˆè§£å†³æ‘©æ‰˜è½¦è¢«åˆ†æˆID13å’ŒID15çš„é—®é¢˜ã€‚
        """
        print(f"\nã€Step 2.6: Track ID é‡è¿æ£€æµ‹ã€‘")
        print(f"  â„¹ï¸  æ£€æµ‹å¹¶åˆå¹¶è¢«æ–­å¼€çš„è½¨è¿¹ (æ¶ˆå¤±<{max_gap_frames}å¸§åé‡æ–°å‡ºç°)")
        
        # ç¬¬ä¸€æ­¥ï¼šä¸ºæ¯ä¸ªTrack IDç»Ÿè®¡æœ€åå‡ºç°çš„å¸§å’Œä½ç½®
        track_last_appearance = {}  # {track_id: {'frame': f, 'x': x, 'y': y, 'class': c}}
        
        for frame_data in all_detections:
            frame = frame_data['frame']
            for obj in frame_data['objects']:
                track_id = obj['track_id']
                track_last_appearance[track_id] = {
                    'frame': frame,
                    'x': obj['bbox_xywh'][0],
                    'y': obj['bbox_xywh'][1],
                    'class': obj['class']
                }
        
        # ç¬¬äºŒæ­¥ï¼šå¯»æ‰¾å¯èƒ½æ–­å¼€çš„Track IDå¯¹
        merge_map = {}  # {old_id: new_id} æ˜ å°„
        merged_count = 0
        
        track_ids = sorted(track_last_appearance.keys())
        
        for i, track_a in enumerate(track_ids):
            if track_a in merge_map:
                continue  # å·²ç»è¢«åˆå¹¶è¿‡äº†
            
            last_a = track_last_appearance[track_a]
            frame_a = last_a['frame']
            x_a, y_a = last_a['x'], last_a['y']
            class_a = last_a['class']
            
            # æŸ¥æ‰¾åç»­å‡ºç°çš„Track ID
            for track_b in track_ids[i+1:]:
                if track_b in merge_map:
                    continue
                
                # æ‰¾track_bçš„ç¬¬ä¸€æ¬¡å‡ºç°
                first_b_frame = None
                first_b_pos = None
                first_b_class = None
                
                for frame_data in all_detections:
                    for obj in frame_data['objects']:
                        if obj['track_id'] == track_b:
                            if first_b_frame is None:
                                first_b_frame = frame_data['frame']
                                first_b_pos = (obj['bbox_xywh'][0], obj['bbox_xywh'][1])
                                first_b_class = obj['class']
                            break
                
                if first_b_frame is None:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³é‡è¿æ¡ä»¶
                frame_gap = first_b_frame - frame_a
                if 1 <= frame_gap <= max_gap_frames:  # ä¸­é—´æœ‰é—´éš”ä½†ä¸è¶…è¿‡max_gap
                    distance = np.sqrt((first_b_pos[0] - x_a)**2 + (first_b_pos[1] - y_a)**2)
                    
                    if distance < max_distance_pixels and class_a == first_b_class:
                        # è®¤ä¸ºæ˜¯åŒä¸€ç‰©ä½“ï¼Œåº”è¯¥åˆå¹¶
                        merge_map[track_b] = track_a
                        merged_count += 1
                        print(f"  ğŸ”— åˆå¹¶: Track ID {track_b} (é¦–æ¬¡Frame {first_b_frame}) â†’ ID {track_a} (æœ«æ¬¡Frame {frame_a})")
                        print(f"     é—´éš”: {frame_gap}å¸§, è·ç¦»: {distance:.1f}px, ç±»åˆ«: {class_a}")
        
        # ç¬¬ä¸‰æ­¥ï¼šåº”ç”¨åˆå¹¶åˆ°æ‰€æœ‰æ£€æµ‹ç»“æœ
        if merge_map:
            for frame_data in all_detections:
                for obj in frame_data['objects']:
                    if obj['track_id'] in merge_map:
                        old_id = obj['track_id']
                        new_id = merge_map[old_id]
                        obj['track_id'] = new_id
        
        print(f"  âœ“ Step 2.6 å®Œæˆ: åˆå¹¶äº† {merged_count} ä¸ªæ–­å¼€çš„Track ID")
        
        return all_detections
    
    # =========================================================================
    # STEP 3: å…³é”®å¸§æ£€æµ‹ (æ¥è¿‘äº‹ä»¶)
    # =========================================================================

    def extract_key_frames(self, all_detections, tracks, world_distance_threshold=2.0, debug_threshold=5.0):
        """Step 3: å…³é”®å¸§æ£€æµ‹ (æ¥è¿‘äº‹ä»¶) - åŸºäºHomographyä¸–ç•Œåæ ‡
        
        æµç¨‹è¯´æ˜:
        1. Step 2å·²åœ¨è½¨è¿¹ä¸­ä½¿ç”¨Homographyè½¬æ¢å¾—åˆ°ä¸–ç•Œåæ ‡ (center_x_world, center_y_world)
        2. Step 3ä½¿ç”¨è¿™äº›ä¸–ç•Œåæ ‡è®¡ç®—ç‰©ä½“é—´è·ç¦»ï¼Œæ£€æµ‹æ¥è¿‘äº‹ä»¶
        3. é€šè¿‡ç©ºé—´éªŒè¯è¿‡æ»¤ï¼šç¡®ä¿ç‰©ä½“åœ¨Homographyæ ‡å®šåŒºåŸŸå†… (X[-1.75,1.75]m, Y[0,25]m)
           - è‹¥ç‰©ä½“ä¸–ç•Œåæ ‡è¶…å‡ºèŒƒå›´ï¼Œè¯´æ˜Homographyå˜æ¢å¯èƒ½ä¸å¯é ï¼Œåº”è¿‡æ»¤
        4. ä¿å­˜é€šè¿‡é˜ˆå€¼çš„æ¥è¿‘äº‹ä»¶ä½œä¸ºå…³é”®å¸§
        
        å‚æ•°:
        - all_detections: åŸå§‹æ£€æµ‹ç»“æœ (ç”¨äºä¿å­˜å…³é”®å¸§å›¾åƒ)
        - tracks: Step 2è¿”å›çš„è½¨è¿¹ä¿¡æ¯ (å·²åŒ…å«Homographyå˜æ¢çš„worldåæ ‡)
        - world_distance_threshold: å…³é”®å¸§æ£€æµ‹é˜ˆå€¼ï¼ˆé»˜è®¤ 4.5 ç±³ï¼‰
        """
        print(f"\nã€Step 3: å…³é”®å¸§æ£€æµ‹ (åŸºäºHomographyä¸–ç•Œåæ ‡)ã€‘")
        print(f"  â„¹ï¸  ä½¿ç”¨Step 2ä¸­Homographyå˜æ¢çš„ä¸–ç•Œåæ ‡è¿›è¡Œè·ç¦»è®¡ç®—å’Œç©ºé—´éªŒè¯")
        
        proximity_events = []
        all_proximity_pairs = []
        
        # ç‰©ä½“ç±»åˆ«æ˜ å°„
        class_names = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 
                      4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck'}
        
        # å»ºç«‹track_id -> è½¨è¿¹æ•°æ®çš„æ˜ å°„ï¼Œæ–¹ä¾¿æŸ¥æ‰¾
        track_map = {}
        for track_id, track_points in tracks.items():
            for point in track_points:
                frame = point['frame']
                if frame not in track_map:
                    track_map[frame] = {}
                track_map[frame][int(track_id)] = point
        
        # éå†æ¯ä¸€å¸§ï¼Œæ£€æµ‹ç‰©ä½“å¯¹ä¹‹é—´çš„ä¸–ç•Œåæ ‡è·ç¦»
        for frame_data in all_detections:
            frame = frame_data['frame']
            if frame not in track_map or len(track_map[frame]) < 2:
                continue
            
            objects = frame_data['objects']
            frame_tracks = track_map[frame]
            
            # æ£€æŸ¥æ‰€æœ‰ç‰©ä½“å¯¹
            for i in range(len(objects)):
                for j in range(i+1, len(objects)):
                    obj1 = objects[i]
                    obj2 = objects[j]
                    
                    tid1 = obj1['track_id']
                    tid2 = obj2['track_id']
                    
                    # è·å–è½¨è¿¹ä¸­ä¿å­˜çš„ä¸–ç•Œåæ ‡
                    if tid1 not in frame_tracks or tid2 not in frame_tracks:
                        continue
                    
                    track1 = frame_tracks[tid1]
                    track2 = frame_tracks[tid2]
                    
                    # è·å–ä¸–ç•Œåæ ‡ (Option B: ç›´æ¥ä½¿ç”¨ä¿å­˜çš„worldåæ ‡)
                    x1_world = track1['center_x_world']
                    y1_world = track1['center_y_world']
                    x2_world = track2['center_x_world']
                    y2_world = track2['center_y_world']
                    
                    # âœ¨ æ–°å¢: éªŒè¯ä¸¤ä¸ªå¯¹è±¡éƒ½åœ¨æ ‡å®šåŒºåŸŸå†…
                    # æ ‡å®šåŒºåŸŸèŒƒå›´: X [-1.75, 1.75] m, Y [0, 25] m
                    world_x_min, world_x_max = -1.75, 1.75
                    world_y_min, world_y_max = 0.0, 25.0
                    world_margin = 0.3  # å…è®¸è½»å¾®è¶…å‡ºèŒƒå›´
                    
                    # æ£€æŸ¥ä¸¤ä¸ªç‰©ä½“æ˜¯å¦éƒ½åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    obj1_valid = (world_x_min - world_margin <= x1_world <= world_x_max + world_margin and
                                  world_y_min - world_margin <= y1_world <= world_y_max + world_margin)
                    obj2_valid = (world_x_min - world_margin <= x2_world <= world_x_max + world_margin and
                                  world_y_min - world_margin <= y2_world <= world_y_max + world_margin)
                    
                    if not (obj1_valid and obj2_valid):
                        # è·³è¿‡è¶…å‡ºæ ‡å®šåŒºåŸŸçš„å¯¹è±¡å¯¹
                        continue
                    
                    # è·å–åƒç´ åæ ‡ç”¨äºå›¾åƒä¿å­˜
                    x1_px = track1['center_x']
                    y1_px = track1['center_y']
                    x2_px = track2['center_x']
                    y2_px = track2['center_y']
                    distance_pixel = np.sqrt((x2_px-x1_px)**2 + (y2_px-y1_px)**2)
                    
                    # ä½¿ç”¨å¤šé”šç‚¹ç¢°æ’æ£€æµ‹åˆ†æå™¨
                    # âš ï¸ æ³¨æ„: å¤šé”šç‚¹åˆ†æè™½ç„¶å·²é›†æˆï¼Œä½†ä¸ºäº†æ€§èƒ½è€ƒè™‘ï¼Œæš‚æ—¶ç¦ç”¨
                    # å¦‚éœ€å¯ç”¨ï¼Œè®¾ç½® USE_MULTI_ANCHOR=True
                    USE_MULTI_ANCHOR = False
                    
                    if USE_MULTI_ANCHOR:
                        try:
                            # è·å–é”šç‚¹
                            anchors1 = self._get_object_anchors(obj1['class'], obj1['bbox_xywh'])
                            anchors2 = self._get_object_anchors(obj2['class'], obj2['bbox_xywh'])
                            
                            # è·å–é€Ÿåº¦ä¿¡æ¯ï¼ˆä½¿ç”¨ä¸–ç•Œåæ ‡é€Ÿåº¦ m/sï¼Œè€Œä¸æ˜¯åƒç´ é€Ÿåº¦ï¼‰
                            # ä¸–ç•Œåæ ‡é€Ÿåº¦å·²ç»è€ƒè™‘äº†è·³å¸§ï¼Œå•ä½ä¸º m/s
                            vx1 = track1.get('vx_world', 0.0)
                            vy1 = track1.get('vy_world', 0.0)
                            vx2 = track2.get('vx_world', 0.0)
                            vy2 = track2.get('vy_world', 0.0)
                            
                            # åˆ›å»ºç¢°æ’åˆ†æå™¨
                            analyzer = CollisionAnalyzer(pixel_per_meter=self.pixel_per_meter)
                            
                            # æ‰§è¡Œç¢°æ’åˆ†æ
                            collision_result = analyzer.analyze(
                                obj1=obj1,
                                obj2=obj2,
                                obj1_anchors=anchors1,
                                obj2_anchors=anchors2,
                                obj1_velocity=(vx1, vy1),
                                obj2_velocity=(vx2, vy2),
                                obj1_track=track1,
                                obj2_track=track2,
                                H=self.H
                            )
                            
                            # ä½¿ç”¨å¤šé”šç‚¹è·ç¦»
                            distance_meters = collision_result.min_distance
                            closest_parts = (collision_result.object1_part, collision_result.object2_part)
                            ttc = collision_result.ttc
                            risk_level = collision_result.risk_level
                            
                        except Exception as e:
                            # å¦‚æœå¤šé”šç‚¹åˆ†æå¤±è´¥ï¼Œå›é€€åˆ°ä¸­å¿ƒç‚¹è·ç¦»
                            print(f"  âš ï¸  å¤šé”šç‚¹åˆ†æå¼‚å¸¸: {e}ï¼Œä½¿ç”¨ä¸­å¿ƒç‚¹è·ç¦»")
                            distance_meters = np.sqrt((x2_world-x1_world)**2 + (y2_world-y1_world)**2)
                            closest_parts = ('center', 'center')
                            ttc = None
                            risk_level = 'UNKNOWN'
                    else:
                        # ä½¿ç”¨ä¸­å¿ƒç‚¹è·ç¦»ï¼ˆä¸ä¹‹å‰å…¼å®¹ï¼‰
                        distance_meters = np.sqrt((x2_world-x1_world)**2 + (y2_world-y1_world)**2)
                        closest_parts = ('center', 'center')
                        ttc = None
                        risk_level = 'UNKNOWN'
                    
                    class1_name = class_names.get(obj1['class'], f"class_{obj1['class']}")
                    class2_name = class_names.get(obj2['class'], f"class_{obj2['class']}")
                    
                    # è®°å½•æ‰€æœ‰ < debug_threshold çš„æ£€æµ‹ï¼Œç”¨äºè°ƒè¯•
                    if distance_meters < debug_threshold:
                        all_proximity_pairs.append({
                            'frame': frame,
                            'class_1': class1_name,
                            'class_2': class2_name,
                            'distance_meters': distance_meters,
                            'track_ids': [tid1, tid2]
                        })
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæ¥è¿‘äº‹ä»¶ (ä½¿ç”¨ä¸–ç•Œè·ç¦»é˜ˆå€¼)
                    if distance_meters < world_distance_threshold:
                        event = {
                            'frame': frame,
                            'time': frame_data['time'],
                            'track_id_1': tid1,
                            'track_id_2': tid2,
                            'class_1': class1_name,
                            'class_2': class2_name,
                            'distance_pixel': float(distance_pixel),
                            'distance_meters': float(distance_meters),
                            'object_classes': (obj1['class'], obj2['class']),
                            'center_1_px': [float(x1_px), float(y1_px)],
                            'center_2_px': [float(x2_px), float(y2_px)],
                            'center_1_world': [float(x1_world), float(y1_world)],
                            'center_2_world': [float(x2_world), float(y2_world)],
                            'positions': {
                                'obj1': {'x': x1_px, 'y': y1_px},
                                'obj2': {'x': x2_px, 'y': y2_px}
                            },
                            'positions_world': {
                                'obj1': {'x': x1_world, 'y': y1_world},
                                'obj2': {'x': x2_world, 'y': y2_world}
                            },
                            # å¤šé”šç‚¹ç¢°æ’åˆ†æä¿¡æ¯
                            'multi_anchor': {
                                'closest_parts': closest_parts,
                                'risk_level': risk_level,
                                'ttc': ttc
                            }
                        }
                        proximity_events.append(event)
                        
                        # ä¿å­˜å…³é”®å¸§å›¾åƒ
                        frame_img_path = self.keyframe_dir / f"keyframe_{frame:04d}_ID{tid1}_ID{tid2}.jpg"
                        self.save_keyframe_with_distance(self.video_path, frame, frame_img_path, event)
        
        # ä¿å­˜æ¥è¿‘äº‹ä»¶
        events_path = self.keyframe_dir / 'proximity_events.json'
        with open(events_path, 'w') as f:
            json.dump(proximity_events, f, indent=2)
        
        print(f"  âœ“ Step 3å®Œæˆ: {len(proximity_events)}ä¸ªå…³é”®å¸§ (< {world_distance_threshold}m)")
        print(f"    æ€»æ£€æµ‹åˆ°çš„è¿‘è·ç¦»å¯¹: {len(all_proximity_pairs)}ä¸ª (< {debug_threshold}m)")
        print(f"    è·ç¦»é˜ˆå€¼: {world_distance_threshold}ç±³ (ä¸–ç•Œåæ ‡)")
        print(f"    åæ ‡æ¥æº: Step 2ä¿å­˜çš„è½¨è¿¹æ•°æ® (å·²è½¬æ¢)")
        print(f"    è¾“å‡º: {events_path.name}")
        
        # æ‰“å°è¢«æ’é™¤çš„äº‹ä»¶
        excluded_count = len(all_proximity_pairs) - len(proximity_events)
        if excluded_count > 0:
            print(f"\n  â„¹ï¸  è¢«æ’é™¤çš„æ¥è¿‘äº‹ä»¶ ({world_distance_threshold}-{debug_threshold}m):")
            for pair in all_proximity_pairs:
                if pair['distance_meters'] >= world_distance_threshold:
                    print(f"     - Frame {pair['frame']:03d}: Track {pair['track_ids'][0]}({pair['class_1']}) + Track {pair['track_ids'][1]}({pair['class_2']}) = {pair['distance_meters']:.2f}m")
        
        # è°ƒè¯•è¾“å‡ºï¼šæ£€æŸ¥è¾“å…¥æ•°æ®
        total_object_pairs = 0
        for frame_data in all_detections:
            if len(frame_data['objects']) >= 2:
                total_object_pairs += len(frame_data['objects']) * (len(frame_data['objects']) - 1) // 2
        print(f"\n  â„¹ï¸  è°ƒè¯•ä¿¡æ¯: æ£€æŸ¥äº† {total_object_pairs} ä¸ªç‰©ä½“å¯¹ï¼Œå…¶ä¸­ {len(all_proximity_pairs)} ä¸ªè·ç¦» < {debug_threshold}m")
        
        return proximity_events
    
    # =========================================================================
    # STEP 3.1: è·å–ç‰©ä½“çš„é”šç‚¹
    # =========================================================================
    
    def _shrink_bbox(self, bbox_xywh, shrink_ratio=0.8):
        """ç¼©å°bounding box - ä»ä¸­å¿ƒå¾€å¤–ç¼©å°åˆ°åŸæ¥çš„æ¯”ä¾‹
        
        Args:
            bbox_xywh: [x_center, y_center, width, height]
            shrink_ratio: ç¼©å°æ¯”ä¾‹ (0.8 = ä¿ç•™åŸæ¥çš„80%)
        
        Returns:
            ç¼©å°åçš„ bbox [x_center, y_center, width*shrink_ratio, height*shrink_ratio]
        """
        x, y, w, h = bbox_xywh
        new_w = w * shrink_ratio
        new_h = h * shrink_ratio
        return [x, y, new_w, new_h]
    
    def _get_object_anchors(self, class_id, bbox_xywh):
        """æ ¹æ®ç‰©ä½“ç±»åˆ«è·å–ç›¸åº”çš„é”šç‚¹
        
        Args:
            class_id: YOLOç‰©ä½“ç±»åˆ«ID (0=person, 1=bicycle, 2=car, 3=motorcycle, etc.)
            bbox_xywh: è¾¹ç•Œæ¡† [x_center, y_center, width, height]
        
        Returns:
            dict: {anchor_name: (x, y), ...}
        """
        # ç¼©å°bounding boxåˆ°åŸæ¥çš„80%ï¼Œç¡®ä¿é”šç‚¹åœ¨ç‰©ä½“å†…
        bbox_xywh = self._shrink_bbox(bbox_xywh, shrink_ratio=0.8)
        
        try:
            if class_id == 0:  # person
                return PedestrianAnchors.get_anchors(bbox_xywh)
            elif class_id == 2:  # car
                return VehicleAnchors.get_anchors(bbox_xywh, class_id)
            elif class_id == 1:  # bicycle
                return BicycleAnchors.get_anchors(bbox_xywh)
            elif class_id == 3:  # motorcycle
                return MotorcycleAnchors.get_anchors(bbox_xywh)
            elif class_id == 5:  # bus
                return VehicleAnchors.get_anchors(bbox_xywh, class_id)
            elif class_id == 7:  # truck
                return VehicleAnchors.get_anchors(bbox_xywh, class_id)
            else:
                # å…¶ä»–ç±»åˆ«ä½¿ç”¨é€šç”¨é”šç‚¹
                return VehicleAnchors.get_anchors(bbox_xywh, class_id)
        except Exception as e:
            print(f"  âš ï¸  è·å–é”šç‚¹å¤±è´¥ (class_id={class_id}): {e}")
            # é™çº§æ–¹æ¡ˆï¼šè¿”å›ç®€å•çš„ä¸­å¿ƒé”šç‚¹
            return {'center': (bbox_xywh[0], bbox_xywh[1])}
    
    # =========================================================================
    # STEP 3.5: åŒç±»åˆ«ç‰©ä½“è¯¯æ£€è¿‡æ»¤
    # =========================================================================
    
    def filter_same_class_false_positives(self, proximity_events, same_class_distance_threshold=0.3):
        """è¿‡æ»¤åŒç±»åˆ«ç‰©ä½“çš„è¯¯æ£€ + è·¨å¸§åŒä¸€ç‰©ä½“çš„è¯¯åˆ†å‰²
        
        è¿‡æ»¤æ¡ä»¶ï¼š
        1. æè¿‘è·ç¦» (< 0.1m) â†’ åŒä¸€ç‰©ä½“ä¸¤éƒ¨åˆ†
        2. ä¸åˆç†çš„ç±»åˆ«ç»„åˆ + è·ç¦»ç¨³å®š (std < 0.5) â†’ åŒé€Ÿä¸å¯èƒ½
        3. æ–­æ–­ç»­ç»­å‡ºç°çš„Track IDå¯¹ + è·ç¦»è¿‘ (< 2.0m) â†’ åŒä¸€ç‰©ä½“è¢«è¯¯åˆ†å‰²
        4. éƒ½æ˜¯æ±½è½¦ç±»å‹ + è·ç¦» < 0.5m â†’ åŒä¸€è½¦è¾†çš„ä¸åŒéƒ¨åˆ†ï¼ˆå¦‚å¡è½¦å¤´å’Œèº«ä½“ï¼‰
        """
        print(f"\nã€Step 3.5: ç‰©ä½“è¯¯æ£€è¿‡æ»¤ (æ™ºèƒ½ç­–ç•¥)ã€‘")
        
        # ä¸åˆç†çš„ç±»åˆ«ç»„åˆï¼ˆä¸å¯èƒ½åŒæ—¶å‡ºç°ä¸”åŒé€Ÿè¿åŠ¨ï¼‰
        illogical_class_combinations = [
            ('person', 'motorcycle'),
            ('person', 'car'),
            ('person', 'truck'),
            ('person', 'bus'),
            ('bicycle', 'motorcycle'),
            ('bicycle', 'car'),
        ]
        
        # å®šä¹‰æ±½è½¦ç±»å‹
        vehicle_types = {'car', 'truck', 'bus', 'motorcycle'}
        
        # é¦–å…ˆåˆ†æTrack IDå¯¹çš„å‡ºç°æƒ…å†µ
        track_pair_analysis = {}
        for event in proximity_events:
            tid1, tid2 = event['track_id_1'], event['track_id_2']
            pair_key = tuple(sorted([tid1, tid2]))
            
            if pair_key not in track_pair_analysis:
                track_pair_analysis[pair_key] = {
                    'events': [],
                    'frames': [],
                    'distances': [],
                    'classes': None
                }
            
            track_pair_analysis[pair_key]['events'].append(event)
            track_pair_analysis[pair_key]['frames'].append(event['frame'])
            track_pair_analysis[pair_key]['distances'].append(event['distance_meters'])
            track_pair_analysis[pair_key]['classes'] = (event['class_1'], event['class_2'])
        
        # è¯†åˆ«"æ–­æ–­ç»­ç»­å‡ºç°"çš„Track IDå¯¹
        suspicious_discontinuous_pairs = set()
        for pair, info in track_pair_analysis.items():
            frames = sorted(info['frames'])
            distances = info['distances']
            avg_distance = sum(distances) / len(distances)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„é—´éš”ï¼ˆå‡ºç°-æ¶ˆå¤±-å†å‡ºç°ï¼‰
            has_gap = False
            for i in range(len(frames) - 1):
                if frames[i+1] - frames[i] > 3:  # é—´éš” > 3å¸§
                    has_gap = True
                    break
            
            # å¦‚æœæ–­æ–­ç»­ç»­å‡ºç°ä¸”è·ç¦»è¿‘ï¼Œæ ‡è®°ä¸ºå¯ç–‘
            if has_gap and avg_distance < 2.0:
                suspicious_discontinuous_pairs.add(pair)
        
        # ç°åœ¨è¿›è¡Œé€ä¸ªäº‹ä»¶çš„è¿‡æ»¤
        filtered_events = []
        filtered_count = 0
        filter_reasons = []
        
        for event in proximity_events:
            class_1 = event['class_1']
            class_2 = event['class_2']
            distance = event['distance_meters']
            frame = event['frame']
            tid1, tid2 = event['track_id_1'], event['track_id_2']
            pair_key = tuple(sorted([tid1, tid2]))
            
            reason = None
            
            # æ¡ä»¶1: æè¿‘è·ç¦» (< 0.1m) â†’ åŒä¸€ç‰©ä½“çš„ä¸¤éƒ¨åˆ†
            if distance < 0.1:
                reason = f"æè¿‘ ({distance:.3f}m < 0.1m)"
                filtered_count += 1
                filter_reasons.append((frame, tid1, tid2, class_1, class_2, distance, reason))
                continue
            
            # æ¡ä»¶4: éƒ½æ˜¯æ±½è½¦ç±»å‹ + è·ç¦» < 0.5m â†’ åŒä¸€è½¦è¾†çš„ä¸åŒéƒ¨åˆ†
            if (class_1 in vehicle_types and class_2 in vehicle_types) and distance < 0.5:
                reason = f"éƒ½æ˜¯æ±½è½¦ç±»å‹ ({class_1}+{class_2}, è·ç¦»{distance:.3f}m < 0.5m)"
                filtered_count += 1
                filter_reasons.append((frame, tid1, tid2, class_1, class_2, distance, reason))
                continue
            
            # æ¡ä»¶2: ä¸åˆç†çš„ç±»åˆ«ç»„åˆ + è·ç¦»ç¨³å®š â†’ åŒé€Ÿä¸å¯èƒ½
            class_pair = tuple(sorted([class_1, class_2]))
            if class_pair in [tuple(sorted(p)) for p in illogical_class_combinations]:
                pair_info = track_pair_analysis[pair_key]
                std_distance = np.std(pair_info['distances'])
                
                if std_distance < 0.5:  # è·ç¦»éå¸¸ç¨³å®š = åŒé€Ÿ = ä¸å¯èƒ½
                    reason = f"ä¸åˆç†ç±»åˆ«ç»„åˆ+åŒé€Ÿ ({class_1}+{class_2}, std={std_distance:.2f}m)"
                    filtered_count += 1
                    filter_reasons.append((frame, tid1, tid2, class_1, class_2, distance, reason))
                    continue
            
            # æ¡ä»¶3: æ–­æ–­ç»­ç»­å‡ºç°çš„Track IDå¯¹ + è·ç¦»è¿‘ â†’ åŒä¸€ç‰©ä½“è¢«è¯¯åˆ†å‰²
            if pair_key in suspicious_discontinuous_pairs:
                pair_info = track_pair_analysis[pair_key]
                reason = f"æ–­æ–­ç»­ç»­å‡ºç°({len(pair_info['frames'])}å¸§) + è·ç¦»è¿‘"
                filtered_count += 1
                filter_reasons.append((frame, tid1, tid2, class_1, class_2, distance, reason))
                continue
            
            # ä¿ç•™è¿™ä¸ªäº‹ä»¶
            filtered_events.append(event)
        
        # æ‰“å°è¿‡æ»¤è¯¦æƒ…
        if filter_reasons:
            print(f"  ğŸ—‘ï¸  è¿‡æ»¤çš„äº‹ä»¶:")
            for frame, tid1, tid2, class1, class2, dist, reason in filter_reasons[:20]:  # åªæ‰“å°å‰20ä¸ª
                print(f"      Frame {frame}: {class1}({tid1}) + {class2}({tid2}) = {dist:.3f}m ({reason})")
            if len(filter_reasons) > 20:
                print(f"      ... è¿˜æœ‰ {len(filter_reasons)-20} ä¸ª")
        
        print(f"  âœ“ è¿‡æ»¤å®Œæˆ: æ’é™¤äº† {filtered_count} ä¸ªè¯¯æ£€, ä¿ç•™ {len(filtered_events)} ä¸ªäº‹ä»¶")
        print(f"    æ¡ä»¶1: è·ç¦» < 0.1m")
        print(f"    æ¡ä»¶4: éƒ½æ˜¯æ±½è½¦ç±»å‹ (car/truck/bus/motorcycleç­‰) + è·ç¦» < 0.5m")
        print(f"    æ¡ä»¶2: ä¸åˆç†ç±»åˆ«ç»„åˆ (person/motorcycleç­‰) + è·ç¦»ç¨³å®š (std < 0.5m)")
        print(f"    æ¡ä»¶3: Track IDå¯¹æ–­æ–­ç»­ç»­å‡ºç° + å¹³å‡è·ç¦» < 2.0m")
        
        # ä¿å­˜è¿‡æ»¤åçš„äº‹ä»¶
        events_path = self.keyframe_dir / 'proximity_events_filtered.json'
        with open(events_path, 'w') as f:
            json.dump({
                'total_detected': len(proximity_events),
                'false_positives_filtered': filtered_count,
                'valid_events': len(filtered_events),
                'events': filtered_events
            }, f, indent=2)
        
        return filtered_events
    
    # =========================================================================
    # STEP 3.6: æ¸…ç†è¢«è¿‡æ»¤çš„å…³é”®å¸§å›¾ç‰‡
    # =========================================================================
    
    def cleanup_filtered_keyframes(self, original_events, filtered_events):
        """åˆ é™¤è¢«è¿‡æ»¤æ‰çš„å…³é”®å¸§å›¾ç‰‡æ–‡ä»¶"""
        # æ”¶é›†è¢«ä¿ç•™çš„ keyframe æ–‡ä»¶å
        kept_frames = set()
        for event in filtered_events:
            frame_id = event['frame']
            tid1 = event['track_id_1']
            tid2 = event['track_id_2']
            # ç”Ÿæˆåº”è¯¥è¢«ä¿ç•™çš„æ–‡ä»¶å
            filename = f"keyframe_{frame_id:04d}_ID{tid1}_ID{tid2}.jpg"
            kept_frames.add(filename)
        
        # åˆ é™¤ä¸åœ¨ kept_frames ä¸­çš„ keyframe æ–‡ä»¶
        for img_file in self.keyframe_dir.glob('keyframe_*.jpg'):
            if img_file.name not in kept_frames:
                try:
                    img_file.unlink()
                    print(f"  ğŸ—‘ï¸  åˆ é™¤å…³é”®å¸§å›¾ç‰‡: {img_file.name}")
                except Exception as e:
                    print(f"  âš ï¸  åˆ é™¤å›¾ç‰‡å¤±è´¥ {img_file.name}: {e}")
    
    # =========================================================================
    # STEP 3.6: å¤šé”šç‚¹ç¢°æ’åˆ†æ (ä»…å…³é”®å¸§) âœ¨ æ–°å¢åŠŸèƒ½
    # =========================================================================
    
    def analyze_keyframes_with_multi_anchor(self, proximity_events, all_detections, tracks):
        """Step 3.6: å¯¹å…³é”®å¸§æ‰§è¡Œå¤šé”šç‚¹ç¢°æ’åˆ†æï¼ˆä»…åœ¨å·²ç¡®å®šä¸ºæ¥è¿‘äº‹ä»¶çš„å¸§ä¸Šæ‰§è¡Œï¼‰
        
        è¿™æ ·å¯ä»¥å¤§å¹…é™ä½è®¡ç®—é‡ï¼š
        - Step 3: ç”¨ç®€å•çš„ä¸­å¿ƒç‚¹è·ç¦»å¿«é€Ÿç­›é€‰æ¥è¿‘äº‹ä»¶
        - Step 3.6: åªå¯¹è¿™äº›å…³é”®å¸§æ‰§è¡Œè¯¦ç»†çš„å¤šé”šç‚¹åˆ†æ
        
        Args:
            proximity_events: ä»Step 3ç­›é€‰å‡ºçš„æ¥è¿‘äº‹ä»¶
            all_detections: æ‰€æœ‰æ£€æµ‹ç»“æœ
            tracks: è½¨è¿¹æ•°æ®
        
        Returns:
            proximity_events: å¢å¼ºåçš„äº‹ä»¶ï¼ˆåŒ…å«å¤šé”šç‚¹åˆ†æä¿¡æ¯ï¼‰
        """
        print(f"\nã€Step 3.6: å¤šé”šç‚¹ç¢°æ’åˆ†æ (ä»…å…³é”®å¸§)ã€‘")
        
        if not proximity_events:
            print(f"  â„¹ï¸  æ— å…³é”®å¸§ï¼Œè·³è¿‡å¤šé”šç‚¹åˆ†æ")
            return proximity_events
        
        # å»ºç«‹track_id -> è½¨è¿¹æ•°æ®çš„æ˜ å°„
        track_map = {}
        for track_id, track_points in tracks.items():
            for point in track_points:
                frame = point['frame']
                if frame not in track_map:
                    track_map[frame] = {}
                track_map[frame][int(track_id)] = point
        
        # å»ºç«‹frame -> objectsçš„æ˜ å°„
        detection_map = {}
        for frame_data in all_detections:
            detection_map[frame_data['frame']] = frame_data
        
        # å¯¹æ¯ä¸ªå…³é”®å¸§äº‹ä»¶æ‰§è¡Œå¤šé”šç‚¹åˆ†æ
        analyzed_count = 0
        failed_frames = []
        
        for event in proximity_events:
            frame = event['frame']
            tid1 = event['track_id_1']
            tid2 = event['track_id_2']
            
            # è·³è¿‡å·²ç»æœ‰å¤šé”šç‚¹ä¿¡æ¯çš„
            if 'multi_anchor_detailed' in event:
                continue
            
            try:
                # è·å–è¯¥å¸§çš„æ£€æµ‹å’Œè½¨è¿¹æ•°æ®
                if frame not in detection_map or frame not in track_map:
                    failed_frames.append((frame, tid1, tid2, "Frame/Track data not found"))
                    continue
                
                frame_data = detection_map[frame]
                frame_tracks = track_map[frame]
                
                # æŸ¥æ‰¾ä¸¤ä¸ªç‰©ä½“
                obj1, obj2 = None, None
                track1_point, track2_point = None, None
                track1_history, track2_history = None, None
                
                for obj in frame_data['objects']:
                    if obj['track_id'] == tid1:
                        obj1 = obj
                        track1_point = frame_tracks.get(tid1)
                        # è·å–å®Œæ•´çš„è½¨è¿¹å†å²ï¼ˆç”¨äºè®¡ç®—é€Ÿåº¦å’Œæ–¹å‘ï¼‰
                        if tid1 in tracks:
                            track1_history = tracks[tid1]
                    elif obj['track_id'] == tid2:
                        obj2 = obj
                        track2_point = frame_tracks.get(tid2)
                        # è·å–å®Œæ•´çš„è½¨è¿¹å†å²
                        if tid2 in tracks:
                            track2_history = tracks[tid2]
                
                if obj1 is None or obj2 is None or track1_point is None or track2_point is None:
                    reason = []
                    if obj1 is None: reason.append(f"obj1 not found")
                    if obj2 is None: reason.append(f"obj2 not found")
                    if track1_point is None: reason.append(f"track1_point not found")
                    if track2_point is None: reason.append(f"track2_point not found")
                    failed_frames.append((frame, tid1, tid2, ", ".join(reason)))
                    continue
                
                # è·å–é”šç‚¹
                anchors1 = self._get_object_anchors(obj1['class'], obj1['bbox_xywh'])
                anchors2 = self._get_object_anchors(obj2['class'], obj2['bbox_xywh'])
                
                # è·å–é€Ÿåº¦ä¿¡æ¯ï¼ˆä»è¯¥å¸§çš„è½¨è¿¹ç‚¹ï¼‰
                vx1 = track1_point.get('vx', 0.0)
                vy1 = track1_point.get('vy', 0.0)
                vx2 = track2_point.get('vx', 0.0)
                vy2 = track2_point.get('vy', 0.0)
                
                # æ‰§è¡Œå¤šé”šç‚¹ç¢°æ’åˆ†æ
                analyzer = CollisionAnalyzer(pixel_per_meter=self.pixel_per_meter)
                collision_result = analyzer.analyze(
                    obj1=obj1,
                    obj2=obj2,
                    obj1_anchors=anchors1,
                    obj2_anchors=anchors2,
                    obj1_velocity=(vx1, vy1),
                    obj2_velocity=(vx2, vy2),
                    obj1_track=track1_history,  # ä¼ å…¥å®Œæ•´çš„è½¨è¿¹å†å²
                    obj2_track=track2_history,  # ä¼ å…¥å®Œæ•´çš„è½¨è¿¹å†å²
                    H=self.H
                )
                
                # æ·»åŠ è¯¦ç»†çš„å¤šé”šç‚¹åˆ†æç»“æœ
                event['multi_anchor_detailed'] = collision_result.to_dict()
                analyzed_count += 1
                
            except Exception as e:
                # è®°å½•å¤±è´¥çš„å¸§
                import traceback
                error_msg = f"{type(e).__name__}: {str(e)}"
                failed_frames.append((frame, tid1, tid2, error_msg))
        
        # æŠ¥å‘Šåˆ†æç»“æœ
        if analyzed_count > 0:
            print(f"  âœ“ å¤šé”šç‚¹åˆ†æå®Œæˆ: {analyzed_count}/{len(proximity_events)}ä¸ªå…³é”®å¸§")
        
        if failed_frames:
            print(f"  âš ï¸  {len(failed_frames)}ä¸ªå…³é”®å¸§åˆ†æå¤±è´¥:")
            for frame, tid1, tid2, reason in failed_frames:
                print(f"     - Frame {frame}: ID{tid1}+ID{tid2} ({reason})")
        else:
            print(f"  âš ï¸  å¤šé”šç‚¹åˆ†æå®Œæˆ: 0/{len(proximity_events)}ä¸ªå…³é”®å¸§ (æ— æ³•è·å–é”šç‚¹æ•°æ®æˆ–å‘ç”Ÿé”™è¯¯)")
        
        # =================================================================
        # STEP 3.7: å¤šé”šç‚¹è·ç¦»è¿‡æ»¤ï¼ˆä»…ä¿ç•™è·ç¦» â‰¤ 1.0m çš„é«˜é£é™©äº‹ä»¶ï¼‰
        # =================================================================
        print(f"\nã€Step 3.7: å¤šé”šç‚¹è·ç¦»è¿‡æ»¤ (â‰¤1.0m)ã€‘")
        
        anchor_filtered_events = []
        for event in proximity_events:
            multi = event.get('multi_anchor_detailed', {})
            min_distance = multi.get('min_distance_meters', float('inf'))
            
            # ä¿ç•™è·ç¦» â‰¤ 1.0m çš„äº‹ä»¶ï¼ˆé«˜é£é™©ï¼‰
            if min_distance <= 1.0:
                anchor_filtered_events.append(event)
            else:
                frame = event['frame']
                tid1, tid2 = event['track_id_1'], event['track_id_2']
                print(f"  âŠ— è¿‡æ»¤ Frame {frame}: Track {tid1}+{tid2} (é”šç‚¹è·ç¦»={min_distance:.2f}m > 1.0m)")
        
        filtered_count = len(proximity_events) - len(anchor_filtered_events)
        print(f"  ğŸ” å¤šé”šç‚¹è·ç¦»è¿‡æ»¤: æ’é™¤ {filtered_count} ä¸ªäº‹ä»¶")
        print(f"  âœ“ Step 3.7å®Œæˆ: ä¿ç•™ {len(anchor_filtered_events)} ä¸ªå…³é”®å¸§ (â‰¤ 1.0m)")
        
        return anchor_filtered_events
    
    # =========================================================================
    # STEP 4: Homography ä¿¡æ¯ä¿å­˜ (ä»…ä½œå…ƒæ•°æ®ä¿å­˜) âœ¨ Homographyå·²åœ¨Step 2ä½¿ç”¨
    # =========================================================================
    
    def transform_key_frames_to_world(self, proximity_events):
        """Step 4: Homography ä¿¡æ¯ä¿å­˜
        
        Warning: Homography transformation already completed in Step 2!
        - Step 2: Trajectory construction + Homography transform -> world coordinates
        - Step 3: Use world coordinates to detect keyframes
        - Step 4: Only save Homography metadata, no duplicate transform
        """
        print(f"\nã€Step 4: Homography ä¿¡æ¯ä¿å­˜ã€‘")
        print(f"  â„¹ï¸  æ³¨æ„: åæ ‡å˜æ¢å·²åœ¨Step 2ä¸­å®Œæˆï¼ˆä½¿ç”¨Homographyï¼‰")
        
        if self.H is None:
            print(f"  âš ï¸  æœªåŠ è½½Homography")
            return proximity_events
        
        # ç›´æ¥è¿”å›äº‹ä»¶ï¼ˆå·²åŒ…å«ä¸–ç•Œåæ ‡ï¼‰
        # ä¿å­˜ Homography çŸ©é˜µä¿¡æ¯ä¾›å‚è€ƒ
        trans_path = self.homography_dir / 'transformed_key_frames.json'
        with open(trans_path, 'w') as f:
            json.dump(proximity_events, f, indent=2)
        
        print(f"  âœ“ Step 4å®Œæˆ: {len(proximity_events)}ä¸ªå…³é”®å¸§ä¿¡æ¯å·²ä¿å­˜")
        print(f"    ç¼©æ”¾å› å­: {self.pixel_per_meter:.2f} px/m")
        print(f"    åæ ‡ç³»ç»Ÿ: ä¸–ç•Œåæ ‡ (å·²åœ¨Step 3å˜æ¢)")
        print(f"    è¾“å‡º: {trans_path.name}")
        
        return proximity_events
    
    # =========================================================================
    # STEP 5: TTC å’Œ Event åˆ†çº§
    # =========================================================================
    
    def analyze_collision_risk(self, transformed_events):
        """Step 5: TTC è®¡ç®—å’Œ Event åˆ†çº§
        
        è®¡ç®— TTCï¼Œåˆ†çº§äº‹ä»¶ (L1/L2/L3)
        
         æ”¹è¿›: è¿‡æ»¤åŒç±»åˆ«ç‰©ä½“çš„æè¿‘æ¥è¿‘äº‹ä»¶
        - å¦‚æœä¸¤ä¸ªç‰©ä½“éƒ½æ˜¯åŒä¸€ç±»åˆ«ï¼ˆå¦‚ä¸¤ä¸ªcarï¼Œä¸¤ä¸ªmotorcycleï¼‰
        - ä¸”è·ç¦» < 0.5mï¼Œåˆ™å¯èƒ½æ˜¯åŒä¸€ç‰©ä½“çš„è¯¯æ£€
        - æ ‡è®°ä¸º 'Filtered_SameClass' å¹¶æ’é™¤
        """
        print(f"\nã€Step 5: ç¢°æ’é£é™©åˆ†æã€‘")
        
        if not transformed_events:
            print(f"  âš ï¸  æ²¡æœ‰å…³é”®å¸§ï¼Œæ— æ³•åˆ†æ")
            return [], {0: 0, 1: 0, 2: 0, 3: 0}
        
        analyzed_events = []
        filtered_count = 0
        
        for event in transformed_events:
            analyzed = event.copy()
            
            # åœ¨ä¸–ç•Œåæ ‡ä¸­è¿›è¡Œåˆ†çº§
            distance = event['distance_meters']
            class_1 = event.get('class_1', '')
            class_2 = event.get('class_2', '')
            
            # ğŸ” æ£€æŸ¥æ˜¯å¦ä¸ºåŒç±»åˆ«ç‰©ä½“çš„æè¿‘æ¥è¿‘äº‹ä»¶
            if class_1 == class_2 and distance < 0.5:
                # åŒä¸€ç±»åˆ« + è·ç¦»å¾ˆè¿‘ = å¯èƒ½æ˜¯åŒä¸€ç‰©ä½“çš„ä¸åŒéƒ¨åˆ†
                analyzed['level'] = 0
                analyzed['level_name'] = 'Filtered_SameClass'
                analyzed['reason'] = f"Same class ({class_1}) with distance {distance:.3f}m < 0.5m - likely same object"
                filtered_count += 1
            # åˆ†çº§æ ‡å‡† (ç±³)
            elif distance < 0.5:
                analyzed['level'] = 1
                analyzed['level_name'] = 'Collision'
            elif distance < 1.5:
                analyzed['level'] = 2
                analyzed['level_name'] = 'Near Miss'
            else:
                analyzed['level'] = 3
                analyzed['level_name'] = 'Avoidance'
            
            analyzed_events.append(analyzed)
        
        # ç»Ÿè®¡
        level_counts = {0: 0, 1: 0, 2: 0, 3: 0}
        for event in analyzed_events:
            level_counts[event['level']] += 1
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_path = self.analysis_dir / 'collision_events.json'
        with open(analysis_path, 'w') as f:
            json.dump(analyzed_events, f, indent=2)
        
        print(f"  âœ“ Step 5å®Œæˆ")
        print(f"    - Filtered (Same class, <0.5m): {level_counts[0]} ğŸš«")
        print(f"    - Level 1 (Collision, <0.5m): {level_counts[1]}")
        print(f"    - Level 2 (Near Miss, 0.5-1.5m): {level_counts[2]}")
        print(f"    - Level 3 (Avoidance, >1.5m): {level_counts[3]}")
        print(f"    è¾“å‡º: {analysis_path.name}")
        
        return analyzed_events, level_counts
    
    # =========================================================================
    # æŠ¥å‘Šç”Ÿæˆ
    # =========================================================================
    
    def generate_report(self, proximity_events, analyzed_events, level_counts):
        """ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š (æ”¹è¿›ç‰ˆï¼šæ ¹æ®TTCåŠ¨æ€åˆ†ç±»)"""
        report_path = self.analysis_dir / 'analysis_report.txt'
        
        # è¾…åŠ©å‡½æ•°ï¼šæ ¼å¼åŒ–TTCå€¼ï¼ˆæ”¯æŒæ¯«ç§’æ˜¾ç¤ºï¼‰
        def format_ttc(ttc_seconds):
            if ttc_seconds is None or ttc_seconds <= 0:
                return "N/A"
            elif ttc_seconds < 0.01:  # å°äº10msï¼Œç”¨æ¯«ç§’æ˜¾ç¤º
                return f"{ttc_seconds*1000:.2f}ms"
            elif ttc_seconds < 0.1:   # å°äº100msï¼Œç”¨4ä½å°æ•°
                return f"{ttc_seconds:.4f}s"
            else:  # å¤§äºç­‰äº100ms
                return f"{ttc_seconds:.2f}s"
        
        with open(report_path, 'w') as f:
            f.write("="*70 + "\n")
            f.write("YOLO-First ç¢°æ’æ£€æµ‹åˆ†ææŠ¥å‘Š\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è¾“å…¥è§†é¢‘: {self.video_path}\n")
            f.write(f"Homography: {self.homography_path if self.H is not None else 'æœªæä¾›'}\n")
            f.write(f"ç»“æœç›®å½•: {self.run_dir}\n\n")
            
            f.write(f"å¤„ç†æ–¹å¼: YOLO-First\n")
            f.write(f"æµç¨‹: YOLOæ£€æµ‹ â†’ è½¨è¿¹(px) â†’ å…³é”®å¸§ â†’ Homography(å…³é”®å¸§) â†’ åˆ†æ\n\n")
            
            f.write(f"å…³é”®å¸§ç»Ÿè®¡:\n\n")
            f.write(f"æ€»æ¥è¿‘äº‹ä»¶: {len(analyzed_events)}\n")
            if analyzed_events:
                f.write(f"Level 1 (Collision): {level_counts[1]}\n")
                f.write(f"Level 2 (Near Miss): {level_counts[2]}\n")
                f.write(f"Level 3 (Avoidance): {level_counts[3]}\n\n")
            
            # æ ¹æ®TTCåˆ†ç±»äº‹ä»¶
            ttc_classified = self._classify_events_by_ttc(analyzed_events)
            
            # è¾“å‡ºåˆ†ç±»ç»“æœ
            f.write("æ ¹æ®TTCå€¼çš„ç¢°æ’é£é™©åˆ†ç±»:\n\n")
            
            # Rear-end ç¢°æ’
            if ttc_classified['rear_end_serious']:
                f.write(f"ã€Rear-end - Serious Conflict (TTC 0-2.8s)ã€‘: {len(ttc_classified['rear_end_serious'])} ä¸ª\n")
                for event in ttc_classified['rear_end_serious'][:5]:
                    ttc = event['multi_anchor_detailed'].get('ttc_seconds', 0)
                    ttc_str = format_ttc(ttc)
                    f.write(f"  Frame {event['frame']}: TTC={ttc_str}, è·ç¦»={event['multi_anchor_detailed'].get('min_distance_meters', 0):.3f}m\n")
                if len(ttc_classified['rear_end_serious']) > 5:
                    f.write(f"  ... è¿˜æœ‰ {len(ttc_classified['rear_end_serious']) - 5} ä¸ª\n")
                f.write("\n")
            
            if ttc_classified['rear_end_general']:
                f.write(f"ã€Rear-end - General Conflict (TTC 2.8-4.7s)ã€‘: {len(ttc_classified['rear_end_general'])} ä¸ª\n")
                for event in ttc_classified['rear_end_general'][:5]:
                    ttc = event['multi_anchor_detailed'].get('ttc_seconds', 0)
                    ttc_str = format_ttc(ttc)
                    f.write(f"  Frame {event['frame']}: TTC={ttc_str}, è·ç¦»={event['multi_anchor_detailed'].get('min_distance_meters', 0):.3f}m\n")
                if len(ttc_classified['rear_end_general']) > 5:
                    f.write(f"  ... è¿˜æœ‰ {len(ttc_classified['rear_end_general']) - 5} ä¸ª\n")
                f.write("\n")
            
            # Sideswipe ç¢°æ’
            if ttc_classified['sideswipe_serious']:
                f.write(f"ã€Sideswipe - Serious Conflict (TTC 0-2.3s)ã€‘: {len(ttc_classified['sideswipe_serious'])} ä¸ª\n")
                for event in ttc_classified['sideswipe_serious'][:5]:
                    ttc = event['multi_anchor_detailed'].get('ttc_seconds', 0)
                    ttc_str = format_ttc(ttc)
                    f.write(f"  Frame {event['frame']}: TTC={ttc_str}, è·ç¦»={event['multi_anchor_detailed'].get('min_distance_meters', 0):.3f}m\n")
                if len(ttc_classified['sideswipe_serious']) > 5:
                    f.write(f"  ... è¿˜æœ‰ {len(ttc_classified['sideswipe_serious']) - 5} ä¸ª\n")
                f.write("\n")
            
            if ttc_classified['sideswipe_general']:
                f.write(f"ã€Sideswipe - General Conflict (TTC 2.3-4.2s)ã€‘: {len(ttc_classified['sideswipe_general'])} ä¸ª\n")
                for event in ttc_classified['sideswipe_general'][:5]:
                    ttc = event['multi_anchor_detailed'].get('ttc_seconds', 0)
                    ttc_str = format_ttc(ttc)
                    f.write(f"  Frame {event['frame']}: TTC={ttc_str}, è·ç¦»={event['multi_anchor_detailed'].get('min_distance_meters', 0):.3f}m\n")
                if len(ttc_classified['sideswipe_general']) > 5:
                    f.write(f"  ... è¿˜æœ‰ {len(ttc_classified['sideswipe_general']) - 5} ä¸ª\n")
                f.write("\n")
            
            if not any([ttc_classified['rear_end_serious'], ttc_classified['rear_end_general'],
                       ttc_classified['sideswipe_serious'], ttc_classified['sideswipe_general']]):
                f.write("æœªæ£€æµ‹åˆ°å…·æœ‰æœ‰æ•ˆTTCå€¼çš„ç¢°æ’äº‹ä»¶\n\n")
            
            f.write("\nå‰10ä¸ªé«˜é£é™©äº‹ä»¶ï¼ˆè¯¦ç»†ä¿¡æ¯ï¼‰:\n\n")
            
            if analyzed_events:
                sorted_events = sorted(analyzed_events, key=lambda e: e.get('level', 3))
                
                for event in sorted_events[:10]:
                    f.write(f"Frame {event['frame']} ({event['time']:.2f}s)\n")
                    obj_ids = event.get('object_ids') or [event.get('track_id_1', -1), event.get('track_id_2', -1)]
                    f.write(f"ç‰©ä½“ID: {obj_ids}\n")
                    f.write(f"é£é™©ç­‰çº§: Level {event['level']} ({event.get('level_name', '?')})\n")
                    f.write(f"è·ç¦»(åƒç´ ): {event['distance_pixel']:.1f}px\n")
                    
                    if 'distance_meters' in event:
                        f.write(f"è·ç¦»(ç±³): {event['distance_meters']:.2f}m\n")
                    
                    # ä»multi_anchor_detailedä¸­æå–TTCå’Œç¢°æ’ç±»å‹ä¿¡æ¯
                    if 'multi_anchor_detailed' in event:
                        multi_anchor = event['multi_anchor_detailed']
                        ttc = multi_anchor.get('ttc_seconds')
                        approaching = multi_anchor.get('heading_analysis', {}).get('approaching', False)
                        
                        if ttc is not None and ttc > 0:
                            ttc_str = format_ttc(ttc)
                            f.write(f"TTC (æ—¶é—´ç¢°æ’): {ttc_str}\n")
                        else:
                            # æ ¹æ®approachingæ ‡å¿—åˆ¤æ–­åŸå› 
                            if approaching:
                                f.write(f"TTC (æ—¶é—´ç¢°æ’): æ— æ³•è®¡ç®— / Insufficient Speed\n")
                            else:
                                f.write(f"TTC (æ—¶é—´ç¢°æ’): è¿œç¦» / Separating\n")
                        
                        closest_parts = multi_anchor.get('closest_parts', {})
                        if 'description' in closest_parts:
                            f.write(f"ç¢°æ’éƒ¨ä½: {closest_parts['description']}\n")
                        
                        min_dist = multi_anchor.get('min_distance_meters')
                        if min_dist is not None:
                            f.write(f"æœ€å°è·ç¦»: {min_dist:.3f}m\n")
                    
                    f.write("\n")
            else:
                f.write("æœªæ£€æµ‹åˆ°æ¥è¿‘äº‹ä»¶\n\n")
            
            f.write("="*70 + "\n\n")
            
            # TTC åˆ†çº§æ ‡å‡†è¡¨
            f.write("TTC (æ—¶é—´ç¢°æ’) åˆ†çº§æ ‡å‡†å‚è€ƒ:\n\n")
            f.write("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n")
            f.write("â”‚ ç¢°æ’ç±»å‹         â”‚ ä¸¥é‡ç¨‹åº¦         â”‚ TTCé˜ˆå€¼ (ç§’)     â”‚\n")
            f.write("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n")
            f.write("â”‚ Rear-end        â”‚ Serious conflict â”‚ 0 â€“ 2.8 s        â”‚\n")
            f.write("â”‚ (è¿½å°¾)          â”‚ General conflict â”‚ 2.8 â€“ 4.7 s      â”‚\n")
            f.write("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n")
            f.write("â”‚ Sideswipe       â”‚ Serious conflict â”‚ 0 â€“ 2.3 s        â”‚\n")
            f.write("â”‚ (ä¾§é¢ç¢°æ’)      â”‚ General conflict â”‚ 2.3 â€“ 4.2 s      â”‚\n")
            f.write("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n")
            
            f.write("="*70 + "\n")
            f.write("æŠ¥å‘Šç»“æŸ\n")
        
        print(f"\n  âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path.name}")
    
    def _classify_events_by_ttc(self, analyzed_events):
        """æ ¹æ®TTCå€¼å’Œç›¸å¯¹æ–¹å‘åˆ¤æ–­ç¢°æ’ç±»å‹å’Œä¸¥é‡ç¨‹åº¦"""
        classified = {
            'rear_end_serious': [],      # TTC 0-2.8s
            'rear_end_general': [],      # TTC 2.8-4.7s
            'sideswipe_serious': [],     # TTC 0-2.3s
            'sideswipe_general': [],     # TTC 2.3-4.2s
            'no_ttc': []                 # æ²¡æœ‰æœ‰æ•ˆTTC
        }
        
        for event in analyzed_events:
            if 'multi_anchor_detailed' not in event:
                classified['no_ttc'].append(event)
                continue
            
            ttc = event['multi_anchor_detailed'].get('ttc_seconds')
            if ttc is None or ttc <= 0:
                classified['no_ttc'].append(event)
                continue
            
            # æ ¹æ®ç›¸å¯¹headingåˆ¤æ–­æ˜¯rear-endè¿˜æ˜¯sideswipe
            # headingæ¥è¿‘0æˆ–Ï€ = rear-end (å‰åå‘)
            # headingæ¥è¿‘Ï€/2æˆ–-Ï€/2 = sideswipe (ä¾§å‘)
            relative_heading = event['multi_anchor_detailed'].get('heading_analysis', {}).get('relative_heading_rad', 0)
            
            # å°†headingæ ‡å‡†åŒ–åˆ°[-Ï€, Ï€]
            import math
            heading_abs = abs(relative_heading)
            is_sideswipe = heading_abs > math.pi / 4  # å¤§äº45åº¦åˆ™åˆ¤å®šä¸ºä¾§å‘
            
            if is_sideswipe:
                # Sideswipe ç¢°æ’
                if ttc < 2.3:
                    classified['sideswipe_serious'].append(event)
                elif ttc < 4.2:
                    classified['sideswipe_general'].append(event)
                else:
                    classified['no_ttc'].append(event)
            else:
                # Rear-end ç¢°æ’
                if ttc < 2.8:
                    classified['rear_end_serious'].append(event)
                elif ttc < 4.7:
                    classified['rear_end_general'].append(event)
                else:
                    classified['no_ttc'].append(event)
        
        return classified
    
    def _copy_results_to_workspace(self):
        """è‡ªåŠ¨å¤åˆ¶ç»“æœåˆ° /workspace/ultralytics/resultsï¼ˆä½¿å…¶åœ¨ VS Code ä¸­å¯è§ï¼‰"""
        import shutil
        
        workspace_results = Path("/workspace/ultralytics/results")
        if workspace_results.exists() and self.run_dir.parent != workspace_results:
            try:
                # æ£€æŸ¥ç»“æœæ˜¯å¦å·²åœ¨ workspace_results ä¸­
                result_in_workspace = workspace_results / self.run_dir.name
                if not result_in_workspace.exists():
                    shutil.copytree(self.run_dir, result_in_workspace)
                    print(f"\n  âœ“ ç»“æœå·²å¤åˆ¶åˆ°: {result_in_workspace}")
                    print(f"    ç°åœ¨å¯ä»¥åœ¨ VS Code ä¸­ç›´æ¥æŸ¥çœ‹ï¼")
            except Exception as e:
                print(f"\n  âš ï¸  å¤åˆ¶å¤±è´¥ ({e})ï¼Œä½†ç»“æœå·²ä¿å­˜åœ¨: {self.run_dir}")
    
    # =========================================================================
    # ç®¡é“ç¼–æ’
    # =========================================================================
    
    def run(self, conf_threshold=0.45):
        """è¿è¡Œå®Œæ•´ YOLO-First ç®¡é“ (Method A)"""
        try:
            # Step 0: åŠ è½½Homography (å¦‚æœæä¾›)
            if self.homography_path:
                print(f"\nã€Step 0: åŠ è½½èµ„æºã€‘")
                self.load_homography()
            
            # Step 1: YOLO æ£€æµ‹
            all_detections = self.run_yolo_detection(conf_threshold)
            
            if not all_detections:
                print(f"\nâŒ æœªæ£€æµ‹åˆ°ä»»ä½•ç‰©ä½“ï¼Œåœæ­¢å¤„ç†")
                return
            
            # Step 1.5: åŒå¸§å†…ç‰©ä½“åˆ†å‰²åˆå¹¶  - åˆå¹¶YOLOåˆ†å‰²çš„åŒç±»ç‰©ä½“
            all_detections = self.merge_fragmented_objects_in_frame(all_detections, same_class_distance_threshold=100)
            
            # è°ƒè¯•ï¼šæ£€æŸ¥Step 1.5åçš„æ•°æ®
            print(f"\nã€è°ƒè¯•: Step 1.5åçš„æ•°æ®ã€‘")
            for frame_data in all_detections:
                if frame_data['frame'] == 115:
                    frame_115_objects_after_15 = [(obj['track_id'], obj['class']) for obj in frame_data['objects']]
                    print(f"  Frame 115 Step 1.5å: {frame_115_objects_after_15}")
            
            # Step 2: è½¨è¿¹æ„å»º
            tracks = self.build_trajectories(all_detections)
            
            # Step 2.4: è½¨è¿¹é—´æ–­æ£€æµ‹  - æ£€æµ‹å‡ºç°â†’æ¶ˆå¤±â†’é‡æ–°å‡ºç°çš„å¯ç–‘è½¨è¿¹
            suspicious_tracks = self.detect_discontinuous_tracks(all_detections, max_gap_frames=3)
            
            # Step 2.5: è½¨è¿¹è¿ç»­æ€§è¿‡æ»¤  - ç§»é™¤çŸ­è½¨è¿¹è¯¯æ£€
            all_detections = self.filter_short_tracks(all_detections, min_track_length=self.min_track_length)
            
            # è°ƒè¯•ï¼šæ£€æŸ¥Step 2.5åçš„æ•°æ®
            print(f"\nã€è°ƒè¯•: Step 2.5åçš„æ•°æ®ã€‘")
            frame_115_objects = []
            frame_148_objects = []
            for frame_data in all_detections:
                if frame_data['frame'] == 115:
                    frame_115_objects = [(obj['track_id'], obj['class']) for obj in frame_data['objects']]
                if frame_data['frame'] == 148:
                    frame_148_objects = [(obj['track_id'], obj['class']) for obj in frame_data['objects']]
            print(f"  Frame 115: {frame_115_objects}")
            print(f"  Frame 148: {frame_148_objects}")
            
            # âš ï¸ å…³é”®ï¼šStep 2.5è¿‡æ»¤åéœ€è¦é‡æ–°æ„å»ºè½¨è¿¹ï¼Œå¦åˆ™Step 3.6ä¼šæ‰¾ä¸åˆ°å¯¹è±¡
            tracks = self.build_trajectories(all_detections)
            
            # Step 3: å…³é”®å¸§æ£€æµ‹ (Option B: ä½¿ç”¨Step 2ä¿å­˜çš„è½¨è¿¹worldåæ ‡)
            proximity_events = self.extract_key_frames(all_detections, tracks, world_distance_threshold=4.5)
            
            if not proximity_events:
                print(f"\nâš ï¸  æœªæ£€æµ‹åˆ°æ¥è¿‘äº‹ä»¶")
                analyzed_events = []
                level_counts = {0: 0, 1: 0, 2: 0, 3: 0}
            else:
                # Step 3.5: åŒç±»åˆ«ç‰©ä½“è¯¯æ£€è¿‡æ»¤ âœ¨ æ–°å¢
                filtered_events = self.filter_same_class_false_positives(proximity_events, same_class_distance_threshold=0.3)
                
                # æ¸…ç†ï¼šåˆ é™¤è¢«è¿‡æ»¤æ‰çš„å…³é”®å¸§å›¾ç‰‡
                if len(filtered_events) < len(proximity_events):
                    self.cleanup_filtered_keyframes(proximity_events, filtered_events)
                
                # Step 3.6: å¤šé”šç‚¹ç¢°æ’åˆ†æ (ä»…å…³é”®å¸§)
                try:
                    filtered_events = self.analyze_keyframes_with_multi_anchor(filtered_events, all_detections, tracks)
                except Exception as e:
                    print(f"\n  âš ï¸  Step 3.6 å¤šé”šç‚¹åˆ†æå¤±è´¥: {e}")
                    print(f"     ç»§ç»­ä½¿ç”¨ç®€å•çš„ä¸­å¿ƒç‚¹è·ç¦»åˆ†æç»“æœ")
                
                # ä¿å­˜æœ€ç»ˆçš„proximity_eventsï¼ˆåŒ…å«å¤šé”šç‚¹åˆ†æç»“æœï¼‰
                events_path = self.keyframe_dir / 'proximity_events.json'
                with open(events_path, 'w') as f:
                    json.dump(filtered_events, f, indent=2)
                
                # é‡æ–°ç»˜åˆ¶å…³é”®å¸§ï¼ˆç°åœ¨åŒ…å«å¤šé”šç‚¹å¯è§†åŒ–ï¼‰
                for event in filtered_events:
                    frame_num = event['frame']
                    tid1 = event['track_id_1']
                    tid2 = event['track_id_2']
                    frame_img_path = self.keyframe_dir / f"keyframe_{frame_num:04d}_ID{tid1}_ID{tid2}.jpg"
                    self.save_keyframe_with_distance(self.video_path, frame_num, frame_img_path, event)
                
                # STEP 3.7: å¤šé”šç‚¹è·ç¦»è¿‡æ»¤ï¼ˆåœ¨è¿™é‡Œæ‰§è¡Œï¼Œä¸æ˜¯åœ¨Step 5ï¼‰
                print(f"\nã€Step 3.7: å¤šé”šç‚¹è·ç¦»è¿‡æ»¤ (â‰¤1.0m)ã€‘")
                anchor_filtered_events = []
                removed_reasons = {'no_anchor_data': [], 'distance_too_far': []}
                
                for event in filtered_events:
                    frame = event['frame']
                    tid1 = event['track_id_1']
                    tid2 = event['track_id_2']
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¤šé”šç‚¹åˆ†ææ•°æ®
                    if 'multi_anchor_detailed' not in event:
                        removed_reasons['no_anchor_data'].append((frame, tid1, tid2))
                        continue
                    
                    multi = event['multi_anchor_detailed']
                    min_distance = multi.get('min_distance_meters', float('inf'))
                    
                    # ä¿ç•™è·ç¦» â‰¤ 1.0m çš„äº‹ä»¶ï¼ˆé«˜é£é™©ï¼‰
                    if min_distance <= 1.0:
                        anchor_filtered_events.append(event)
                    else:
                        removed_reasons['distance_too_far'].append((frame, tid1, tid2, min_distance))
                
                # æŠ¥å‘Šè¢«è¿‡æ»¤çš„äº‹ä»¶
                if removed_reasons['no_anchor_data']:
                    print(f"  âŠ— ç§»é™¤ {len(removed_reasons['no_anchor_data'])} ä¸ªæ— å¤šé”šç‚¹æ•°æ®çš„äº‹ä»¶")
                
                if removed_reasons['distance_too_far']:
                    print(f"  âŠ— ç§»é™¤ {len(removed_reasons['distance_too_far'])} ä¸ªè·ç¦»>1.0mçš„äº‹ä»¶:")
                    for frame, tid1, tid2, dist in removed_reasons['distance_too_far']:
                        print(f"     - Frame {frame}: ID{tid1}+ID{tid2} (é”šç‚¹è·ç¦»={dist:.2f}m)")
                
                filtered_count = len(filtered_events) - len(anchor_filtered_events)
                print(f"  ğŸ” å¤šé”šç‚¹è·ç¦»è¿‡æ»¤: æ’é™¤ {filtered_count} ä¸ªäº‹ä»¶")
                print(f"  âœ“ Step 3.7å®Œæˆ: ä¿ç•™ {len(anchor_filtered_events)} ä¸ªå…³é”®å¸§ (â‰¤ 1.0m)")
                
                # æ¸…ç†è¢«è¿‡æ»¤æ‰çš„å…³é”®å¸§å›¾ç‰‡
                self.cleanup_filtered_keyframes(filtered_events, anchor_filtered_events)
                
                # ä¿å­˜Step 3.7åçš„æœ€ç»ˆå…³é”®å¸§JSON
                events_path = self.keyframe_dir / 'proximity_events.json'
                with open(events_path, 'w') as f:
                    json.dump(anchor_filtered_events, f, indent=2)
                
                # ç”¨Step 3.7è¿‡æ»¤åçš„äº‹ä»¶ç»§ç»­åç»­æ­¥éª¤
                filtered_events = anchor_filtered_events
                # Step 4: Homography å˜æ¢ (ä»…å…³é”®å¸§)
                if self.H is not None:
                    transformed_events = self.transform_key_frames_to_world(filtered_events)
                else:
                    print(f"\nã€Step 4: Homography å˜æ¢ã€‘")
                    print(f"  âš ï¸  è·³è¿‡ (æœªåŠ è½½Homography)")
                    transformed_events = filtered_events
                
                # Step 5: é£é™©åˆ†æ
                analyzed_events, level_counts = self.analyze_collision_risk(transformed_events if transformed_events else filtered_events)
            
            # ç”ŸæˆæŠ¥å‘Š
            self.generate_report(proximity_events, analyzed_events, level_counts)
            
            print(f"\n{'='*70}")
            print(f"âœ“ YOLO-First Pipeline (Method A) å®Œæˆï¼")
            print(f"{'='*70}")
            print(f"ç»“æœä¿å­˜åœ¨: {self.run_dir}")
            
            # è‡ªåŠ¨å¤åˆ¶ç»“æœåˆ° /workspace/ultralytics/resultsï¼ˆå¦‚æœä¸åŒçš„è¯ï¼‰
            self._copy_results_to_workspace()
            
            print(f"\næ–‡ä»¶å¤¹ç»“æ„:")
            print(f"  1_yolo_detection/")
            print(f"    â”œâ”€â”€ detections_pixel.json")
            print(f"    â”œâ”€â”€ detection_stats.json")
            print(f"    â””â”€â”€ *.jpg (æ‰€æœ‰æœ‰æ£€æµ‹çš„å¸§)")
            print(f"  2_trajectories/")
            print(f"    â”œâ”€â”€ tracks.json")
            print(f"    â””â”€â”€ track_stats.json")
            print(f"  3_key_frames/")
            print(f"    â”œâ”€â”€ proximity_events.json")
            print(f"    â””â”€â”€ *.jpg (æ¥è¿‘äº‹ä»¶çš„å…³é”®å¸§)")
            print(f"  4_homography_transform/")
            print(f"    â”œâ”€â”€ homography.json")
            print(f"    â””â”€â”€ transformed_key_frames.json")
            print(f"  5_collision_analysis/")
            print(f"    â”œâ”€â”€ collision_events.json")
            print(f"    â””â”€â”€ analysis_report.txt")
            
        except Exception as e:
            print(f"\nâŒ Pipeline é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='YOLO-First ç¢°æ’æ£€æµ‹Pipeline (Method A - å¯¼å¸ˆæ¨è)')
    parser.add_argument('--video', type=str, required=True, help='è¾“å…¥è§†é¢‘è·¯å¾„')
    parser.add_argument('--homography', type=str, default=None, 
                       help='Homography JSONè·¯å¾„ (å¯é€‰)')
    parser.add_argument('--output', type=str, default='../../results', 
                       help='ç»“æœåŸºç¡€ç›®å½•')
    parser.add_argument('--conf', type=float, default=0.45, 
                       help='YOLOç½®ä¿¡åº¦é˜ˆå€¼ (è¶Šé«˜=è¶Šä¸¥æ ¼ï¼Œå‡å°‘è¯¯æ£€) (é»˜è®¤: 0.45)')
    parser.add_argument('--skip-frames', type=int, default=3,
                       help='æŠ½å¸§å‚æ•°: 3=æ¯éš”3å¸§å¤„ç†1å¸§, 5=æ¯éš”5å¸§å¤„ç†1å¸§ (æœ€å°å€¼ä¸º3ï¼Œç”¨äºæé«˜é€Ÿåº¦è®¡ç®—å‡†ç¡®æ€§) (é»˜è®¤: 3)')
    parser.add_argument('--model', type=str, default='yolo11m',
                       help='YOLO æ¨¡å‹: yolo11n(å¿«é€Ÿ), yolo11m(ä¸­ç­‰,æ›´ç²¾ç¡®), yolo11l(æœ€ç²¾ç¡®) (é»˜è®¤: yolo11m)')
    parser.add_argument('--min-track-length', type=int, default=3,
                       help='æœ€å°è½¨è¿¹é•¿åº¦(å¸§æ•°)ï¼ŒçŸ­äºæ­¤çš„è½¨è¿¹è¢«è®¤ä¸ºæ˜¯è¯¯æ£€å¹¶æ’é™¤ (é»˜è®¤: 3)')
    
    args = parser.parse_args()
    
    pipeline = YOLOFirstPipelineA(args.video, args.homography, args.output, 
                                  skip_frames=args.skip_frames, 
                                  model=args.model,
                                  min_track_length=args.min_track_length)
    pipeline.run(args.conf)
