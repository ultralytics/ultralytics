# åŸºæœ¬é…ç½®

- æ“ä½œç³»ç»Ÿï¼šubuntu25.04
- æ“ä½œå†…æ ¸ï¼š6.14.0-1006-intel
- GPUï¼šè“æˆŸæˆ–é“­ç‘„B60Proï¼Œå®é™…ä¸Šä¸ç¡¬ä»¶æ— å…³ï¼Œå› ä¸ºtorchå¹¶æ²¡æœ‰å’ŒGPUç»‘å®šä½†æ˜¯é©±åŠ¨å®‰è£…æ¯”è¾ƒå¤æ‚ï¼Œæ‰€ä»¥åªè¦æ‚¨èƒ½è£…ä¸Šé©±åŠ¨ï¼ŒåŸºæœ¬ä¸Šå¯ä»¥æ­£å¸¸è¿›è¡Œæ“ä½œ
- é©±åŠ¨åŠå®‰è£…æ•™ç¨‹ï¼šhttps://github.com/intel/llm-scaler/blob/main/vllm/README.md/#1-getting-started-and-usagexit
- é©±åŠ¨ç‰ˆæœ¬ï¼šmulti-arc-bmg-offline-installer-25.38.4.1

# å¦‚ä½•å®‰è£…ç¯å¢ƒ

- å®šåˆ¶åŒ–å®‰è£…torchæ”¯æŒï¼šhttps://pytorch-extension.intel.com/installation
- å®‰è£…å’Œæˆ‘ä¸€æ ·çš„torchç‰ˆæœ¬æ”¯æŒ

```bash
cd ultralytics
pip install -e .
pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/xpu
# ä¸‹é¢ä¸¤ä¸ªå¯ä»¥ä¸è£…ï¼Œå› ä¸ºç›®å‰ä¸æ”¯æŒå¤šå¡è®­ç»ƒ
# pip install intel-extension-for-pytorch==2.8.10+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/
# pip install oneccl_bind_pt==2.8.0+xpu --index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/
```

- éªŒè¯æ‚¨æ˜¯å¦å®‰è£…æˆåŠŸ

```bash
(B60) root@b60:~/ultralytics# python
Python 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>> print(torch.version.xpu)
20250101
>>> print(torch.xpu.is_available())
True
>>> print(torch.xpu.get_device_name(0))
Intel(R) Graphics [0xe211]
```

# ä»£ç ä¿®æ”¹å¤„

- ultralytics/ultralytics/utils/torch_utils.py/ time_sync()
- ç›®çš„ï¼šæ—¶é—´åŒæ­¥

```python
def time_sync():
    """Return PyTorch-accurate time."""
    if hasattr(torch, "xpu") and torch.xpu.is_available():
        try:
            torch.xpu.synchronize()
        except Exception:
            pass
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    return time.time()
```

- æµ‹è¯•ç”¨ä¾‹ï¼šæ— 

---

- ultralytics/ultralytics/utils/torch_utils.py/ get_gpu_info()
- ç›®çš„ï¼šæ”¯æŒè§£æxpuä¿¡æ¯

```python
@functools.lru_cache
def get_gpu_info(index):
    """Return a string with system GPU information, i.e. 'Tesla T4, 15102MiB'."""
    properties = torch.cuda.get_device_properties(index)
    if hasattr(torch, "xpu") and torch.xpu.is_available():
        properties = torch.xpu.get_device_properties(index)
    return f"{properties.name}, {properties.total_memory / (1 << 20):.0f}MiB"
```

- æµ‹è¯•ç”¨ä¾‹ï¼šä¸è®­ç»ƒä»£ç ç›¸åŒ
- æµ‹è¯•ç»“æœï¼šä¿®æ”¹æ­¤å¤„åä¼šæ”¯æŒåœ¨è®­ç»ƒçš„æ—¶å€™è¾“å‡ºæ˜¾å¡ä¿¡æ¯

```bash
Ultralytics 8.3.231 ğŸš€ Python-3.10.19 torch-2.8.0+xpu XPU:0 (Intel(R) Graphics [0xe211]
```

---

- ultralytics/ultralytics/utils/torch_utils.py/ select_device()
- é€‰æ‹©GPUçš„æ—¶å€™æ”¯æŒé€‰æ‹©xpuï¼Œç›®å‰ä½ å¯ä»¥å¡«å†™å¤šå—GPUï¼Œä½†åªä¼šé»˜è®¤ä½¿ç”¨0å¡

```python
def select_device(device="", newline=False, verbose=True):
    """Select the appropriate PyTorch device based on the provided arguments.

    The function takes a string specifying the device or a torch.device object and returns a torch.device object
    representing the selected device. The function also validates the number of available devices and raises an
    exception if the requested device(s) are not available.

    Args:
        device (str | torch.device, optional): Device string or torch.device object. Options are 'None', 'cpu', or
            'cuda', or '0' or '0,1,2,3'. Auto-selects the first available GPU, or CPU if no GPU is available.
        newline (bool, optional): If True, adds a newline at the end of the log string.
        verbose (bool, optional): If True, logs the device information.

    Returns:
        (torch.device): Selected device.

    Examples:
        >>> select_device("cuda:0")
        device(type='cuda', index=0)

        >>> select_device("cpu")
        device(type='cpu')

    Notes:
        Sets the 'CUDA_VISIBLE_DEVICES' environment variable for specifying which GPUs to use.
    """
    if isinstance(device, torch.device) or str(device).startswith(("tpu", "intel")):
        return device

    s = f"Ultralytics {__version__} ğŸš€ Python-{PYTHON_VERSION} torch-{TORCH_VERSION} "
    for remove in "cuda:", "none", "(", ")", "[", "]", "'", " ":
        device = device.replace(remove, "")  # to string, 'cuda:0' -> '0' and '(0, 1)' -> '0,1'

    # Auto-select GPUs
    if "-1" in device:
        from ultralytics.utils.autodevice import GPUInfo

        # Replace each -1 with a selected GPU or remove it
        parts = device.split(",")
        selected = GPUInfo().select_idle_gpu(count=parts.count("-1"), min_memory_fraction=0.2)
        for i in range(len(parts)):
            if parts[i] == "-1":
                parts[i] = str(selected.pop(0)) if selected else ""
        device = ",".join(p for p in parts if p)

    cpu = device == "cpu"
    mps = device in {"mps", "mps:0"}  # Apple Metal Performance Shaders (MPS)

    if cpu or mps:
        os.environ["CUDA_VISIBLE_DEVICES"] = ""  # force torch.cuda.is_available() = False
    # â­æ–°å¢ï¼šæ—  CUDA ä½†æœ‰ XPU æ—¶ï¼Œè®© device="0" èµ° XPU
    elif device and not torch.cuda.is_available() and hasattr(torch, "xpu") and torch.xpu.is_available():
        try:
            idx = int(device)
            if idx >= torch.xpu.device_count():
                raise ValueError(f"XPU index {idx} out of range")
            p = torch.xpu.get_device_properties(idx)
            mem = p.total_memory / (1 << 10)
            s += f"XPU:{idx} ({p.name}, {mem:.0f}MiB)\n"
            if verbose:
                LOGGER.info(s)
            return torch.device(f"xpu:{idx}")
        except:
            pass

    # â­ä¿®æ”¹ï¼šåªæœ‰ CUDA å¯ç”¨æ—¶æ‰èµ° CUDA åˆ†æ”¯
    elif device and torch.cuda.is_available():
        if device == "cuda":
            device = "0"
        if "," in device:
            device = ",".join([x for x in device.split(",") if x])
        visible = os.environ.get("CUDA_VISIBLE_DEVICES", None)
        os.environ["CUDA_VISIBLE_DEVICES"] = device

        if not (torch.cuda.is_available() and torch.cuda.device_count() >= len(device.split(","))):
            LOGGER.info(s)
            install = (
                "See https://pytorch.org/get-started/locally/ for up-to-date torch install instructions.\n"
                if torch.cuda.device_count() == 0
                else ""
            )
            raise ValueError(
                f"Invalid CUDA 'device={device}'."
                f"\ntorch.cuda.is_available(): {torch.cuda.is_available()}"
                f"\ntorch.cuda.device_count(): {torch.cuda.device_count()}"
                f"\nos.environ['CUDA_VISIBLE_DEVICES']: {visible}\n"
                f"{install}"
            )

    if not cpu and not mps and torch.cuda.is_available():  # prefer GPU if available
        devices = device.split(",") if device else "0"  # i.e. "0,1" -> ["0", "1"]
        space = " " * len(s)
        for i, d in enumerate(devices):
            s += f"{'' if i == 0 else space}CUDA:{d} ({get_gpu_info(i)})\n"  # bytes to MB
        arg = "cuda:0"
    elif mps and TORCH_2_0 and torch.backends.mps.is_available():
        # Prefer MPS if available
        s += f"MPS ({get_cpu_info()})\n"
        arg = "mps"
    elif hasattr(torch, "xpu") and torch.xpu.is_available():
        # Default auto-detect XPU
        props = torch.xpu.get_device_properties(0)
        mem = props.total_memory / (1 << 10)
        s += f"XPU:0 ({props.name}, {mem:.0f}MiB)\n"
        arg = "xpu"
    else:  # revert to CPU
        s += f"CPU ({get_cpu_info()})\n"
        arg = "cpu"
    if arg in {"cpu", "mps"}:
        torch.set_num_threads(NUM_THREADS)  # reset OMP_NUM_THREADS for cpu training
    if verbose:
        LOGGER.info(s if newline else s.rstrip())
    return torch.device(arg)
```

- æµ‹è¯•ç”¨ä¾‹ï¼šè®­ç»ƒæ—¶ä¿®æ”¹deviceå‚æ•°

```python
from ultralytics import YOLO

model = YOLO("/root/ultralytics/ultralytics/cfg/models/v8/yolov8n.yaml")
model.train(
    data="coco128.yaml",
    epochs=50,
    imgsz=256,
    # device="xpuï¼š1"
    device="xpu",
)
```

- æµ‹è¯•ç»“æœï¼šæ”¯æŒè®­ç»ƒ

---

- ultralytics/ultralytics/utils/checks.py/ check_amp()
- ç›®çš„ç¦æ­¢AMPæ£€æŸ¥

```python
def check_amp(model):
    """Check the PyTorch Automatic Mixed Precision (AMP) functionality of a YOLO model.

    If the checks fail, it means there are anomalies with AMP on the system that may cause NaN losses or zero-mAP
    results, so AMP will be disabled during training.

    Args:
        model (torch.nn.Module): A YOLO model instance.

    Returns:
        (bool): Returns True if the AMP functionality works correctly with YOLO11 model, else False.

    Examples:
        >>> from ultralytics import YOLO
        >>> from ultralytics.utils.checks import check_amp
        >>> model = YOLO("yolo11n.pt").model.cuda()
        >>> check_amp(model)
    """
    from ultralytics.utils.torch_utils import autocast

    device = next(model.parameters()).device  # get model device
    prefix = colorstr("AMP: ")
    if hasattr(torch, "xpu") and torch.xpu.is_available() and device.type == "xpu":
        LOGGER.warning(f"{prefix}Intel XPU detected. AMP is disabled (not supported on XPU).")
        return False

    if device.type in {"cpu", "mps"}:
        return False  # AMP only used on CUDA devices
    else:
        # GPUs that have issues with AMP
        pattern = re.compile(
            r"(nvidia|geforce|quadro|tesla).*?(1660|1650|1630|t400|t550|t600|t1000|t1200|t2000|k40m)", re.IGNORECASE
        )

        gpu = torch.cuda.get_device_name(device)
        if bool(pattern.search(gpu)):
            LOGGER.warning(
                f"{prefix}checks failed âŒ. AMP training on {gpu} GPU may cause "
                f"NaN losses or zero-mAP results, so AMP will be disabled during training."
            )
            return False

    def amp_allclose(m, im):
        """All close FP32 vs AMP results."""
        batch = [im] * 8
        imgsz = max(256, int(model.stride.max() * 4))  # max stride P5-32 and P6-64
        a = m(batch, imgsz=imgsz, device=device, verbose=False)[0].boxes.data  # FP32 inference
        with autocast(enabled=True):
            b = m(batch, imgsz=imgsz, device=device, verbose=False)[0].boxes.data  # AMP inference
        del m
        return a.shape == b.shape and torch.allclose(a, b.float(), atol=0.5)  # close to 0.5 absolute tolerance

    im = ASSETS / "bus.jpg"  # image to check
    LOGGER.info(f"{prefix}running Automatic Mixed Precision (AMP) checks...")
    warning_msg = "Setting 'amp=True'. If you experience zero-mAP or NaN losses you can disable AMP with amp=False."
    try:
        from ultralytics import YOLO

        assert amp_allclose(YOLO("yolo11n.pt"), im)
        LOGGER.info(f"{prefix}checks passed âœ…")
    except ConnectionError:
        LOGGER.warning(f"{prefix}checks skipped. Offline and unable to download YOLO11n for AMP checks. {warning_msg}")
    except (AttributeError, ModuleNotFoundError):
        LOGGER.warning(
            f"{prefix}checks skipped. "
            f"Unable to load YOLO11n for AMP checks due to possible Ultralytics package modifications. {warning_msg}"
        )
    except AssertionError:
        LOGGER.error(
            f"{prefix}checks failed. Anomalies were detected with AMP on your system that may lead to "
            f"NaN losses or zero-mAP results, so AMP will be disabled during training."
        )
        return False
    return True
```

- æµ‹è¯•ç”¨ä¾‹ï¼šæ­£å¸¸è®­ç»ƒ
- æµ‹è¯•ç»“æœï¼šæ˜¾ç¤ºå¦‚ä¸‹å†…å®¹ä¸æ”¯æŒAMP

```bash
WARNING âš ï¸ AMP: Intel XPU detected. AMP is disabled (not supported on XPU).
```

---

- ultralytics/engine/trainer.py \_clear_memory()
- æ”¯æŒæ¸…é™¤æ˜¾å­˜ï¼Œä½†æ˜¯æˆ‘ä¸å¾—ä¸è¯´ï¼Œç”±äºæˆ‘ä»¬çš„ä¼ å‚ä¼šæ”¹å˜æ•°æ®ç±»å‹ï¼Œæ‰€ä»¥å¾ˆå¯èƒ½ä¼ é€’åˆ°è¿™é‡Œçš„å‚æ•°æ˜¯0æˆ–è€…1ï¼Œé‚£å°±æ°¸è¿œä¸ä¼šç”¨åˆ°è¿™ä¸ª
- æ‰€ä»¥å¦‚æœåé¢éœ€è¦æ”¯æŒå¤šå¡è®­ç»ƒï¼Œæˆ‘ä»¬å¾—æ”¹å˜ä¸€ä¸‹å‚æ•°ä¼ é€’çš„è¿‡ç¨‹

```python
    def _clear_memory(self, threshold: float | None = None):
        """Clear accelerator memory by calling garbage collector and emptying cache."""
        if threshold:
            assert 0 <= threshold <= 1, "Threshold must be between 0 and 1."
            if self._get_memory(fraction=True) <= threshold:
                return
        gc.collect()
        if self.device.type == "mps":
            torch.mps.empty_cache()
        elif self.device.type == "cpu":
            return
        elif self.device.type == "xpu":
            torch.xpu.empty_cache()
        else:
            torch.cuda.empty_cache()
```

> [!WARNING]
> è¿™æ˜¯ä¸€ä¸ªè­¦å‘Šå†…å®¹ï¼Œè¯·æ³¨æ„è¿™é‡Œçš„è¯´æ˜ã€‚

- æµ‹è¯•ç”¨ä¾‹ï¼šå°†æ˜¾å¡å¡ä¸Šä¸€ä¸ªæ¨¡å‹å†è®­ç»ƒ
- æµ‹è¯•ç»“æœï¼šintelbugï¼Œå½“æ˜¾å­˜è¢«å æ»¡æ—¶ï¼Œä¼šé©±é€ä¸æ´»è·ƒçš„æ˜¾å­˜è‡³å†…å­˜ä¸Šï¼Œæ•…æ— æ³•æµ‹è¯•ï¼Œæ­¤é—®é¢˜æˆ‘å·²ç»åœ¨ä¸€ä¸ªæœˆå‰åé¦ˆintelç›¸å…³å›¢é˜Ÿ
- æ­¤é—®é¢˜ä¸åº•å±‚é©±åŠ¨æœ‰å…³ï¼Œæš‚æ—¶æ— æ³•éªŒè¯ï¼Œå¾…å®š

---

- ultralytics/engine/trainer.py \_get_memory()
- ç›®çš„ï¼šä¿®æ”¹å¾—åˆ°æ˜¾å­˜çš„æ–¹å¼

```python
    def _get_memory(self, fraction=False):
        """Get accelerator memory utilization in GB or as a fraction of total memory."""
        memory, total = 0, 0
        if self.device.type == "xpu":
            try:
                idx = self.device.index if isinstance(self.device, torch.device) else int(self.device)
                memory = torch.xpu.memory_allocated(idx)
                total = torch.xpu.get_device_properties(self.device).total_memory
                return ((memory / total) if total > 0 else 0) if fraction else (memory / 2**30)
            except Exception:
                return 0.0
        if self.device.type == "mps":
            memory = torch.mps.driver_allocated_memory()
            if fraction:
                return __import__("psutil").virtual_memory().percent / 100
        elif self.device.type != "cpu":
            memory = torch.cuda.memory_reserved()
            if fraction:
                total = torch.cuda.get_device_properties(self.device).total_memory
        return ((memory / total) if total > 0 else 0) if fraction else (memory / 2**30)
```

- é’ˆå¯¹è¿™æ®µä»£ç ï¼Œæˆ‘å°†æä¾›æœ€å°éªŒè¯è¯æ˜æ˜¾å­˜è®¡ç®—çš„æ–¹å¼æ²¡æœ‰é—®é¢˜ï¼Œä»–æ˜¯ä»¥å­—èŠ‚çš„æ–¹å¼å±•ç¤ºæ•°æ®

```bash
(B60) root@b60:~/ultralytics# python
Python 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
Ctrl click to launch VS Code Native REPL
>>> import torch
>>> torch.xpu.get_device_properties(0).total_memory
24385683456
>>> x = torch.randn((1024, 1024, 256), device="xpu")
>>> torch.xpu.memory_allocated(0)
1073741824
```

> [!WARNING]
> è¿™æ˜¯ä¸€ä¸ªè­¦å‘Šå†…å®¹ï¼Œè¯·æ³¨æ„è¿™é‡Œçš„è¯´æ˜ã€‚
> ä½†æ˜¯åœ¨å®é™…è¿è¡Œæ—¶ï¼Œå½“æˆ‘ä½¿ç”¨yolov8xè¿è¡Œï¼Œå¹¶ä¸”å›¾åƒè¾“å…¥ä¸º640æ—¶ï¼Œä»–çš„æ˜¾å­˜å ç”¨éå¸¸å°ï¼Œä½†æˆ‘è§‰å¾—æˆ‘çš„ä»£ç æ²¡æœ‰é—®é¢˜

```bash
Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
1/50      1.18G      3.651       5.77        4.3        162        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 3.4s/it 27.1s
Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 75% â”â”â”â”â”â”â”â”â”â”€â”€â”€ 3/4 4.0s/it 6.4s<4.0s
```

---

# åˆå¹¶æµ‹è¯•ç”¨ä¾‹

- æµ‹è¯•ç”¨ä¾‹

```bash
/root/anaconda3/envs/B60/bin/python -m pytest -s -q test.py
```

- å‘½åä¸ºtest.pyï¼Œå¹¶åˆ›å»ºæ–‡ä»¶

```python
import pytest
import torch

from ultralytics import YOLO

pytestmark = pytest.mark.skipif(
    not hasattr(torch, "xpu") or not torch.xpu.is_available(),
    reason="XPU not available",
)


def test_yolo_xpu_forward():
    model = YOLO("/root/ultralytics/yolov8n.pt")  # å¡«å…¥æœ¬åœ°çš„æ¨¡å‹
    model.to("xpu")
    x = torch.rand(1, 3, 64, 64, device="xpu")
    y = model.model(x)
    assert y is not None
    print("\n[XPU Test] YOLO XPU forward passed successfully âœ”")
```

- æµ‹è¯•ç”¨ä¾‹ç»“æœ

```bash
(B60) root@b60:~/ultralytics# /root/anaconda3/envs/B60/bin/python -m pytest -s -q test.py
[W1128 16:05:14.607604493 OperatorEntry.cpp:218] Warning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
  dispatch key: XPU
  previous kernel: registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37
       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:172 (function operator())

[XPU Test] YOLO XPU forward passed successfully âœ”
.
=============================================================== slowest 30 durations ================================================================
1.27s call     test.py::test_yolo_xpu_forward

(2 durations < 0.005s hidden.  Use -vv to show these durations.)
1 passed in 10.53s
```

# ä»æºç å®‰è£…çš„æµ‹è¯•

```bash
git clone https://github.com/hzdzkjdxyjs/ultralytics.git ultralytics_b60
cd ultralytics_b60
conda activate B60 # è¿™æ˜¯è€ç¯å¢ƒï¼Œä½†æˆ‘è®¤ä¸ºä¸éœ€è¦æ–°ç¯å¢ƒå»éªŒè¯
```

åœ¨å½“å‰ç›®å½•ä¸‹åˆ›å»ºè‡ªåŠ¨æµ‹è¯•æ–‡ä»¶

```python
import pytest
import torch

from ultralytics import YOLO

pytestmark = pytest.mark.skipif(
    not hasattr(torch, "xpu") or not torch.xpu.is_available(),
    reason="XPU not available",
)


def test_yolo_xpu_forward():
    model = YOLO("yolov8n.pt")
    model.to("xpu")
    x = torch.rand(1, 3, 64, 64, device="xpu")
    y = model.model(x)
    assert y is not None
    print("\n[XPU Test] YOLO XPU forward passed successfully âœ”")
```

- æ‰§è¡Œè‡ªåŠ¨åŒ–è„šæœ¬

```bash
(B60) root@b60:~/ultralytics_b60# /root/anaconda3/envs/B60/bin/python -m pytest -s -q test.py

[XPU Test] YOLO XPU forward passed successfully âœ”
.
=============================================================== slowest 30 durations ================================================================
1.18s call     test.py::test_yolo_xpu_forward

(2 durations < 0.005s hidden.  Use -vv to show these durations.)
1 passed in 22.78s
```

- xpuæ”¯æŒæµ‹è¯•
- åœ¨å½“å‰ç›®å½•ä¸‹åˆ›å»ºè®­ç»ƒæ–‡ä»¶ï¼Œä¿®æ”¹deviceå‚æ•° xpu ï¼›xpuï¼š0ï¼›xpuï¼š1ï¼›xpuï¼š0ï¼Œ1

```python
from ultralytics import YOLO

model = YOLO("yolov8n.yaml")
model.train(data="coco128.yaml", epochs=50, imgsz=256, device="xpu:0")
```

- æˆ‘å¿…é¡»å¼ºè°ƒä¸€ä¸‹è¿™ä¸ªçš„ç»“æœï¼Œæ— è®ºä½ è¾“å…¥ä»€ä¹ˆå‚æ•°æœ€ååªä¼šè¿è¡Œ0å¡ï¼Œè¿™æ˜¯å› ä¸ºç›®å‰æˆ‘æ²¡æœ‰çœ‹åˆ°æœ‰æ¯”è¾ƒå¥½çš„æ–¹å¼èƒ½æ”¯æŒå¤šå¡è®­ç»ƒï¼Œæ‰€ä»¥æˆ‘æƒ³ç­‰å¾…ä¹‹åå†å¯¹è¿™ä¸ªä»£ç è¿›è¡Œä¿®æ”¹ï¼Œè¿™ä¸æ˜¯bugï¼ï¼ï¼

- é•¿å‹æµ‹è¯•ï¼Œä¿®æ”¹epochsä¸º50è½®
- éå¸¸é—æ†¾çš„æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»yamlæ–‡ä»¶ä»å¤´å¼€å§‹è®­ç»ƒï¼Œä»–çš„æ•ˆæœå¹¶ä¸å¥½
- ä½†æˆ‘è®¤ä¸ºç¤¾åŒºçš„ç”Ÿæ€ä¸èƒ½ä»…ä»…å±€é™äºNVä¸€å¼ å¡ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å…ˆè¿›è¡Œæ¡†æ¶é€‚é…
- ä¹‹åå†è¿›è¡Œç®—å­é€‚é…ï¼Œè¿™æ—¶å€™å°±éœ€è¦ç£ä¿ƒIntelçš„å›¢é˜Ÿäº†
- å¦‚æœä»…ä»…é¢„è®­ç»ƒçš„æƒé‡æ¥è®­ç»ƒçš„è¯ï¼Œæ•ˆæœä¼šå¥½ä¸€ç‚¹

---

# æœ¬æ¬¡æ›´æ”¹åªæ”¯æŒå•å¡è®­ç»ƒï¼Œä¸ºä»€ä¹ˆä¸æ”¯æŒå¤šå¡ï¼Ÿ

- å¹¶éæ˜¯torchåŠå…¶ä¾èµ–ä¸æ”¯æŒå¤šå¡è®­ç»ƒï¼Œå› ä¸ºæˆ‘èƒ½å¤Ÿåœ¨llamafactoryè¿›è¡Œå¤šå¡è®­ç»ƒ
- è€Œæ˜¯ç¼ºä¹ç›¸åº”çš„å‚æ•°å’Œæ¡†æ¶æ”¯æŒï¼Œ
- ä¾‹å¦‚ultralytics/ultralytics/engine/trainer.pyä¸­çš„BaseTrainer

```python
self.device = select_device(self.args.device)
self.args.device = os.getenv("CUDA_VISIBLE_DEVICES") if "cuda" in str(self.device) else str(self.device)
```

- æˆ‘å‘ç°è¿™ä¸¤è¡Œä»£ç ä¼šå†³å®šæ‚¨çš„è®¾å¤‡æ•°é‡ï¼Œä½†æ˜¯ä¸»è¦æ˜¯os.getenv("CUDA_VISIBLE_DEVICES")å†³å®šçš„
- ultralytics/ultralytics/engine/trainer.pyä¸­çš„BaseTrainerä¼šè¿›ä¸€æ­¥å†³å®šæ˜¯å¦å¼€å¯DDP

```python
        self.callbacks = _callbacks or callbacks.get_default_callbacks()

        if isinstance(self.args.device, str) and len(self.args.device):  # i.e. device='0' or device='0,1,2,3'
            world_size = len(self.args.device.split(","))
        elif isinstance(self.args.device, (tuple, list)):  # i.e. device=[0, 1, 2, 3] (multi-GPU from CLI is list)
            world_size = len(self.args.device)
        elif self.args.device in {"cpu", "mps"}:  # i.e. device='cpu' or 'mps'
            world_size = 0
        elif torch.cuda.is_available():  # i.e. device=None or device='' or device=number
            world_size = 1  # default to device 0
        else:  # i.e. device=None or device=''
            world_size = 0
        self.ddp = world_size > 1 and "LOCAL_RANK" not in os.environ
```

- Intelä¹Ÿæœ‰ç±»ä¼¼çš„ç¯å¢ƒå˜é‡ZE_AFFINITY_MASKï¼Œä½†æ˜¯å¦‚æœæˆ‘é€‰æ‹©è¿™ä¸ªä½œä¸ºè®¾å¤‡æŒ‡å®šæ•°å­—æ—¶
- å½“çœŸæ­£ä¼ å…¥ddpè®­ç»ƒçš„è®¾å¤‡å‚æ•°å°±æ˜¯0ï¼Œ1ï¼Œè¿™äº›æ•°å­—äº†ï¼Œæ­¤æ—¶å†æ¬¡è¿›è¡Œè®¾å¤‡é€‰æ‹©æ—¶åˆ™ä¼šè®¤ä¸ºæ˜¯cudaç±»è®¾å¤‡
- æˆ‘ä¹Ÿç”¨äº†å¾ˆå¤šæ–¹æ³•å»ä¿®æ”¹ï¼Œä½†æ˜¯æˆ‘å‘ç°å› ä¸ºyoloå®åœ¨å¤ªä¾èµ–cudaäº†ï¼Œä»¥è‡³äºé‡Œé¢å’Œå¤šçº¿ç¨‹æœ‰å…³çš„å˜é‡éƒ½ä¸cudaç»‘å®šæ­»äº†ä¾‹å¦‚RANKï¼Œworld_size
- æ‰€ä»¥æˆ‘ä»¬å¾ˆéš¾å»è¿›è¡Œä¿®æ”¹ï¼Œæˆ‘å¸Œæœ›åæœŸèƒ½å¤Ÿæœ‰ä¸€ä¸ªé›†ä¸­çš„å…¥å£ï¼Œæ¯”å¦‚å°†å˜é‡ç»‘å®šåˆ°æŸä¸ªå€¼ä¸Šï¼Œè€Œä¸æ˜¯ä¾èµ–äºCUDA_VISIBLE_DEVICES
- æˆ‘åæ¥æ”¾å¼ƒä¿®æ”¹çš„è¿˜æœ‰ä¸€ä¸ªåŸå› æ˜¯ï¼Œå¦‚æœæˆ‘è¦æ”¹å˜æ•´ä¸ªæ¡†æ¶çš„è¯ï¼Œé‚£å°±å’Œè´¡çŒ®è€…çš„åˆè¡·è¿èƒŒäº†ï¼Œå°½é‡æ˜¯æœ€å°æ”¹åŠ¨
- ä½†æ˜¯æˆ‘æä¾›äº†ä¸€äº›æŒ‰ç†ï¼Œå¸Œæœ›å¯¹æ‚¨æ¡†æ¶è°ƒæ•´æ˜¯æœ‰å¯å‘çš„
  å…³äºxpuå¦‚ä½•å¯åŠ¨å¤šçº¿ç¨‹

```bash
torchrun --nproc_per_node=2 /root/LLaMA-Factory/test_dist.py
```

- test_dist.py

```python
# å¿…é¡»å…ˆå¯¼å…¥ IPEXï¼Œå†å¯¼å…¥ oneCCL
import os

import torch
import torch.distributed as dist


def main():
    # é€šè¿‡ torchrun ä¼ å…¥çš„ LOCAL_RANK è¯†åˆ«å½“å‰è¿›ç¨‹ä½¿ç”¨çš„è®¾å¤‡
    rank = int(os.environ["RANK"])
    local_rank = int(os.environ["LOCAL_RANK"])
    world_size = int(os.environ["WORLD_SIZE"])

    print(f"[Before init] rank={rank}, local_rank={local_rank}")
    # ---- XPU è®¾å¤‡è®¾ç½® ----
    torch.xpu.set_device(local_rank)
    print(f"XPU device set to {local_rank}: {torch.xpu.get_device_name(local_rank)}")
    # ---- CCL init ----
    dist.init_process_group(backend="ccl", rank=rank, world_size=world_size)
    print(f"[After init] Backend={dist.get_backend()}, Rank={dist.get_rank()}, WorldSize={dist.get_world_size()}")
    dist.barrier()
    print(f"[Rank {rank}] barrier passed.")
    dist.destroy_process_group()


if __name__ == "__main__":
    main()
```
