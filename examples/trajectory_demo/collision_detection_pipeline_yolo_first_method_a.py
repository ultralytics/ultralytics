"""
collision_detection_pipeline_yolo_first_method_a.py

YOLO-First ç¢°æ’æ£€æµ‹ç®¡é“ (Method A )
æ‰§è¡Œé¡ºåº: YOLOæ£€æµ‹ â†’ è½¨è¿¹æ„å»º(px) â†’ å…³é”®å¸§æ£€æµ‹ â†’ Homographyå˜æ¢(ä»…å…³é”®å¸§) â†’ TTCåˆ†æ


æµç¨‹:
1. YOLO æ£€æµ‹ (åŸå§‹è§†é¢‘æˆ–è·³å¸§è§†é¢‘)
   - åœ¨åŸå§‹åˆ†è¾¨ç‡ä¸Šæ£€æµ‹æ‰€æœ‰ç‰©ä½“
   - ä¿å­˜æ£€æµ‹æ¡†å’Œ Track ID (åƒç´ ç©ºé—´)

2. è½¨è¿¹æ„å»º (åƒç´ ç©ºé—´)
   - å…³è” Track IDï¼Œå»ºç«‹è½¨è¿¹
   - ä¼°è®¡é€Ÿåº¦ (px/s)
   - æ‰€æœ‰è®¡ç®—åœ¨åƒç´ åæ ‡ç³»

3. å…³é”®å¸§æ£€æµ‹ (æ¥è¿‘äº‹ä»¶è¯†åˆ«)
   - è¯†åˆ«"è·ç¦» < 150px"çš„ç‰©ä½“å¯¹
   - æ ‡è®°ä¸ºå…³é”®å¸§

4. Homography å˜æ¢ (ä»…å…³é”®å¸§) 
   - åªå¯¹å…³é”®å¸§ä¸­çš„ç‰©ä½“ç‚¹åšå˜æ¢
   - è½¬æ¢è·ç¦»å•ä½ (px â†’ m)
   - è½¬æ¢é€Ÿåº¦å•ä½ (px/s â†’ m/s)

5. TTC å’Œ Event åˆ†çº§
   - è®¡ç®— TTC (åœ¨ä¸–ç•Œåæ ‡)
   - åˆ†çº§äº‹ä»¶ (L1/L2/L3)
   - ç”ŸæˆæŠ¥å‘Š

ä¼˜åŠ¿: 
- 
-  æ€§èƒ½æœ€ä¼˜ (ä»…å˜æ¢5-10%çš„æ•°æ®)
-  é€»è¾‘æ¸…æ™° (å…ˆæ‰¾æ¥è¿‘çš„ï¼Œå†ç²¾ç¡®åˆ†æ)
"""

import os
import sys
import json
import cv2
import numpy as np
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# å¯¼å…¥YOLO
sys.path.append(os.path.dirname(__file__))
from ultralytics import YOLO


class YOLOFirstPipelineA:
    def __init__(self, video_path, homography_path=None, output_base=None, skip_frames=1, model='yolo11n', min_track_length=3):
        """åˆå§‹åŒ– YOLO-First pipeline 
        
        Args:
            video_path: åŸå§‹è§†é¢‘è·¯å¾„
            homography_path: Homography JSONè·¯å¾„ (ç”¨äºStep 4)
            output_base: ç»“æœåŸºç¡€ç›®å½•
            skip_frames: æŠ½å¸§å‚æ•°ï¼Œæ¯éš” skip_frames å¸§å¤„ç†ä¸€å¸§ (1=å¤„ç†æ‰€æœ‰å¸§, 3=æ¯éš”3å¸§å¤„ç†ä¸€å¸§)
            model: YOLO æ¨¡å‹é€‰æ‹© (yolo11n/yolo11m/yolo11l)
            min_track_length: æœ€å°è½¨è¿¹é•¿åº¦ï¼ŒçŸ­äºæ­¤çš„è¢«è®¤ä¸ºæ˜¯è¯¯æ£€
        """
        self.video_path = video_path
        self.homography_path = homography_path
        self.skip_frames = skip_frames  # æŠ½å¸§å‚æ•°
        self.model = model  # YOLO æ¨¡å‹
        self.min_track_length = min_track_length  # æœ€å°è½¨è¿¹é•¿åº¦
        # ä½¿ç”¨ /workspace/ultralytics/results ä½œä¸ºè¾“å‡ºç›®å½•ï¼ˆç¡®ä¿è·¯å¾„æ­£ç¡®ï¼‰
        if output_base is None:
            output_base = "/workspace/ultralytics/results"
        self.output_base = Path(output_base)
        self.H = None
        self.pixel_per_meter = 1.0
        
        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_dir = (self.output_base / f"{timestamp}_yolo_first_method_a").resolve()
        
        # åˆ›å»ºå­ç›®å½•ç»“æ„ (Method A)
        self.detection_dir = self.run_dir / "1_yolo_detection"
        self.trajectory_dir = self.run_dir / "2_trajectories"
        self.keyframe_dir = self.run_dir / "3_key_frames"
        self.homography_dir = self.run_dir / "4_homography_transform"
        self.analysis_dir = self.run_dir / "5_collision_analysis"
        
        for d in [self.detection_dir, self.trajectory_dir, self.keyframe_dir, 
                  self.homography_dir, self.analysis_dir]:
            d.mkdir(parents=True, exist_ok=True)
        
        print(f"\n{'='*70}")
        print(f"YOLO-First ç¢°æ’æ£€æµ‹Pipeline (Method A - å¯¼å¸ˆæ¨è)")
        print(f"{'='*70}")
        print(f"æ—¶é—´æˆ³: {timestamp}")
        print(f"ç»“æœç›®å½•: {self.run_dir}")
        print(f"æ‰§è¡Œé¡ºåº: YOLO â†’ è½¨è¿¹(px) â†’ å…³é”®å¸§ â†’ Homography(å…³é”®å¸§) â†’ TTC")
    
    def load_homography(self):
        """åŠ è½½ Homography çŸ©é˜µ (Step 4 éœ€è¦)"""
        if not self.homography_path:
            print(f"\nâš ï¸  æœªæä¾› Homographyï¼Œå°†ä»…åœ¨åƒç´ ç©ºé—´å¤„ç†")
            return False
        
        try:
            with open(self.homography_path) as f:
                H_data = json.load(f)
            
            self.H = np.array(H_data['homography_matrix'], dtype=np.float32)
            pixel_points = H_data['pixel_points']
            world_points = H_data['world_points']
            
            # ä¿å­˜åˆ°è¾“å‡ºç›®å½•
            with open(self.homography_dir / 'homography.json', 'w') as f:
                json.dump(H_data, f, indent=2)
            
            # è®¡ç®—åƒç´ åˆ°ç±³çš„ç¼©æ”¾å› å­
            if len(world_points) >= 2 and len(pixel_points) >= 2:
                px_dist = np.sqrt((pixel_points[0][0] - pixel_points[1][0])**2 + 
                                 (pixel_points[0][1] - pixel_points[1][1])**2)
                world_dist = np.sqrt((world_points[0][0] - world_points[1][0])**2 + 
                                    (world_points[0][1] - world_points[1][1])**2)
                
                self.pixel_per_meter = px_dist / world_dist if world_dist > 0 else 1.0
            
            print(f"  âœ“ Homographyå·²åŠ è½½")
            print(f"    ç¼©æ”¾å› å­: {self.pixel_per_meter:.2f} px/m")
            
            return True
        
        except Exception as e:
            print(f"  âŒ åŠ è½½Homographyå¤±è´¥: {e}")
            return False
    
    # =========================================================================
    # å¯è§†åŒ–å’Œå›¾åƒä¿å­˜
    # =========================================================================
    
    def save_detection_frame(self, video_path, frame_num, output_path, detections=None):
        """ä¿å­˜æŒ‡å®šå¸§çš„å›¾åƒï¼ˆå¸¦ YOLO æ£€æµ‹æ¡†ï¼‰"""
        cap = cv2.VideoCapture(video_path)
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num - 1)
        ret, frame = cap.read()
        cap.release()
        
        if not ret:
            return False
        
        # å¦‚æœæœ‰æ£€æµ‹æ•°æ®ï¼Œç»˜åˆ¶æ£€æµ‹æ¡†
        if detections:
            for obj in detections.get('objects', []):
                x, y, w, h = obj['bbox_xywh']
                x1, y1 = int(x - w/2), int(y - h/2)
                x2, y2 = int(x + w/2), int(y + h/2)
                
                # ç»˜åˆ¶æ£€æµ‹æ¡†
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
                # ç»˜åˆ¶ Track ID
                track_id = obj['track_id']
                conf = obj['conf']
                text = f"ID:{track_id} ({conf:.2f})"
                cv2.putText(frame, text, (x1, y1-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        # ä¿å­˜å›¾åƒ
        cv2.imwrite(str(output_path), frame)
        return True
    
    def save_keyframe_with_distance(self, video_path, frame_num, output_path, proximity_event):
        """ä¿å­˜å…³é”®å¸§å›¾åƒï¼ˆç»˜åˆ¶ä¸¤ä¸ªæ¥è¿‘çš„ç‰©ä½“å’Œè·ç¦»ï¼‰"""
        cap = cv2.VideoCapture(video_path)
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num - 1)
        ret, frame = cap.read()
        cap.release()
        
        if not ret:
            return False
        
        # è·å–ä¸¤ä¸ªç‰©ä½“çš„ä¿¡æ¯
        center_1 = proximity_event.get('center_1_px', proximity_event.get('center_1', [0, 0]))
        center_2 = proximity_event.get('center_2_px', proximity_event.get('center_2', [0, 0]))
        track_id_1 = proximity_event.get('track_id_1', -1)
        track_id_2 = proximity_event.get('track_id_2', -1)
        distance_pixel = proximity_event.get('distance_pixel', 0)
        distance_meters = proximity_event.get('distance_meters', 0)
        class_1 = proximity_event.get('class_1', 'Unknown')
        class_2 = proximity_event.get('class_2', 'Unknown')
        
        # ç»˜åˆ¶ä¸¤ä¸ªç‰©ä½“çš„ä¸­å¿ƒç‚¹
        pt1 = tuple(map(int, center_1))
        pt2 = tuple(map(int, center_2))
        
        # ç»˜åˆ¶åœ†ç‚¹å’ŒID
        cv2.circle(frame, pt1, 5, (0, 255, 0), -1)  # ç»¿è‰²åœ†ç‚¹
        cv2.putText(frame, f"ID:{track_id_1}", (pt1[0]+10, pt1[1]-10),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        cv2.circle(frame, pt2, 5, (0, 0, 255), -1)  # çº¢è‰²åœ†ç‚¹
        cv2.putText(frame, f"ID:{track_id_2}", (pt2[0]+10, pt2[1]-10),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        
        # ç»˜åˆ¶è¿æ¥çº¿
        cv2.line(frame, pt1, pt2, (255, 0, 0), 2)  # è“è‰²çº¿
        
        # æ˜¾ç¤ºè·ç¦»ä¿¡æ¯ (åƒç´ å’Œä¸–ç•Œåæ ‡)
        mid_x = (pt1[0] + pt2[0]) // 2
        mid_y = (pt1[1] + pt2[1]) // 2
        distance_text = f"Distance: {distance_meters:.2f}m ({distance_pixel:.0f}px)"
        cv2.putText(frame, distance_text, (mid_x-100, mid_y-10),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
        
        # æ˜¾ç¤ºç‰©ä½“ç±»åˆ«ä¿¡æ¯
        class_text = f"{class_1} vs {class_2}"
        cv2.putText(frame, class_text, (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
        
        # ä¿å­˜å›¾åƒ
        cv2.imwrite(str(output_path), frame)
        return True
    
    # =========================================================================
    # STEP 1: YOLO æ£€æµ‹ (åƒç´ ç©ºé—´)
    # =========================================================================
    
    def run_yolo_detection(self, conf_threshold=0.45):
        """Step 1: YOLO æ£€æµ‹ (åŸå§‹è§†é¢‘æˆ–è·³å¸§è§†é¢‘)
        
        è¾“å‡º:
        - ä¿å­˜æ‰€æœ‰æ£€æµ‹æ¡†å’Œ Track ID (åƒç´ ç©ºé—´)
        - ç”Ÿæˆæ£€æµ‹ç»Ÿè®¡
        """
        print(f"\nã€Step 1: YOLO æ£€æµ‹ã€‘")
        
        # åŠ è½½æŒ‡å®šçš„ YOLO æ¨¡å‹
        model = YOLO(f'{self.model}.pt')
        print(f"  åŠ è½½æ¨¡å‹: {self.model}.pt")
        
        cap = cv2.VideoCapture(self.video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        all_detections = []
        frame_count = 0
        detection_frames_count = 0
        
        # æŠ½å¸§å¤„ç†
        if self.skip_frames > 1:
            print(f"  å¤„ç†ä¸­: {total_frames}å¸§ @ {fps:.2f}FPS (æ¯éš”{self.skip_frames}å¸§å¤„ç†ä¸€å¸§)...")
        else:
            print(f"  å¤„ç†ä¸­: {total_frames}å¸§ @ {fps:.2f}FPS...")
        
        for result in model.track(source=self.video_path, stream=True, 
                                 persist=True, conf=conf_threshold):
            frame_count += 1
            
            # æŠ½å¸§ï¼šå¦‚æœå¯ç”¨æŠ½å¸§ï¼Œè·³è¿‡ä¸éœ€è¦å¤„ç†çš„å¸§
            if (frame_count - 1) % self.skip_frames != 0:
                continue
            
            if result.boxes is None or len(result.boxes) == 0:
                if frame_count % 30 == 0:
                    print(f"    Frame {frame_count}/{total_frames} - æ— ç‰©ä½“")
                continue
            
            detection_frames_count += 1
            boxes = result.boxes.xywh.cpu().numpy()
            ids = result.boxes.id
            classes = result.boxes.cls.cpu().numpy().astype(int)
            confs = result.boxes.conf.cpu().numpy()
            
            frame_detections = {
                'frame': frame_count,
                'time': frame_count / fps,
                'objects': []
            }
            
            # åªæœ‰åœ¨æœ‰æ£€æµ‹åˆ°å¯¹è±¡æ—¶æ‰å¤„ç†
            if len(boxes) > 0 and ids is not None:
                for i in range(len(boxes)):
                    obj_data = {
                        'track_id': int(ids[i]) if ids[i] is not None else -1,
                        'class': int(classes[i]),
                        'conf': float(confs[i]),
                        'bbox_xywh': boxes[i].tolist(),  # [x_center, y_center, w, h] åƒç´ 
                    }
                    frame_detections['objects'].append(obj_data)
                
                # ä¿å­˜æ£€æµ‹æ¡†å›¾åƒï¼ˆæ¯ä¸ªæœ‰æ£€æµ‹çš„å¸§ï¼‰
                frame_img_path = self.detection_dir / f"frame_{frame_count:04d}.jpg"
                self.save_detection_frame(self.video_path, frame_count, frame_img_path, frame_detections)
            
            all_detections.append(frame_detections)
            
            if frame_count % 30 == 0:
                print(f"    Frame {frame_count}/{total_frames} - {len(boxes)}ä¸ªç‰©ä½“")
        
        cap.release()
        
        # ä¿å­˜åŸå§‹æ£€æµ‹ç»“æœ (åƒç´ ç©ºé—´)
        detections_path = self.detection_dir / 'detections_pixel.json'
        with open(detections_path, 'w') as f:
            json.dump(all_detections, f, indent=2)
        
        # ç”Ÿæˆæ£€æµ‹ç»Ÿè®¡
        stats = {
            'total_frames': total_frames,
            'fps': fps,
            'detection_frames': detection_frames_count,
            'confidence_threshold': conf_threshold,
        }
        stats_path = self.detection_dir / 'detection_stats.json'
        with open(stats_path, 'w') as f:
            json.dump(stats, f, indent=2)
        
        print(f"  âœ“ Step 1å®Œæˆ: {detection_frames_count}å¸§æ£€æµ‹åˆ°ç‰©ä½“")
        print(f"    è¾“å‡º: {detections_path.name}")
        
        return all_detections
    
    # =========================================================================
    # STEP 1.5: åŒå¸§å†…ç‰©ä½“åˆ†å‰²åˆå¹¶ (å¤„ç†YOLOæŠŠä¸€ä¸ªç‰©ä½“åˆ†æˆä¸¤ä¸ªçš„é—®é¢˜)
    # =========================================================================
    
    def merge_fragmented_objects_in_frame(self, all_detections, same_class_distance_threshold=200):
        """åœ¨åŒä¸€å¸§å†…ï¼Œåˆå¹¶è¢«åˆ†å‰²çš„åŒç±»ç‰©ä½“
        
        åŸç†ï¼š
        - YOLOæœ‰æ—¶ä¼šæŠŠä¸€ä¸ªç‰©ä½“æ£€æµ‹æˆå¤šä¸ªï¼ˆæ¯”å¦‚æ‘©æ‰˜è½¦çš„å‰åéƒ¨åˆ†ï¼‰
        - åœ¨åŒä¸€å¸§å†…ï¼Œå¦‚æœä¸¤ä¸ªç‰©ä½“ï¼š
          1. ç±»åˆ«ç›¸åŒï¼ˆéƒ½æ˜¯motorcycleï¼‰
          2. ä¸­å¿ƒè·ç¦» < same_class_distance_threshold (åƒç´ )
          3. åˆ™è®¤ä¸ºæ˜¯åŒä¸€ç‰©ä½“è¢«åˆ†å‰²ï¼Œåˆå¹¶å®ƒä»¬
        - ä¿ç•™ç½®ä¿¡åº¦æ›´é«˜çš„é‚£ä¸ªï¼Œåˆ é™¤ç½®ä¿¡åº¦ä½çš„
        """
        print(f"\nã€Step 1.5: åŒå¸§å†…ç‰©ä½“åˆ†å‰²åˆå¹¶ã€‘")
        print(f"  â„¹ï¸  åœ¨æ¯ä¸€å¸§å†…æ£€æµ‹å’Œåˆå¹¶è¢«åˆ†å‰²çš„åŒç±»ç‰©ä½“")
        
        merged_count = 0
        
        for frame_data in all_detections:
            frame = frame_data['frame']
            objects = frame_data['objects']
            
            # è®°å½•å“ªäº›ç‰©ä½“åº”è¯¥è¢«åˆ é™¤ï¼ˆå› ä¸ºè¢«åˆå¹¶äº†ï¼‰
            to_remove = set()
            
            # æ£€æŸ¥æ‰€æœ‰ç‰©ä½“å¯¹
            for i, obj1 in enumerate(objects):
                if i in to_remove:
                    continue
                
                for j, obj2 in enumerate(objects):
                    if j <= i or j in to_remove:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åŒç±»åˆ«ä¸”è·ç¦»è¿‘
                    if obj1['class'] == obj2['class']:
                        x1, y1 = obj1['bbox_xywh'][0], obj1['bbox_xywh'][1]
                        x2, y2 = obj2['bbox_xywh'][0], obj2['bbox_xywh'][1]
                        
                        distance = np.sqrt((x2-x1)**2 + (y2-y1)**2)
                        
                        if distance < same_class_distance_threshold:
                            # åˆå¹¶ï¼šä¿ç•™ç½®ä¿¡åº¦é«˜çš„ï¼Œåˆ é™¤ç½®ä¿¡åº¦ä½çš„
                            if obj1['conf'] >= obj2['conf']:
                                to_remove.add(j)
                                merged_count += 1
                                class_name = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 
                                             4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck'}.get(obj1['class'], f"class_{obj1['class']}")
                                print(f"  ğŸ”€ åˆå¹¶: Frame {frame:03d} - ä¸¤ä¸ª {class_name} (ID {obj1['track_id']}, ID {obj2['track_id']}) è·ç¦» {distance:.1f}px < {same_class_distance_threshold}px")
                            else:
                                to_remove.add(i)
                                merged_count += 1
                                class_name = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 
                                             4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck'}.get(obj2['class'], f"class_{obj2['class']}")
                                print(f"  ğŸ”€ åˆå¹¶: Frame {frame:03d} - ä¸¤ä¸ª {class_name} (ID {obj2['track_id']}, ID {obj1['track_id']}) è·ç¦» {distance:.1f}px < {same_class_distance_threshold}px")
            
            # åˆ é™¤è¢«åˆå¹¶çš„ç‰©ä½“
            frame_data['objects'] = [obj for i, obj in enumerate(objects) if i not in to_remove]
        
        print(f"  âœ“ Step 1.5 å®Œæˆ: åˆå¹¶äº† {merged_count} ä¸ªåŒå¸§å†…çš„åˆ†å‰²ç‰©ä½“")
        
        return all_detections
    
    # =========================================================================
    # =========================================================================
    
    def build_trajectories(self, all_detections):
        """Step 2: æ„å»ºè½¨è¿¹ (åƒç´ ç©ºé—´ï¼Œpx/s)
        
        è¾“å…¥: åŸå§‹æ£€æµ‹ç»“æœ
        è¾“å‡º: å®Œæ•´è½¨è¿¹ (æŒ‰ track_id ç»„ç»‡)
        """
        print(f"\nã€Step 2: è½¨è¿¹æ„å»º (åƒç´ ç©ºé—´)ã€‘")
        
        # æŒ‰ track_id ç»„ç»‡è½¨è¿¹
        tracks = {}
        
        for frame_data in all_detections:
            for obj in frame_data['objects']:
                track_id = obj['track_id']
                
                if track_id not in tracks:
                    tracks[track_id] = []
                
                # è½¨è¿¹ç‚¹ (åƒç´ ç©ºé—´)
                track_point = {
                    'frame': frame_data['frame'],
                    'time': frame_data['time'],
                    'class': obj['class'],
                    'conf': obj['conf'],
                    'center_x': obj['bbox_xywh'][0],  # åƒç´ 
                    'center_y': obj['bbox_xywh'][1],  # åƒç´ 
                }
                
                tracks[track_id].append(track_point)
        
        # è®¡ç®—æ¯ä¸ªè½¨è¿¹çš„é€Ÿåº¦ä¿¡æ¯ (px/s)
        for track_id, track_points in tracks.items():
            track_points.sort(key=lambda p: p['frame'])
            
            if len(track_points) >= 2:
                for i in range(1, len(track_points)):
                    prev = track_points[i-1]
                    curr = track_points[i]
                    
                    dt = curr['time'] - prev['time']
                    if dt > 0:
                        dx = curr['center_x'] - prev['center_x']
                        dy = curr['center_y'] - prev['center_y']
                        
                        curr['vx'] = dx / dt  # px/s
                        curr['vy'] = dy / dt  # px/s
                        curr['speed'] = np.sqrt(dx**2 + dy**2) / dt  # px/s
                    else:
                        curr['vx'] = 0.0
                        curr['vy'] = 0.0
                        curr['speed'] = 0.0
                
                track_points[0]['vx'] = 0.0
                track_points[0]['vy'] = 0.0
                track_points[0]['speed'] = 0.0
        
        # ä¿å­˜è½¨è¿¹
        tracks_path = self.trajectory_dir / 'tracks.json'
        with open(tracks_path, 'w') as f:
            json.dump(tracks, f, indent=2)
        
        # ç”Ÿæˆè½¨è¿¹ç»Ÿè®¡
        stats = {
            'total_tracks': len(tracks),
            'track_lengths': {str(tid): len(points) for tid, points in tracks.items()},
            'coordinate_system': 'pixel',
            'velocity_unit': 'px/s',
        }
        stats_path = self.trajectory_dir / 'track_stats.json'
        with open(stats_path, 'w') as f:
            json.dump(stats, f, indent=2)
        
        print(f"  âœ“ Step 2å®Œæˆ: {len(tracks)}æ¡è½¨è¿¹ (åƒç´ ç©ºé—´)")
        print(f"    é€Ÿåº¦å•ä½: px/s")
        print(f"    è¾“å‡º: {tracks_path.name}")
        
        return tracks
    
    # =========================================================================
    # STEP 2.4: è½¨è¿¹é—´æ–­æ£€æµ‹ (è¯†åˆ«å‡ºç°â†’æ¶ˆå¤±â†’é‡æ–°å‡ºç°çš„å¯ç–‘è½¨è¿¹)
    # =========================================================================
    
    def detect_discontinuous_tracks(self, all_detections, max_gap_frames=None):
        """æ£€æµ‹è½¨è¿¹é—´æ–­ (å‡ºç°â†’æ¶ˆå¤±â†’é‡æ–°å‡ºç°)
        
        åŸç†ï¼š
        - åŒä¸€ä¸ª Track ID åœ¨æ—¶é—´åºåˆ—ä¸­å‡ºç°äº†é—´æ–­
        - æ¯”å¦‚ Track ID 13 åœ¨ Frame 82-91 å‡ºç°ï¼ŒFrame 92-93 æ¶ˆå¤±ï¼ŒFrame 94 é‡æ–°å‡ºç°
        - è¿™å¾ˆä¸åˆç†ï¼Œé™¤éç‰©ä½“çœŸçš„ç¦»å¼€äº†è§†é‡ï¼ˆæå°‘è§ï¼‰
        - æ›´å¯èƒ½æ˜¯è¿½è¸ªå¤±è´¥æˆ–è¯¯æ£€å¯¼è‡´çš„å¹½çµè½¨è¿¹
        
        æ³¨æ„ï¼šè€ƒè™‘ frame skipping çš„å½±å“
        - å¦‚æœç”¨ --skip-frames 3ï¼Œæ£€æµ‹çš„æ˜¯æ¯éš”3å¸§çš„æƒ…å†µ
        - æ‰€ä»¥"é—´æ–­"çš„å®šä¹‰åº”è¯¥ç›¸å¯¹å®½æ¾
        
        è¿”å›ï¼š
        - suspicious_track_ids: åŒ…å«çœŸæ­£çš„é—´æ–­çš„ Track ID é›†åˆ
        """
        print(f"\nã€Step 2.4: è½¨è¿¹é—´æ–­æ£€æµ‹ã€‘")
        print(f"  â„¹ï¸  æ£€æµ‹è½¨è¿¹é—´æ–­ï¼ˆå‡ºç°â†’æ¶ˆå¤±â†’é‡æ–°å‡ºç°ï¼Œè€ƒè™‘frame skippingï¼‰")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œæ ¹æ® skip_frames è®¾ç½®é»˜è®¤å€¼
        if max_gap_frames is None:
            max_gap_frames = self.skip_frames * 3  # å…è®¸æœ€å¤š skip_frames*3 çš„é—´æ–­
        
        # ä¸ºæ¯ä¸ª Track ID è®°å½•å®ƒå‡ºç°çš„æ‰€æœ‰å¸§
        track_frames = {}  # {track_id: [frame_nums]}
        
        for frame_data in all_detections:
            frame = frame_data['frame']
            for obj in frame_data['objects']:
                track_id = obj['track_id']
                if track_id not in track_frames:
                    track_frames[track_id] = []
                track_frames[track_id].append(frame)
        
        # æ£€æŸ¥æ¯ä¸ªè½¨è¿¹çš„è¿ç»­æ€§
        suspicious_tracks = {}  # {track_id: 'gap_info'}
        
        for track_id, frames in track_frames.items():
            frames = sorted(set(frames))  # å»é‡å¹¶æ’åº
            
            if len(frames) < 2:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰çœŸæ­£çš„é—´æ–­ï¼ˆä¸æ˜¯å› ä¸º frame skipping å¯¼è‡´çš„ï¼‰
            gaps = []
            for i in range(1, len(frames)):
                gap = frames[i] - frames[i-1]
                # åªæœ‰é—´æ–­ > skip_frames æ—¶æ‰ç®—çœŸæ­£çš„é—´æ–­
                if gap > self.skip_frames + 1:  # å…è®¸1å¸§çš„åå·®
                    gaps.append((frames[i-1], frames[i], gap))
            
            if gaps:
                # æœ‰çœŸæ­£é—´æ–­çš„è½¨è¿¹
                suspicious_tracks[track_id] = {
                    'total_frames': len(frames),
                    'first_frame': frames[0],
                    'last_frame': frames[-1],
                    'gaps': gaps
                }
        
        # æ‰“å°å¯ç–‘è½¨è¿¹ä¿¡æ¯
        if suspicious_tracks:
            print(f"  âš ï¸  æ£€æµ‹åˆ° {len(suspicious_tracks)} æ¡æœ‰çœŸæ­£é—´æ–­çš„è½¨è¿¹ï¼ˆå¯èƒ½æ˜¯è¯¯æ£€æˆ–è¿½è¸ªå¤±è´¥ï¼‰:")
            for track_id, info in sorted(suspicious_tracks.items()):
                print(f"     - Track ID {track_id}: {info['total_frames']}å¸§ ({info['first_frame']}-{info['last_frame']})")
                for gap_start, gap_end, gap_size in info['gaps']:
                    print(f"       â””â”€ é—´æ–­: Frame {gap_start} â†’ Frame {gap_end} (é—´éš” {gap_size} å¸§ï¼Œè¶…è¿‡å…è®¸çš„ {self.skip_frames + 1} å¸§)")
        else:
            print(f"  âœ“ æ²¡æœ‰æ£€æµ‹åˆ°çœŸæ­£çš„é—´æ–­è½¨è¿¹ï¼ˆè€ƒè™‘äº† skip_frames={self.skip_frames} çš„å½±å“ï¼‰")
        
        print(f"  â„¹ï¸  æ³¨æ„: é—´æ–­é˜ˆå€¼ = {self.skip_frames + 1} å¸§ï¼ˆåŸºäº skip_frames å‚æ•°ï¼‰")
        
        return suspicious_tracks
    
    # =========================================================================
    # STEP 2.5: è½¨è¿¹è¿ç»­æ€§è¿‡æ»¤ (æ’é™¤çŸ­è½¨è¿¹è¯¯æ£€)
    # =========================================================================
    
    def filter_short_tracks(self, all_detections, min_track_length=3):
        """è¿‡æ»¤çŸ­è½¨è¿¹ (å¯èƒ½æ˜¯ YOLO è¯¯æ£€)
        
        å¦‚æœä¸€ä¸ª Track ID åªå‡ºç°åœ¨å°‘äº min_track_length å¸§ä¸­ï¼Œ
        åˆ™è®¤ä¸ºæ˜¯è¯¯æ£€ï¼Œå°†å…¶ä»æ£€æµ‹ç»“æœä¸­ç§»é™¤ã€‚
        
        åŸç†ï¼š
        - çœŸå®ç‰©ä½“åº”è¯¥åœ¨è¿ç»­çš„å¤šä¸ªå¸§ä¸­è¢«æ£€æµ‹åˆ°
        - å¦‚æœç‰©ä½“çªç„¶å‡ºç°åˆæ¶ˆå¤±ï¼Œé€šå¸¸æ˜¯ YOLO çš„è¯¯æ£€
        """
        print(f"\nã€Step 2.5: è½¨è¿¹è¿ç»­æ€§è¿‡æ»¤ã€‘")
        
        # ç»Ÿè®¡æ¯ä¸ª Track ID çš„å‡ºç°å¸§æ•°
        track_lengths = {}
        for frame_data in all_detections:
            for obj in frame_data['objects']:
                track_id = obj['track_id']
                track_lengths[track_id] = track_lengths.get(track_id, 0) + 1
        
        # æ‰¾å‡ºçŸ­è½¨è¿¹ (å¯èƒ½æ˜¯è¯¯æ£€)
        short_tracks = {tid: length for tid, length in track_lengths.items() if length < min_track_length}
        valid_tracks = {tid: length for tid, length in track_lengths.items() if length >= min_track_length}
        
        print(f"  è½¨è¿¹é•¿åº¦ç»Ÿè®¡:")
        print(f"    - æ€»è½¨è¿¹: {len(track_lengths)}")
        print(f"    - æœ‰æ•ˆè½¨è¿¹ (>= {min_track_length}å¸§): {len(valid_tracks)}")
        print(f"    - çŸ­è½¨è¿¹/è¯¯æ£€ (< {min_track_length}å¸§): {len(short_tracks)}")
        
        if short_tracks:
            print(f"  ğŸ—‘ï¸  ç§»é™¤çš„çŸ­è½¨è¿¹:")
            for tid, length in sorted(short_tracks.items(), key=lambda x: x[1]):
                print(f"     - Track ID {tid}: {length} å¸§")
        
        # è¿‡æ»¤æ£€æµ‹ç»“æœï¼Œç§»é™¤çŸ­è½¨è¿¹ä¸­çš„ç‰©ä½“
        filtered_detections = []
        for frame_data in all_detections:
            new_frame_data = frame_data.copy()
            new_frame_data['objects'] = [
                obj for obj in frame_data['objects']
                if obj['track_id'] in valid_tracks
            ]
            filtered_detections.append(new_frame_data)
        
        print(f"  âœ“ Step 2.5 å®Œæˆ: ç§»é™¤äº† {len(short_tracks)} æ¡çŸ­è½¨è¿¹")
        
        return filtered_detections
    
    # =========================================================================
    # STEP 2.6: Track ID é‡è¿æ£€æµ‹ (åŒä¸€ç‰©ä½“å¤šä¸ªIDåˆå¹¶)
    # =========================================================================
    
    def merge_fragmented_tracks(self, all_detections, max_gap_frames=2, max_distance_pixels=100):
        """åˆå¹¶è¢«æ–­å¼€çš„Track ID (åŒä¸€ç‰©ä½“è¿½è¸ªå¤±è´¥å¯¼è‡´çš„é‡å¤ID)
        
        åŸç†ï¼š
        - å¦‚æœ Track ID A åœ¨æŸå¸§æ¶ˆå¤±
        - ç„¶ååœ¨ max_gap_frames å¸§å†…ï¼Œä¸€ä¸ªæ–°çš„ Track ID B å‡ºç°
        - ä¸”ä¸¤ä¸ªIDçš„ç‰©ä½“ä¸­å¿ƒè·ç¦» < max_distance_pixels
        - åˆ™è®¤ä¸ºæ˜¯åŒä¸€ç‰©ä½“ï¼Œåº”è¯¥åˆå¹¶ID
        
        è¿™ç‰¹åˆ«é€‚åˆè§£å†³æ‘©æ‰˜è½¦è¢«åˆ†æˆID13å’ŒID15çš„é—®é¢˜ã€‚
        """
        print(f"\nã€Step 2.6: Track ID é‡è¿æ£€æµ‹ã€‘")
        print(f"  â„¹ï¸  æ£€æµ‹å¹¶åˆå¹¶è¢«æ–­å¼€çš„è½¨è¿¹ (æ¶ˆå¤±<{max_gap_frames}å¸§åé‡æ–°å‡ºç°)")
        
        # ç¬¬ä¸€æ­¥ï¼šä¸ºæ¯ä¸ªTrack IDç»Ÿè®¡æœ€åå‡ºç°çš„å¸§å’Œä½ç½®
        track_last_appearance = {}  # {track_id: {'frame': f, 'x': x, 'y': y, 'class': c}}
        
        for frame_data in all_detections:
            frame = frame_data['frame']
            for obj in frame_data['objects']:
                track_id = obj['track_id']
                track_last_appearance[track_id] = {
                    'frame': frame,
                    'x': obj['bbox_xywh'][0],
                    'y': obj['bbox_xywh'][1],
                    'class': obj['class']
                }
        
        # ç¬¬äºŒæ­¥ï¼šå¯»æ‰¾å¯èƒ½æ–­å¼€çš„Track IDå¯¹
        merge_map = {}  # {old_id: new_id} æ˜ å°„
        merged_count = 0
        
        track_ids = sorted(track_last_appearance.keys())
        
        for i, track_a in enumerate(track_ids):
            if track_a in merge_map:
                continue  # å·²ç»è¢«åˆå¹¶è¿‡äº†
            
            last_a = track_last_appearance[track_a]
            frame_a = last_a['frame']
            x_a, y_a = last_a['x'], last_a['y']
            class_a = last_a['class']
            
            # æŸ¥æ‰¾åç»­å‡ºç°çš„Track ID
            for track_b in track_ids[i+1:]:
                if track_b in merge_map:
                    continue
                
                # æ‰¾track_bçš„ç¬¬ä¸€æ¬¡å‡ºç°
                first_b_frame = None
                first_b_pos = None
                first_b_class = None
                
                for frame_data in all_detections:
                    for obj in frame_data['objects']:
                        if obj['track_id'] == track_b:
                            if first_b_frame is None:
                                first_b_frame = frame_data['frame']
                                first_b_pos = (obj['bbox_xywh'][0], obj['bbox_xywh'][1])
                                first_b_class = obj['class']
                            break
                
                if first_b_frame is None:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³é‡è¿æ¡ä»¶
                frame_gap = first_b_frame - frame_a
                if 1 <= frame_gap <= max_gap_frames:  # ä¸­é—´æœ‰é—´éš”ä½†ä¸è¶…è¿‡max_gap
                    distance = np.sqrt((first_b_pos[0] - x_a)**2 + (first_b_pos[1] - y_a)**2)
                    
                    if distance < max_distance_pixels and class_a == first_b_class:
                        # è®¤ä¸ºæ˜¯åŒä¸€ç‰©ä½“ï¼Œåº”è¯¥åˆå¹¶
                        merge_map[track_b] = track_a
                        merged_count += 1
                        print(f"  ğŸ”— åˆå¹¶: Track ID {track_b} (é¦–æ¬¡Frame {first_b_frame}) â†’ ID {track_a} (æœ«æ¬¡Frame {frame_a})")
                        print(f"     é—´éš”: {frame_gap}å¸§, è·ç¦»: {distance:.1f}px, ç±»åˆ«: {class_a}")
        
        # ç¬¬ä¸‰æ­¥ï¼šåº”ç”¨åˆå¹¶åˆ°æ‰€æœ‰æ£€æµ‹ç»“æœ
        if merge_map:
            for frame_data in all_detections:
                for obj in frame_data['objects']:
                    if obj['track_id'] in merge_map:
                        old_id = obj['track_id']
                        new_id = merge_map[old_id]
                        obj['track_id'] = new_id
        
        print(f"  âœ“ Step 2.6 å®Œæˆ: åˆå¹¶äº† {merged_count} ä¸ªæ–­å¼€çš„Track ID")
        
        return all_detections
    
    # =========================================================================
    # STEP 3: å…³é”®å¸§æ£€æµ‹ (æ¥è¿‘äº‹ä»¶)
    # =========================================================================

    def extract_key_frames(self, all_detections, world_distance_threshold=2.0, debug_threshold=5.0):
        """Step 3: å…³é”®å¸§æ£€æµ‹ (æ¥è¿‘äº‹ä»¶)
        
        ğŸ”„ FIXED: ç°åœ¨åœ¨ã€ä¸–ç•Œåæ ‡ã€‘ä¸­è®¡ç®—è·ç¦»ï¼Œè€Œä¸æ˜¯åƒç´ åæ ‡
        - ä½¿ç”¨ Homography æ—©æœŸå˜æ¢åæ ‡
        - è®¾ç½®å®é™…è·ç¦»é˜ˆå€¼ (é»˜è®¤ 2ç±³) è€Œä¸æ˜¯å›ºå®šåƒç´ é˜ˆå€¼
        - é¿å…åŒä¸€ç‰©ä½“çš„ä¸åŒéƒ¨åˆ†è¢«è¯†åˆ«ä¸ºä¸¤ä¸ªæ¥è¿‘äº‹ä»¶
        
        å‚æ•°:
        - world_distance_threshold: å…³é”®å¸§æ£€æµ‹çš„ä¸»é˜ˆå€¼ï¼ˆé»˜è®¤ 2.0 ç±³ï¼‰
        - debug_threshold: è°ƒè¯•æ˜¾ç¤ºçš„é˜ˆå€¼ï¼ˆé»˜è®¤ 5.0 ç±³ï¼‰ï¼Œç”¨äºäº†è§£è¢«æ’é™¤çš„äº‹ä»¶
        
        âš ï¸ æ”¹è¿›: è¿‡æ»¤æ‰åŒä¸€ç‰©ä½“è¢«å¤šæ¬¡æ£€æµ‹çš„æƒ…å†µ
        - è·³è¿‡åŒç±»åˆ«ç‰©ä½“çš„æ¥è¿‘æ£€æµ‹ (ä¸¤ä¸ªcar, ä¸¤ä¸ªmotorcycle ç­‰)
        - åªä¿ç•™ä¸åŒç±»åˆ«ç‰©ä½“çš„äº¤äº’ (carä¸motorcycle, carä¸personç­‰)
        """
        print(f"\nã€Step 3: å…³é”®å¸§æ£€æµ‹ (ä¸–ç•Œåæ ‡ ğŸ”„)ã€‘")
        print(f"  â„¹ï¸  ç°åœ¨åœ¨ä¸–ç•Œåæ ‡ä¸­è®¡ç®—è·ç¦»ï¼ˆä½¿ç”¨Homographyï¼‰")
        
        proximity_events = []
        all_proximity_pairs = []  # ç”¨äºè°ƒè¯•ï¼Œè®°å½•æ‰€æœ‰ < debug_threshold çš„æ£€æµ‹
        
        # ç‰©ä½“ç±»åˆ«æ˜ å°„
        class_names = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 
                      4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck'}
        
        # éå†æ¯ä¸€å¸§ï¼Œæ£€æµ‹ç‰©ä½“å¯¹ä¹‹é—´çš„è·ç¦»
        for frame_data in all_detections:
            if len(frame_data['objects']) < 2:
                continue
            
            frame = frame_data['frame']
            objects = frame_data['objects']
            
            # æ£€æŸ¥æ‰€æœ‰ç‰©ä½“å¯¹
            for i in range(len(objects)):
                for j in range(i+1, len(objects)):
                    obj1 = objects[i]
                    obj2 = objects[j]
                    
                    # âš ï¸ è¿‡æ»¤: è·³è¿‡åŒç±»åˆ«ç‰©ä½“ï¼ˆé¿å…æ‘©æ‰˜è½¦çš„ä¸åŒéƒ¨åˆ†è¢«è¯†åˆ«ä¸ºä¸¤ä¸ªç‰©ä½“ï¼‰
                    if obj1['class'] == obj2['class']:
                        # åªæœ‰å½“è·ç¦»å¾ˆè¿‘æ—¶æ‰ä¿ç•™åŒç±»åˆ«ç‰©ä½“ï¼ˆå¯èƒ½æ˜¯çœŸå®çš„ç¢°æ’ï¼‰
                        # å¦åˆ™è·³è¿‡ï¼ˆå¯èƒ½æ˜¯åŒä¸€ç‰©ä½“çš„å¤šä¸ªæ£€æµ‹ï¼‰
                        pass  # æš‚æ—¶ä¿ç•™ï¼Œå¯ä»¥åœ¨åç»­ä¼˜åŒ–
                    
                    # è®¡ç®—ä¸­å¿ƒç‚¹è·ç¦» (å…ˆåœ¨ä¸–ç•Œåæ ‡ä¸­è®¡ç®—)
                    x1_px, y1_px = obj1['bbox_xywh'][0], obj1['bbox_xywh'][1]
                    x2_px, y2_px = obj2['bbox_xywh'][0], obj2['bbox_xywh'][1]
                    
                    # ğŸ”„ è½¬æ¢åˆ°ä¸–ç•Œåæ ‡ (ä½¿ç”¨ Homography)
                    x1_world = x1_px / self.pixel_per_meter
                    y1_world = y1_px / self.pixel_per_meter
                    x2_world = x2_px / self.pixel_per_meter
                    y2_world = y2_px / self.pixel_per_meter
                    
                    # åœ¨ä¸–ç•Œåæ ‡ä¸­è®¡ç®—è·ç¦» (ç±³)
                    distance_meters = np.sqrt((x2_world-x1_world)**2 + (y2_world-y1_world)**2)
                    distance_pixel = np.sqrt((x2_px-x1_px)**2 + (y2_px-y1_px)**2)
                    
                    class1_name = class_names.get(obj1['class'], f"class_{obj1['class']}")
                    class2_name = class_names.get(obj2['class'], f"class_{obj2['class']}")
                    
                    # è®°å½•æ‰€æœ‰ < debug_threshold çš„æ£€æµ‹ï¼Œç”¨äºè°ƒè¯•
                    if distance_meters < debug_threshold:
                        all_proximity_pairs.append({
                            'frame': frame,
                            'class_1': class1_name,
                            'class_2': class2_name,
                            'distance_meters': distance_meters,
                            'track_ids': [obj1['track_id'], obj2['track_id']]
                        })
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæ¥è¿‘äº‹ä»¶ (ä½¿ç”¨ä¸–ç•Œè·ç¦»é˜ˆå€¼)
                    if distance_meters < world_distance_threshold:
                        class1_name = class_names.get(obj1['class'], f"class_{obj1['class']}")
                        class2_name = class_names.get(obj2['class'], f"class_{obj2['class']}")
                        
                        event = {
                            'frame': frame,
                            'time': frame_data['time'],
                            'track_id_1': obj1['track_id'],
                            'track_id_2': obj2['track_id'],
                            'class_1': class1_name,
                            'class_2': class2_name,
                            'distance_pixel': float(distance_pixel),
                            'distance_meters': float(distance_meters),  
                            'object_classes': (obj1['class'], obj2['class']),
                            'center_1_px': [float(x1_px), float(y1_px)],
                            'center_2_px': [float(x2_px), float(y2_px)],
                            'center_1_world': [float(x1_world), float(y1_world)],  
                            'center_2_world': [float(x2_world), float(y2_world)],  
                            'positions': {
                                'obj1': {'x': x1_px, 'y': y1_px},
                                'obj2': {'x': x2_px, 'y': y2_px}
                            },
                            'positions_world': {  
                                'obj1': {'x': x1_world, 'y': y1_world},
                                'obj2': {'x': x2_world, 'y': y2_world}
                            }
                        }
                        proximity_events.append(event)
                        
                        # ä¿å­˜å…³é”®å¸§å›¾åƒï¼ˆä¸¤ä¸ªæ¥è¿‘ç‰©ä½“çš„å¸§ï¼‰
                        frame_img_path = self.keyframe_dir / f"keyframe_{frame:04d}_ID{obj1['track_id']}_ID{obj2['track_id']}.jpg"
                        self.save_keyframe_with_distance(self.video_path, frame, frame_img_path, event)
        
        # ä¿å­˜æ¥è¿‘äº‹ä»¶
        events_path = self.keyframe_dir / 'proximity_events.json'
        with open(events_path, 'w') as f:
            json.dump(proximity_events, f, indent=2)
        
        print(f"  âœ“ Step 3å®Œæˆ: {len(proximity_events)}ä¸ªå…³é”®å¸§ (< {world_distance_threshold}m)")
        print(f"    æ€»æ£€æµ‹åˆ°çš„è¿‘è·ç¦»å¯¹: {len(all_proximity_pairs)}ä¸ª (< {debug_threshold}m)")
        print(f"    è·ç¦»é˜ˆå€¼: {world_distance_threshold}ç±³ (ä¸–ç•Œåæ ‡)")
        print(f"    ç¼©æ”¾å› å­: {self.pixel_per_meter:.2f} px/m")
        print(f"    è¾“å‡º: {events_path.name}")
        
        # æ‰“å°æ‰€æœ‰è¢« debug_threshold æ•æ‰çš„äº‹ä»¶ï¼Œä½†è¢« world_distance_threshold æ’é™¤çš„
        excluded_count = len(all_proximity_pairs) - len(proximity_events)
        if excluded_count > 0:
            print(f"\n  â„¹ï¸  è¢«æ’é™¤çš„æ¥è¿‘äº‹ä»¶ ({world_distance_threshold}-{debug_threshold}m):")
            for pair in all_proximity_pairs:
                if pair['distance_meters'] >= world_distance_threshold:
                    print(f"     - Frame {pair['frame']:03d}: Track {pair['track_ids'][0]}({pair['class_1']}) + Track {pair['track_ids'][1]}({pair['class_2']}) = {pair['distance_meters']:.2f}m")
        
        # è°ƒè¯•è¾“å‡ºï¼šæ£€æŸ¥è¾“å…¥æ•°æ®
        total_object_pairs = 0
        for frame_data in all_detections:
            if len(frame_data['objects']) >= 2:
                total_object_pairs += len(frame_data['objects']) * (len(frame_data['objects']) - 1) // 2
        print(f"\n  â„¹ï¸  è°ƒè¯•ä¿¡æ¯: æ£€æŸ¥äº† {total_object_pairs} ä¸ªç‰©ä½“å¯¹ï¼Œå…¶ä¸­ {len(all_proximity_pairs)} ä¸ªè·ç¦» < {debug_threshold}m")
        
        return proximity_events
    
    # =========================================================================
    # STEP 3.5: åŒç±»åˆ«ç‰©ä½“è¯¯æ£€è¿‡æ»¤
    # =========================================================================
    
    def filter_same_class_false_positives(self, proximity_events, same_class_distance_threshold=0.3):
        """è¿‡æ»¤åŒç±»åˆ«ç‰©ä½“çš„è¯¯æ£€ + è·¨å¸§åŒä¸€ç‰©ä½“çš„è¯¯åˆ†å‰²
        
        è¿‡æ»¤æ¡ä»¶ï¼š
        1. æè¿‘è·ç¦» (< 0.1m) â†’ åŒä¸€ç‰©ä½“ä¸¤éƒ¨åˆ†
        2. ä¸åˆç†çš„ç±»åˆ«ç»„åˆ + è·ç¦»ç¨³å®š (std < 0.5) â†’ åŒé€Ÿä¸å¯èƒ½
        3. æ–­æ–­ç»­ç»­å‡ºç°çš„Track IDå¯¹ + è·ç¦»è¿‘ (< 2.0m) â†’ åŒä¸€ç‰©ä½“è¢«è¯¯åˆ†å‰²
        4. éƒ½æ˜¯æ±½è½¦ç±»å‹ + è·ç¦» < 0.5m â†’ åŒä¸€è½¦è¾†çš„ä¸åŒéƒ¨åˆ†ï¼ˆå¦‚å¡è½¦å¤´å’Œèº«ä½“ï¼‰
        """
        print(f"\nã€Step 3.5: ç‰©ä½“è¯¯æ£€è¿‡æ»¤ (æ™ºèƒ½ç­–ç•¥)ã€‘")
        
        # ä¸åˆç†çš„ç±»åˆ«ç»„åˆï¼ˆä¸å¯èƒ½åŒæ—¶å‡ºç°ä¸”åŒé€Ÿè¿åŠ¨ï¼‰
        illogical_class_combinations = [
            ('person', 'motorcycle'),
            ('person', 'car'),
            ('person', 'truck'),
            ('person', 'bus'),
            ('bicycle', 'motorcycle'),
            ('bicycle', 'car'),
        ]
        
        # å®šä¹‰æ±½è½¦ç±»å‹
        vehicle_types = {'car', 'truck', 'bus', 'motorcycle'}
        
        # é¦–å…ˆåˆ†æTrack IDå¯¹çš„å‡ºç°æƒ…å†µ
        track_pair_analysis = {}
        for event in proximity_events:
            tid1, tid2 = event['track_id_1'], event['track_id_2']
            pair_key = tuple(sorted([tid1, tid2]))
            
            if pair_key not in track_pair_analysis:
                track_pair_analysis[pair_key] = {
                    'events': [],
                    'frames': [],
                    'distances': [],
                    'classes': None
                }
            
            track_pair_analysis[pair_key]['events'].append(event)
            track_pair_analysis[pair_key]['frames'].append(event['frame'])
            track_pair_analysis[pair_key]['distances'].append(event['distance_meters'])
            track_pair_analysis[pair_key]['classes'] = (event['class_1'], event['class_2'])
        
        # è¯†åˆ«"æ–­æ–­ç»­ç»­å‡ºç°"çš„Track IDå¯¹
        suspicious_discontinuous_pairs = set()
        for pair, info in track_pair_analysis.items():
            frames = sorted(info['frames'])
            distances = info['distances']
            avg_distance = sum(distances) / len(distances)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„é—´éš”ï¼ˆå‡ºç°-æ¶ˆå¤±-å†å‡ºç°ï¼‰
            has_gap = False
            for i in range(len(frames) - 1):
                if frames[i+1] - frames[i] > 3:  # é—´éš” > 3å¸§
                    has_gap = True
                    break
            
            # å¦‚æœæ–­æ–­ç»­ç»­å‡ºç°ä¸”è·ç¦»è¿‘ï¼Œæ ‡è®°ä¸ºå¯ç–‘
            if has_gap and avg_distance < 2.0:
                suspicious_discontinuous_pairs.add(pair)
        
        # ç°åœ¨è¿›è¡Œé€ä¸ªäº‹ä»¶çš„è¿‡æ»¤
        filtered_events = []
        filtered_count = 0
        filter_reasons = []
        
        for event in proximity_events:
            class_1 = event['class_1']
            class_2 = event['class_2']
            distance = event['distance_meters']
            frame = event['frame']
            tid1, tid2 = event['track_id_1'], event['track_id_2']
            pair_key = tuple(sorted([tid1, tid2]))
            
            reason = None
            
            # æ¡ä»¶1: æè¿‘è·ç¦» (< 0.1m) â†’ åŒä¸€ç‰©ä½“çš„ä¸¤éƒ¨åˆ†
            if distance < 0.1:
                reason = f"æè¿‘ ({distance:.3f}m < 0.1m)"
                filtered_count += 1
                filter_reasons.append((frame, tid1, tid2, class_1, class_2, distance, reason))
                continue
            
            # æ¡ä»¶4: éƒ½æ˜¯æ±½è½¦ç±»å‹ + è·ç¦» < 0.5m â†’ åŒä¸€è½¦è¾†çš„ä¸åŒéƒ¨åˆ†
            if (class_1 in vehicle_types and class_2 in vehicle_types) and distance < 0.5:
                reason = f"éƒ½æ˜¯æ±½è½¦ç±»å‹ ({class_1}+{class_2}, è·ç¦»{distance:.3f}m < 0.5m)"
                filtered_count += 1
                filter_reasons.append((frame, tid1, tid2, class_1, class_2, distance, reason))
                continue
            
            # æ¡ä»¶2: ä¸åˆç†çš„ç±»åˆ«ç»„åˆ + è·ç¦»ç¨³å®š â†’ åŒé€Ÿä¸å¯èƒ½
            class_pair = tuple(sorted([class_1, class_2]))
            if class_pair in [tuple(sorted(p)) for p in illogical_class_combinations]:
                pair_info = track_pair_analysis[pair_key]
                std_distance = np.std(pair_info['distances'])
                
                if std_distance < 0.5:  # è·ç¦»éå¸¸ç¨³å®š = åŒé€Ÿ = ä¸å¯èƒ½
                    reason = f"ä¸åˆç†ç±»åˆ«ç»„åˆ+åŒé€Ÿ ({class_1}+{class_2}, std={std_distance:.2f}m)"
                    filtered_count += 1
                    filter_reasons.append((frame, tid1, tid2, class_1, class_2, distance, reason))
                    continue
            
            # æ¡ä»¶3: æ–­æ–­ç»­ç»­å‡ºç°çš„Track IDå¯¹ + è·ç¦»è¿‘ â†’ åŒä¸€ç‰©ä½“è¢«è¯¯åˆ†å‰²
            if pair_key in suspicious_discontinuous_pairs:
                pair_info = track_pair_analysis[pair_key]
                reason = f"æ–­æ–­ç»­ç»­å‡ºç°({len(pair_info['frames'])}å¸§) + è·ç¦»è¿‘"
                filtered_count += 1
                filter_reasons.append((frame, tid1, tid2, class_1, class_2, distance, reason))
                continue
            
            # ä¿ç•™è¿™ä¸ªäº‹ä»¶
            filtered_events.append(event)
        
        # æ‰“å°è¿‡æ»¤è¯¦æƒ…
        if filter_reasons:
            print(f"  ğŸ—‘ï¸  è¿‡æ»¤çš„äº‹ä»¶:")
            for frame, tid1, tid2, class1, class2, dist, reason in filter_reasons[:20]:  # åªæ‰“å°å‰20ä¸ª
                print(f"      Frame {frame}: {class1}({tid1}) + {class2}({tid2}) = {dist:.3f}m ({reason})")
            if len(filter_reasons) > 20:
                print(f"      ... è¿˜æœ‰ {len(filter_reasons)-20} ä¸ª")
        
        print(f"  âœ“ è¿‡æ»¤å®Œæˆ: æ’é™¤äº† {filtered_count} ä¸ªè¯¯æ£€, ä¿ç•™ {len(filtered_events)} ä¸ªäº‹ä»¶")
        print(f"    æ¡ä»¶1: è·ç¦» < 0.1m")
        print(f"    æ¡ä»¶4: éƒ½æ˜¯æ±½è½¦ç±»å‹ (car/truck/bus/motorcycleç­‰) + è·ç¦» < 0.5m")
        print(f"    æ¡ä»¶2: ä¸åˆç†ç±»åˆ«ç»„åˆ (person/motorcycleç­‰) + è·ç¦»ç¨³å®š (std < 0.5m)")
        print(f"    æ¡ä»¶3: Track IDå¯¹æ–­æ–­ç»­ç»­å‡ºç° + å¹³å‡è·ç¦» < 2.0m")
        
        # ä¿å­˜è¿‡æ»¤åçš„äº‹ä»¶
        events_path = self.keyframe_dir / 'proximity_events_filtered.json'
        with open(events_path, 'w') as f:
            json.dump({
                'total_detected': len(proximity_events),
                'false_positives_filtered': filtered_count,
                'valid_events': len(filtered_events),
                'events': filtered_events
            }, f, indent=2)
        
        return filtered_events
    
    # =========================================================================
    # STEP 3.6: æ¸…ç†è¢«è¿‡æ»¤çš„å…³é”®å¸§å›¾ç‰‡
    # =========================================================================
    
    def cleanup_filtered_keyframes(self, original_events, filtered_events):
        """åˆ é™¤è¢«è¿‡æ»¤æ‰çš„å…³é”®å¸§å›¾ç‰‡æ–‡ä»¶"""
        # æ”¶é›†è¢«ä¿ç•™çš„ keyframe æ–‡ä»¶å
        kept_frames = set()
        for event in filtered_events:
            frame_id = event['frame']
            tid1 = event['track_id_1']
            tid2 = event['track_id_2']
            # ç”Ÿæˆåº”è¯¥è¢«ä¿ç•™çš„æ–‡ä»¶å
            filename = f"keyframe_{frame_id:04d}_ID{tid1}_ID{tid2}.jpg"
            kept_frames.add(filename)
        
        # åˆ é™¤ä¸åœ¨ kept_frames ä¸­çš„ keyframe æ–‡ä»¶
        for img_file in self.keyframe_dir.glob('keyframe_*.jpg'):
            if img_file.name not in kept_frames:
                try:
                    img_file.unlink()
                    print(f"  ğŸ—‘ï¸  åˆ é™¤å…³é”®å¸§å›¾ç‰‡: {img_file.name}")
                except Exception as e:
                    print(f"  âš ï¸  åˆ é™¤å›¾ç‰‡å¤±è´¥ {img_file.name}: {e}")
    
    # =========================================================================
    # STEP 4: Homography è®°å½• (ä»…ä¿¡æ¯) âœ¨ å·²åœ¨Step 3ä¸­ä½¿ç”¨
    # =========================================================================
    
    def transform_key_frames_to_world(self, proximity_events):
        """Step 4: Homography ä¿¡æ¯ä¿å­˜
        
        â„¹ï¸ æ³¨æ„: Homography å˜æ¢å·²ç»åœ¨ Step 3 ä¸­å®Œæˆï¼
        - Step 3: åœ¨ã€ä¸–ç•Œåæ ‡ã€‘ä¸­è®¡ç®—æ¥è¿‘åº¦
        - Step 4: åªè®°å½•å’Œä¿å­˜ Homography ä¿¡æ¯
        """
        print(f"\nã€Step 4: Homography ä¿¡æ¯ä¿å­˜ã€‘")
        print(f"  â„¹ï¸  æ³¨æ„: åæ ‡å˜æ¢å·²åœ¨Step 3ä¸­å®Œæˆï¼ˆä½¿ç”¨Homographyï¼‰")
        
        if self.H is None:
            print(f"  âš ï¸  æœªåŠ è½½Homography")
            return proximity_events
        
        # ç›´æ¥è¿”å›äº‹ä»¶ï¼ˆå·²åŒ…å«ä¸–ç•Œåæ ‡ï¼‰
        # ä¿å­˜ Homography çŸ©é˜µä¿¡æ¯ä¾›å‚è€ƒ
        trans_path = self.homography_dir / 'transformed_key_frames.json'
        with open(trans_path, 'w') as f:
            json.dump(proximity_events, f, indent=2)
        
        print(f"  âœ“ Step 4å®Œæˆ: {len(proximity_events)}ä¸ªå…³é”®å¸§ä¿¡æ¯å·²ä¿å­˜")
        print(f"    ç¼©æ”¾å› å­: {self.pixel_per_meter:.2f} px/m")
        print(f"    åæ ‡ç³»ç»Ÿ: ä¸–ç•Œåæ ‡ (å·²åœ¨Step 3å˜æ¢)")
        print(f"    è¾“å‡º: {trans_path.name}")
        
        return proximity_events
    
    # =========================================================================
    # STEP 5: TTC å’Œ Event åˆ†çº§
    # =========================================================================
    
    def analyze_collision_risk(self, transformed_events):
        """Step 5: TTC è®¡ç®—å’Œ Event åˆ†çº§
        
        è®¡ç®— TTCï¼Œåˆ†çº§äº‹ä»¶ (L1/L2/L3)
        
         æ”¹è¿›: è¿‡æ»¤åŒç±»åˆ«ç‰©ä½“çš„æè¿‘æ¥è¿‘äº‹ä»¶
        - å¦‚æœä¸¤ä¸ªç‰©ä½“éƒ½æ˜¯åŒä¸€ç±»åˆ«ï¼ˆå¦‚ä¸¤ä¸ªcarï¼Œä¸¤ä¸ªmotorcycleï¼‰
        - ä¸”è·ç¦» < 0.5mï¼Œåˆ™å¯èƒ½æ˜¯åŒä¸€ç‰©ä½“çš„è¯¯æ£€
        - æ ‡è®°ä¸º 'Filtered_SameClass' å¹¶æ’é™¤
        """
        print(f"\nã€Step 5: ç¢°æ’é£é™©åˆ†æã€‘")
        
        if not transformed_events:
            print(f"  âš ï¸  æ²¡æœ‰å…³é”®å¸§ï¼Œæ— æ³•åˆ†æ")
            return [], {0: 0, 1: 0, 2: 0, 3: 0}
        
        analyzed_events = []
        filtered_count = 0
        
        for event in transformed_events:
            analyzed = event.copy()
            
            # åœ¨ä¸–ç•Œåæ ‡ä¸­è¿›è¡Œåˆ†çº§
            distance = event['distance_meters']
            class_1 = event.get('class_1', '')
            class_2 = event.get('class_2', '')
            
            # ğŸ” æ£€æŸ¥æ˜¯å¦ä¸ºåŒç±»åˆ«ç‰©ä½“çš„æè¿‘æ¥è¿‘äº‹ä»¶
            if class_1 == class_2 and distance < 0.5:
                # åŒä¸€ç±»åˆ« + è·ç¦»å¾ˆè¿‘ = å¯èƒ½æ˜¯åŒä¸€ç‰©ä½“çš„ä¸åŒéƒ¨åˆ†
                analyzed['level'] = 0
                analyzed['level_name'] = 'Filtered_SameClass'
                analyzed['reason'] = f"Same class ({class_1}) with distance {distance:.3f}m < 0.5m - likely same object"
                filtered_count += 1
            # åˆ†çº§æ ‡å‡† (ç±³)
            elif distance < 0.5:
                analyzed['level'] = 1
                analyzed['level_name'] = 'Collision'
            elif distance < 1.5:
                analyzed['level'] = 2
                analyzed['level_name'] = 'Near Miss'
            else:
                analyzed['level'] = 3
                analyzed['level_name'] = 'Avoidance'
            
            analyzed_events.append(analyzed)
        
        # ç»Ÿè®¡
        level_counts = {0: 0, 1: 0, 2: 0, 3: 0}
        for event in analyzed_events:
            level_counts[event['level']] += 1
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_path = self.analysis_dir / 'collision_events.json'
        with open(analysis_path, 'w') as f:
            json.dump(analyzed_events, f, indent=2)
        
        print(f"  âœ“ Step 5å®Œæˆ")
        print(f"    - Filtered (Same class, <0.5m): {level_counts[0]} ğŸš«")
        print(f"    - Level 1 (Collision, <0.5m): {level_counts[1]}")
        print(f"    - Level 2 (Near Miss, 0.5-1.5m): {level_counts[2]}")
        print(f"    - Level 3 (Avoidance, >1.5m): {level_counts[3]}")
        print(f"    è¾“å‡º: {analysis_path.name}")
        
        return analyzed_events, level_counts
    
    # =========================================================================
    # æŠ¥å‘Šç”Ÿæˆ
    # =========================================================================
    
    def generate_report(self, proximity_events, analyzed_events, level_counts):
        """ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š"""
        report_path = self.analysis_dir / 'analysis_report.txt'
        
        with open(report_path, 'w') as f:
            f.write("="*70 + "\n")
            f.write("YOLO-First ç¢°æ’æ£€æµ‹åˆ†ææŠ¥å‘Š (Method A - å¯¼å¸ˆæ¨è)\n")
            f.write("="*70 + "\n\n")
            
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è¾“å…¥è§†é¢‘: {self.video_path}\n")
            f.write(f"Homography: {self.homography_path if self.H is not None else 'æœªæä¾›'}\n")
            f.write(f"ç»“æœç›®å½•: {self.run_dir}\n\n")
            
            f.write(f"å¤„ç†æ–¹å¼: YOLO-First (Method A - å¯¼å¸ˆæ¨è)\n")
            f.write(f"æµç¨‹: YOLOæ£€æµ‹ â†’ è½¨è¿¹(px) â†’ å…³é”®å¸§ â†’ Homography(å…³é”®å¸§) â†’ åˆ†æ\n")
            f.write(f"åæ ‡ç³»è½¬æ¢: ä»…åœ¨å…³é”®å¸§è¿›è¡Œ\n")
            f.write(f"ä¼˜åŒ–: ä»…å¯¹~{len(proximity_events)}ä¸ªå…³é”®å¸§åšHomography\n\n")
            
            f.write(f"å…³é”®å¸§ç»Ÿè®¡:\n")
            f.write(f"  - æ€»æ¥è¿‘äº‹ä»¶: {len(proximity_events)}\n")
            if analyzed_events:
                f.write(f"  - Level 1 (Collision): {level_counts[1]}\n")
                f.write(f"  - Level 2 (Near Miss): {level_counts[2]}\n")
                f.write(f"  - Level 3 (Avoidance): {level_counts[3]}\n\n")
            
            if analyzed_events:
                f.write("å‰10ä¸ªé«˜é£é™©äº‹ä»¶:\n")
                f.write("-"*70 + "\n")
                
                sorted_events = sorted(analyzed_events, key=lambda e: e.get('level', 3))
                
                for i, event in enumerate(sorted_events[:10], 1):
                    f.write(f"\n{i}. Frame {event['frame']} ({event['time']:.2f}s)\n")
                    # å¤„ç†ä¸åŒçš„ç‰©ä½“IDå­—æ®µå
                    obj_ids = event.get('object_ids') or [event.get('track_id_1', -1), event.get('track_id_2', -1)]
                    f.write(f"   ç‰©ä½“ID: {obj_ids}\n")
                    f.write(f"   é£é™©ç­‰çº§: Level {event['level']} ({event.get('level_name', '?')})\n")
                    f.write(f"   è·ç¦»(åƒç´ ): {event['distance_pixel']:.1f}px\n")
                    if 'distance_meters' in event:
                        f.write(f"   è·ç¦»(ç±³): {event['distance_meters']:.2f}m\n")
            else:
                f.write("æœªæ£€æµ‹åˆ°æ¥è¿‘äº‹ä»¶\n")
            
            f.write("\n" + "="*70 + "\n")
            f.write("æŠ¥å‘Šç»“æŸ\n")
        
        print(f"\n  âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path.name}")
    
    def _copy_results_to_workspace(self):
        """è‡ªåŠ¨å¤åˆ¶ç»“æœåˆ° /workspace/ultralytics/resultsï¼ˆä½¿å…¶åœ¨ VS Code ä¸­å¯è§ï¼‰"""
        import shutil
        
        workspace_results = Path("/workspace/ultralytics/results")
        if workspace_results.exists() and self.run_dir.parent != workspace_results:
            try:
                # æ£€æŸ¥ç»“æœæ˜¯å¦å·²åœ¨ workspace_results ä¸­
                result_in_workspace = workspace_results / self.run_dir.name
                if not result_in_workspace.exists():
                    shutil.copytree(self.run_dir, result_in_workspace)
                    print(f"\n  âœ“ ç»“æœå·²å¤åˆ¶åˆ°: {result_in_workspace}")
                    print(f"    ç°åœ¨å¯ä»¥åœ¨ VS Code ä¸­ç›´æ¥æŸ¥çœ‹ï¼")
            except Exception as e:
                print(f"\n  âš ï¸  å¤åˆ¶å¤±è´¥ ({e})ï¼Œä½†ç»“æœå·²ä¿å­˜åœ¨: {self.run_dir}")
    
    # =========================================================================
    # ç®¡é“ç¼–æ’
    # =========================================================================
    
    def run(self, conf_threshold=0.45):
        """è¿è¡Œå®Œæ•´ YOLO-First ç®¡é“ (Method A)"""
        try:
            # Step 0: åŠ è½½Homography (å¦‚æœæä¾›)
            if self.homography_path:
                print(f"\nã€Step 0: åŠ è½½èµ„æºã€‘")
                self.load_homography()
            
            # Step 1: YOLO æ£€æµ‹
            all_detections = self.run_yolo_detection(conf_threshold)
            
            if not all_detections:
                print(f"\nâŒ æœªæ£€æµ‹åˆ°ä»»ä½•ç‰©ä½“ï¼Œåœæ­¢å¤„ç†")
                return
            
            # Step 1.5: åŒå¸§å†…ç‰©ä½“åˆ†å‰²åˆå¹¶  - åˆå¹¶YOLOåˆ†å‰²çš„åŒç±»ç‰©ä½“
            all_detections = self.merge_fragmented_objects_in_frame(all_detections, same_class_distance_threshold=100)
            
            # è°ƒè¯•ï¼šæ£€æŸ¥Step 1.5åçš„æ•°æ®
            print(f"\nã€è°ƒè¯•: Step 1.5åçš„æ•°æ®ã€‘")
            for frame_data in all_detections:
                if frame_data['frame'] == 115:
                    frame_115_objects_after_15 = [(obj['track_id'], obj['class']) for obj in frame_data['objects']]
                    print(f"  Frame 115 Step 1.5å: {frame_115_objects_after_15}")
            
            # Step 2: è½¨è¿¹æ„å»º
            tracks = self.build_trajectories(all_detections)
            
            # Step 2.4: è½¨è¿¹é—´æ–­æ£€æµ‹  - æ£€æµ‹å‡ºç°â†’æ¶ˆå¤±â†’é‡æ–°å‡ºç°çš„å¯ç–‘è½¨è¿¹
            suspicious_tracks = self.detect_discontinuous_tracks(all_detections, max_gap_frames=3)
            
            # Step 2.5: è½¨è¿¹è¿ç»­æ€§è¿‡æ»¤  - ç§»é™¤çŸ­è½¨è¿¹è¯¯æ£€
            all_detections = self.filter_short_tracks(all_detections, min_track_length=self.min_track_length)
            
            # è°ƒè¯•ï¼šæ£€æŸ¥Step 2.5åçš„æ•°æ®
            print(f"\nã€è°ƒè¯•: Step 2.5åçš„æ•°æ®ã€‘")
            frame_115_objects = []
            frame_148_objects = []
            for frame_data in all_detections:
                if frame_data['frame'] == 115:
                    frame_115_objects = [(obj['track_id'], obj['class']) for obj in frame_data['objects']]
                if frame_data['frame'] == 148:
                    frame_148_objects = [(obj['track_id'], obj['class']) for obj in frame_data['objects']]
            print(f"  Frame 115: {frame_115_objects}")
            print(f"  Frame 148: {frame_148_objects}")
            
            # Step 3: å…³é”®å¸§æ£€æµ‹
            proximity_events = self.extract_key_frames(all_detections, world_distance_threshold=4.5)
            
            if not proximity_events:
                print(f"\nâš ï¸  æœªæ£€æµ‹åˆ°æ¥è¿‘äº‹ä»¶")
                analyzed_events = []
                level_counts = {0: 0, 1: 0, 2: 0, 3: 0}
            else:
                # Step 3.5: åŒç±»åˆ«ç‰©ä½“è¯¯æ£€è¿‡æ»¤ âœ¨ æ–°å¢
                filtered_events = self.filter_same_class_false_positives(proximity_events, same_class_distance_threshold=0.3)
                
                # æ¸…ç†ï¼šåˆ é™¤è¢«è¿‡æ»¤æ‰çš„å…³é”®å¸§å›¾ç‰‡
                if len(filtered_events) < len(proximity_events):
                    self.cleanup_filtered_keyframes(proximity_events, filtered_events)
                
                # Step 4: Homography å˜æ¢ (ä»…å…³é”®å¸§)
                if self.H is not None:
                    transformed_events = self.transform_key_frames_to_world(filtered_events)
                else:
                    print(f"\nã€Step 4: Homography å˜æ¢ã€‘")
                    print(f"  âš ï¸  è·³è¿‡ (æœªåŠ è½½Homography)")
                    transformed_events = filtered_events
                
                # Step 5: é£é™©åˆ†æ
                analyzed_events, level_counts = self.analyze_collision_risk(transformed_events if transformed_events else filtered_events)
            
            # ç”ŸæˆæŠ¥å‘Š
            self.generate_report(proximity_events, analyzed_events, level_counts)
            
            print(f"\n{'='*70}")
            print(f"âœ“ YOLO-First Pipeline (Method A) å®Œæˆï¼")
            print(f"{'='*70}")
            print(f"ç»“æœä¿å­˜åœ¨: {self.run_dir}")
            
            # è‡ªåŠ¨å¤åˆ¶ç»“æœåˆ° /workspace/ultralytics/resultsï¼ˆå¦‚æœä¸åŒçš„è¯ï¼‰
            self._copy_results_to_workspace()
            
            print(f"\næ–‡ä»¶å¤¹ç»“æ„:")
            print(f"  1_yolo_detection/")
            print(f"    â”œâ”€â”€ detections_pixel.json")
            print(f"    â”œâ”€â”€ detection_stats.json")
            print(f"    â””â”€â”€ *.jpg (æ‰€æœ‰æœ‰æ£€æµ‹çš„å¸§)")
            print(f"  2_trajectories/")
            print(f"    â”œâ”€â”€ tracks.json")
            print(f"    â””â”€â”€ track_stats.json")
            print(f"  3_key_frames/")
            print(f"    â”œâ”€â”€ proximity_events.json")
            print(f"    â””â”€â”€ *.jpg (æ¥è¿‘äº‹ä»¶çš„å…³é”®å¸§)")
            print(f"  4_homography_transform/")
            print(f"    â”œâ”€â”€ homography.json")
            print(f"    â””â”€â”€ transformed_key_frames.json")
            print(f"  5_collision_analysis/")
            print(f"    â”œâ”€â”€ collision_events.json")
            print(f"    â””â”€â”€ analysis_report.txt")
            
        except Exception as e:
            print(f"\nâŒ Pipeline é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='YOLO-First ç¢°æ’æ£€æµ‹Pipeline (Method A - å¯¼å¸ˆæ¨è)')
    parser.add_argument('--video', type=str, required=True, help='è¾“å…¥è§†é¢‘è·¯å¾„')
    parser.add_argument('--homography', type=str, default=None, 
                       help='Homography JSONè·¯å¾„ (å¯é€‰)')
    parser.add_argument('--output', type=str, default='../../results', 
                       help='ç»“æœåŸºç¡€ç›®å½•')
    parser.add_argument('--conf', type=float, default=0.45, 
                       help='YOLOç½®ä¿¡åº¦é˜ˆå€¼ (è¶Šé«˜=è¶Šä¸¥æ ¼ï¼Œå‡å°‘è¯¯æ£€) (é»˜è®¤: 0.45)')
    parser.add_argument('--skip-frames', type=int, default=1,
                       help='æŠ½å¸§å‚æ•°: 1=å¤„ç†æ‰€æœ‰å¸§, 3=æ¯éš”3å¸§å¤„ç†1å¸§, 5=æ¯éš”5å¸§å¤„ç†1å¸§ (é»˜è®¤: 1)')
    parser.add_argument('--model', type=str, default='yolo11m',
                       help='YOLO æ¨¡å‹: yolo11n(å¿«é€Ÿ), yolo11m(ä¸­ç­‰,æ›´ç²¾ç¡®), yolo11l(æœ€ç²¾ç¡®) (é»˜è®¤: yolo11m)')
    parser.add_argument('--min-track-length', type=int, default=3,
                       help='æœ€å°è½¨è¿¹é•¿åº¦(å¸§æ•°)ï¼ŒçŸ­äºæ­¤çš„è½¨è¿¹è¢«è®¤ä¸ºæ˜¯è¯¯æ£€å¹¶æ’é™¤ (é»˜è®¤: 3)')
    
    args = parser.parse_args()
    
    pipeline = YOLOFirstPipelineA(args.video, args.homography, args.output, 
                                  skip_frames=args.skip_frames, 
                                  model=args.model,
                                  min_track_length=args.min_track_length)
    pipeline.run(args.conf)
